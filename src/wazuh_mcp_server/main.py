#!/usr/bin/env python3
"""
Wazuh MCP Server for Claude Desktop Integration - Production Edition
-------------------------------------------------------------------
Production-grade MCP server with advanced security analysis capabilities,
comprehensive validation, and enterprise-ready features.
"""

import os
import sys
import json
import asyncio
import uuid
from collections import Counter
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

import urllib3
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

# Clean absolute imports within the package
from wazuh_mcp_server.config import WazuhConfig, ComplianceFramework
from wazuh_mcp_server.__version__ import __version__
from wazuh_mcp_server.api.wazuh_client_manager import WazuhClientManager
from wazuh_mcp_server.analyzers import SecurityAnalyzer, ComplianceAnalyzer
from wazuh_mcp_server.utils import (
    setup_logging, get_logger, LogContext,
    validate_alert_query, validate_agent_query, validate_threat_analysis,
    validate_alert_summary_query, validate_vulnerability_summary_query,
    validate_critical_vulnerabilities_query, validate_running_agents_query,
    validate_rules_summary_query, validate_weekly_stats_query,
    validate_remoted_stats_query, validate_log_collector_stats_query,
    validate_cluster_health_query, validate_manager_error_logs_query,
    validate_agent_processes_query, validate_agent_ports_query,
    validate_ip_address, validate_file_hash, 
    ValidationError, WazuhMCPError, ConfigurationError, APIError
)
from wazuh_mcp_server.utils.error_standardization import (
    config_error_handler, optional_feature_handler, safe_execute,
    StandardErrorResponse, ErrorAggregator
)
from wazuh_mcp_server.utils.platform_utils import (
    get_wazuh_log_path, get_wazuh_paths, get_suspicious_paths
)
from wazuh_mcp_server.tools.factory import ToolFactory

# SSL warnings will be disabled per-request basis in clients if needed
# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # SECURITY: Removed global disable


class WazuhMCPServer:
    """Production-grade MCP Server implementation for Wazuh integration."""
    
    @config_error_handler(context={"operation": "server_initialization"})
    def __init__(self):
        # Initialize configuration first
        self.config = WazuhConfig.from_env()
        
        # Setup logging with configuration
        self.logger = setup_logging(
            log_level=self.config.log_level,
            log_dir="logs" if not self.config.debug else None,
            enable_structured=True,
            enable_rotation=True
        )
        
        self.logger.info(f"Initializing Wazuh MCP Server v{__version__}")
        
        # Check optional dependencies and warn about missing features
        self._check_optional_dependencies()
        
        # Initialize components
        self.server = Server("wazuh-mcp")
        self.api_client = WazuhClientManager(self.config)
        self.security_analyzer = SecurityAnalyzer()
        self.compliance_analyzer = ComplianceAnalyzer()
        
        # Initialize optional prompt enhancement system (Phase 5)
        self.context_aggregator = self._initialize_prompt_enhancement()
        
        # Initialize modular tool system for better organization
        self.tool_factory = safe_execute(
            lambda: ToolFactory(self),
            default_value=None,
            error_context={"operation": "tool_factory_init"},
            log_errors=True
        )
        
        # Setup handlers
        self._setup_handlers()
        
        self.logger.info("Wazuh MCP Server initialized successfully")
    
    def _check_optional_dependencies(self):
        """Check for optional dependencies and warn about missing features."""
        missing_features = []
        available_features = []
        
        # Check external API dependencies
        if self.config.virustotal_api_key:
            try:
                import requests  # Basic dependency for API calls
                available_features.append("VirusTotal integration")
            except ImportError:
                missing_features.append("VirusTotal integration (requests library missing)")
        
        if self.config.shodan_api_key:
            try:
                import requests
                available_features.append("Shodan integration")
            except ImportError:
                missing_features.append("Shodan integration (requests library missing)")
        
        if self.config.abuseipdb_api_key:
            try:
                import requests
                available_features.append("AbuseIPDB integration")
            except ImportError:
                missing_features.append("AbuseIPDB integration (requests library missing)")
        
        # Check prompt enhancement dependencies
        if getattr(self.config, 'enable_prompt_enhancement', False):
            try:
                from .prompt_enhancement import PromptContextAggregator
                available_features.append("Prompt enhancement system")
            except ImportError:
                missing_features.append("Prompt enhancement system (implementation missing)")
        
        # Check ML analysis dependencies (if enabled)
        if getattr(self.config, 'enable_ml_analysis', False):
            try:
                # Basic check for common ML libraries
                import json  # Always available
                available_features.append("Basic ML analysis")
                
                # Check for advanced ML libraries (optional)
                try:
                    import numpy
                    available_features.append("Advanced numerical analysis")
                except ImportError:
                    self.logger.debug("NumPy not available - basic analysis only")
                    
            except ImportError:
                missing_features.append("ML analysis capabilities")
        
        # Log results
        if available_features:
            self.logger.info(f"Optional features available: {', '.join(available_features)}")
        
        if missing_features:
            self.logger.warning(f"Optional features unavailable: {', '.join(missing_features)}")
            self.logger.info("Install missing dependencies to enable additional features")
        
        # Store for later reference
        self._available_features = available_features
        self._missing_features = missing_features
    
    @optional_feature_handler(context={"feature": "prompt_enhancement"})
    def _initialize_prompt_enhancement(self):
        """Initialize prompt enhancement system with standardized error handling."""
        if not getattr(self.config, 'enable_prompt_enhancement', False):
            return None
        
        from .prompt_enhancement import PromptContextAggregator
        context_aggregator = PromptContextAggregator(self)
        # Setup pipelines after all components are initialized
        context_aggregator.setup_pipelines()
        self.logger.info("Prompt enhancement system initialized")
        return context_aggregator
    
    def _get_current_timestamp(self) -> str:
        """Get current timestamp in ISO format."""
        from datetime import datetime
        return datetime.utcnow().isoformat() + "Z"
    
    def _format_error_response(self, error: Exception, request_id: str = None, 
                               execution_time: float = None) -> str:
        """Format error response consistently."""
        error_data = {
            "error": str(error),
            "error_type": type(error).__name__
        }
        
        if request_id:
            error_data["request_id"] = request_id
        
        if execution_time is not None:
            error_data["execution_time"] = round(execution_time, 2)
        
        error_data["timestamp"] = datetime.utcnow().isoformat()
        
        return json.dumps(error_data, indent=2)
    
    async def initialize_connections(self):
        """Initialize connections and detect Wazuh version."""
        try:
            async with self.api_client as client:
                version = await client.detect_wazuh_version()
                if version:
                    self.logger.info(f"Connected to Wazuh {version}")
                else:
                    self.logger.warning("Could not detect Wazuh version")
        except Exception as e:
            self.logger.error(f"Failed to initialize connections: {str(e)}")
    
    def _setup_handlers(self):
        """Setup MCP protocol handlers with production-grade capabilities."""
        
        @self.server.list_resources()
        async def handle_list_resources() -> list[types.Resource]:
            """List available Wazuh resources."""
            return [
                types.Resource(
                    uri="wazuh://alerts/recent",
                    name="Recent Alerts",
                    description="Most recent security alerts from Wazuh",
                    mimeType="application/json"
                ),
                types.Resource(
                    uri="wazuh://alerts/summary",
                    name="Alert Summary",
                    description="Statistical summary of alerts",
                    mimeType="application/json"
                ),
                types.Resource(
                    uri="wazuh://agents/status",
                    name="Agent Status",
                    description="Status of all Wazuh agents",
                    mimeType="application/json"
                ),
                types.Resource(
                    uri="wazuh://vulnerabilities/critical",
                    name="Critical Vulnerabilities",
                    description="Critical vulnerabilities across all agents",
                    mimeType="application/json"
                ),
                types.Resource(
                    uri="wazuh://compliance/status",
                    name="Compliance Status",
                    description="Current compliance posture",
                    mimeType="application/json"
                ),
                types.Resource(
                    uri="wazuh://threats/active",
                    name="Active Threats",
                    description="Currently active threat indicators",
                    mimeType="application/json"
                ),
                types.Resource(
                    uri="wazuh://system/health",
                    name="System Health",
                    description="Overall system health metrics",
                    mimeType="application/json"
                )
            ]
        
        @self.server.list_prompts()
        async def handle_list_prompts() -> list[types.Prompt]:
            """List available security analysis prompts."""
            return [
                types.Prompt(
                    name="security-incident-analysis",
                    description="Analyze a security incident from Wazuh alerts with comprehensive investigation steps",
                    arguments=[
                        types.PromptArgument(
                            name="alert_id",
                            description="The ID of the alert to analyze",
                            required=True
                        ),
                        types.PromptArgument(
                            name="include_context",
                            description="Include surrounding context and related alerts",
                            required=False
                        )
                    ]
                ),
                types.Prompt(
                    name="threat-hunting-query",
                    description="Generate threat hunting queries based on IOCs and attack patterns",
                    arguments=[
                        types.PromptArgument(
                            name="threat_type",
                            description="Type of threat to hunt for (malware, intrusion, data_exfiltration, etc.)",
                            required=True
                        ),
                        types.PromptArgument(
                            name="time_range",
                            description="Time range for the hunt (e.g., '24h', '7d', '30d')",
                            required=False
                        ),
                        types.PromptArgument(
                            name="target_agents",
                            description="Specific agents to focus on (comma-separated IDs)",
                            required=False
                        )
                    ]
                ),
                types.Prompt(
                    name="compliance-assessment",
                    description="Perform a comprehensive compliance assessment against security frameworks",
                    arguments=[
                        types.PromptArgument(
                            name="framework",
                            description="Compliance framework (pci_dss, hipaa, gdpr, nist, iso27001)",
                            required=True
                        ),
                        types.PromptArgument(
                            name="scope",
                            description="Assessment scope (full, critical_controls, specific_section)",
                            required=False
                        )
                    ]
                ),
                types.Prompt(
                    name="security-report-generation",
                    description="Generate executive security reports with recommendations",
                    arguments=[
                        types.PromptArgument(
                            name="report_type",
                            description="Type of report (executive, technical, compliance, incident)",
                            required=True
                        ),
                        types.PromptArgument(
                            name="time_period",
                            description="Reporting period (daily, weekly, monthly, quarterly)",
                            required=False
                        ),
                        types.PromptArgument(
                            name="audience",
                            description="Target audience (executives, security_team, compliance_team)",
                            required=False
                        )
                    ]
                ),
                types.Prompt(
                    name="vulnerability-prioritization",
                    description="Prioritize vulnerabilities based on risk, exploitability, and business impact",
                    arguments=[
                        types.PromptArgument(
                            name="severity_threshold",
                            description="Minimum severity level (low, medium, high, critical)",
                            required=False
                        ),
                        types.PromptArgument(
                            name="asset_criticality",
                            description="Focus on assets with specific criticality (low, medium, high, critical)",
                            required=False
                        )
                    ]
                ),
                types.Prompt(
                    name="forensic-analysis",
                    description="Perform forensic analysis of security incidents with timeline reconstruction",
                    arguments=[
                        types.PromptArgument(
                            name="incident_id",
                            description="Incident identifier or alert ID to investigate",
                            required=True
                        ),
                        types.PromptArgument(
                            name="analysis_depth",
                            description="Depth of analysis (surface, detailed, comprehensive)",
                            required=False
                        )
                    ]
                )
            ]
        
        @self.server.get_prompt()
        async def handle_get_prompt(name: str, arguments: dict) -> types.GetPromptResult:
            """Get specific security analysis prompt with context."""
            request_id = str(uuid.uuid4())
            
            try:
                with LogContext(request_id):
                    self.logger.info(f"Generating prompt: {name}", extra={"details": arguments})
                    
                    if name == "security-incident-analysis":
                        return await self._get_security_incident_analysis_prompt(arguments)
                    elif name == "threat-hunting-query":
                        return await self._get_threat_hunting_query_prompt(arguments)
                    elif name == "compliance-assessment":
                        return await self._get_compliance_assessment_prompt(arguments)
                    elif name == "security-report-generation":
                        return await self._get_security_report_generation_prompt(arguments)
                    elif name == "vulnerability-prioritization":
                        return await self._get_vulnerability_prioritization_prompt(arguments)
                    elif name == "forensic-analysis":
                        return await self._get_forensic_analysis_prompt(arguments)
                    else:
                        raise ValueError(f"Unknown prompt: {name}")
                        
            except Exception as e:
                self.logger.error(f"Error generating prompt {name}: {str(e)}")
                return types.GetPromptResult(
                    description=f"Error generating prompt: {str(e)}",
                    messages=[
                        types.PromptMessage(
                            role="user",
                            content=types.TextContent(
                                type="text",
                                text=f"Error: {str(e)}"
                            )
                        )
                    ]
                )
        
        @self.server.read_resource()
        async def handle_read_resource(uri: str) -> str:
            """Read specific Wazuh resource with comprehensive error handling."""
            request_id = str(uuid.uuid4())
            
            try:
                with LogContext(request_id):
                    self.logger.info(f"Reading resource: {uri}")
                    
                    if uri == "wazuh://alerts/recent":
                        data = await self.api_client.get_alerts(limit=50)
                        return json.dumps(self._format_alerts(data), indent=2)
                    
                    elif uri == "wazuh://alerts/summary":
                        data = await self.api_client.get_alerts(limit=1000)
                        summary = self._generate_alert_summary(data)
                        return json.dumps(summary, indent=2)
                    
                    elif uri == "wazuh://agents/status":
                        data = await self.api_client.get_agents()
                        return json.dumps(self._format_agents(data), indent=2)
                    
                    elif uri == "wazuh://vulnerabilities/critical":
                        agents_data = await self.api_client.get_agents(status="active")
                        critical_vulns = await self._get_critical_vulnerabilities(agents_data)
                        return json.dumps(critical_vulns, indent=2)
                    
                    elif uri == "wazuh://compliance/status":
                        # Quick compliance overview
                        agents_data = await self.api_client.get_agents()
                        alerts_data = await self.api_client.get_alerts(limit=500)
                        compliance_overview = await self._get_compliance_overview(agents_data, alerts_data)
                        return json.dumps(compliance_overview, indent=2)
                    
                    elif uri == "wazuh://threats/active":
                        alerts_data = await self.api_client.get_alerts(limit=500, time_range=3600)
                        threat_summary = await self._get_active_threats(alerts_data)
                        return json.dumps(threat_summary, indent=2)
                    
                    elif uri == "wazuh://system/health":
                        health_data = await self.api_client.health_check()
                        return json.dumps(health_data, indent=2)
                    
                    else:
                        raise ValueError(f"Unknown or unsupported resource URI: {uri}")
                        
            except Exception as e:
                self.logger.error(f"Error reading resource {uri}: {str(e)}")
                return self._format_error_response(e, request_id)
        
        @self.server.list_tools()
        async def handle_list_tools() -> list[types.Tool]:
            """List available Wazuh tools with comprehensive capabilities."""
            return [
                types.Tool(
                    name="get_alerts",
                    description="Retrieve Wazuh alerts with advanced filtering and validation",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "limit": {
                                "type": "integer",
                                "description": "Maximum number of alerts to retrieve",
                                "default": 100,
                                "minimum": 1,
                                "maximum": 10000
                            },
                            "level": {
                                "type": "integer",
                                "description": "Minimum alert level (1-15)",
                                "minimum": 1,
                                "maximum": 15
                            },
                            "time_range": {
                                "type": "integer",
                                "description": "Time range in seconds (e.g., 3600 for last hour)",
                                "minimum": 300,
                                "maximum": 86400
                            },
                            "agent_id": {
                                "type": "string",
                                "description": "Filter alerts by specific agent ID"
                            }
                        }
                    }
                ),
                types.Tool(
                    name="analyze_threats",
                    description="Perform comprehensive threat analysis with ML-based risk assessment",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "category": {
                                "type": "string",
                                "description": "Threat category to analyze",
                                "enum": ["all", "intrusion", "malware", "vulnerability", "compliance", "authentication"]
                            },
                            "time_range": {
                                "type": "integer",
                                "description": "Analysis time window in seconds",
                                "default": 3600,
                                "minimum": 300,
                                "maximum": 86400
                            },
                            "include_patterns": {
                                "type": "boolean",
                                "description": "Include attack pattern detection",
                                "default": False
                            }
                        }
                    }
                ),
                types.Tool(
                    name="check_agent_health",
                    description="Comprehensive agent health monitoring and diagnostics",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "agent_id": {
                                "type": "string",
                                "description": "Specific agent ID to check (optional)"
                            },
                            "include_stats": {
                                "type": "boolean",
                                "description": "Include detailed statistics",
                                "default": False
                            }
                        }
                    }
                ),
                types.Tool(
                    name="compliance_check",
                    description="Perform compliance assessment against security frameworks",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "framework": {
                                "type": "string",
                                "description": "Compliance framework to assess",
                                "enum": ["pci_dss", "hipaa", "gdpr", "nist", "iso27001"]
                            },
                            "include_evidence": {
                                "type": "boolean",
                                "description": "Include detailed evidence and recommendations",
                                "default": True
                            }
                        }
                    }
                ),
                types.Tool(
                    name="check_ioc",
                    description="Check indicators of compromise against threat intelligence",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "ip_address": {
                                "type": "string",
                                "description": "IP address to check",
                                "pattern": "^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$"
                            },
                            "file_hash": {
                                "type": "string",
                                "description": "File hash to check (MD5, SHA1, or SHA256)"
                            }
                        }
                    }
                ),
                types.Tool(
                    name="risk_assessment",
                    description="Perform comprehensive risk assessment of the environment",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "time_window_hours": {
                                "type": "integer",
                                "description": "Time window in hours for risk assessment",
                                "default": 24,
                                "minimum": 1,
                                "maximum": 168
                            },
                            "include_vulnerabilities": {
                                "type": "boolean",
                                "description": "Include vulnerability analysis in risk assessment",
                                "default": True
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_agent_processes",
                    description="Detect malicious processes and unauthorized software with comprehensive threat detection and behavior analysis",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "agent_id": {
                                "type": "string",
                                "description": "The ID of the agent to query"
                            },
                            "process_filter": {
                                "type": "string",
                                "description": "Process name pattern filter (regex supported)"
                            },
                            "include_children": {
                                "type": "boolean",
                                "description": "Include child process hierarchy analysis",
                                "default": True
                            },
                            "sort_by": {
                                "type": "string",
                                "description": "Sort processes by specified metric",
                                "enum": ["cpu", "memory", "pid", "name", "threat_score"],
                                "default": "threat_score"
                            },
                            "include_hashes": {
                                "type": "boolean",
                                "description": "Include file hash verification and reputation checking",
                                "default": True
                            },
                            "suspicious_only": {
                                "type": "boolean",
                                "description": "Show only processes flagged as suspicious or malicious",
                                "default": False
                            },
                            "threat_detection": {
                                "type": "boolean",
                                "description": "Enable advanced threat detection and behavior analysis",
                                "default": True
                            },
                            "include_network_activity": {
                                "type": "boolean",
                                "description": "Include network connections and communication analysis",
                                "default": True
                            },
                            "baseline_comparison": {
                                "type": "boolean",
                                "description": "Compare against known good baseline for anomaly detection",
                                "default": True
                            },
                            "max_processes": {
                                "type": "integer",
                                "description": "Maximum number of processes to analyze",
                                "default": 500,
                                "minimum": 1,
                                "maximum": 2000
                            }
                        },
                        "required": ["agent_id"]
                    }
                ),
                types.Tool(
                    name="get_agent_ports",
                    description="Identify exposed services and potential backdoors with comprehensive network port analysis and threat detection",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "agent_id": {
                                "type": "string",
                                "description": "The ID of the agent to query"
                            },
                            "port_state": {
                                "type": "array",
                                "description": "Port states to include in analysis",
                                "items": {
                                    "type": "string",
                                    "enum": ["open", "listening", "established", "closed", "all"]
                                },
                                "default": ["open", "listening"]
                            },
                            "protocol": {
                                "type": "array",
                                "description": "Network protocols to analyze",
                                "items": {
                                    "type": "string",
                                    "enum": ["tcp", "udp", "all"]
                                },
                                "default": ["tcp", "udp"]
                            },
                            "include_process": {
                                "type": "boolean",
                                "description": "Include process information for each port",
                                "default": True
                            },
                            "known_services_only": {
                                "type": "boolean",
                                "description": "Show only well-known service ports",
                                "default": False
                            },
                            "exposure_analysis": {
                                "type": "boolean",
                                "description": "Enable exposure risk analysis and scoring",
                                "default": True
                            },
                            "backdoor_detection": {
                                "type": "boolean",
                                "description": "Enable backdoor and C2 detection analysis",
                                "default": True
                            },
                            "baseline_comparison": {
                                "type": "boolean",
                                "description": "Compare against known port baseline",
                                "default": True
                            },
                            "include_firewall_analysis": {
                                "type": "boolean",
                                "description": "Include firewall rule correlation analysis",
                                "default": True
                            },
                            "threat_intelligence": {
                                "type": "boolean",
                                "description": "Enable threat intelligence correlation",
                                "default": True
                            },
                            "max_ports": {
                                "type": "integer",
                                "description": "Maximum number of ports to analyze",
                                "default": 1000,
                                "minimum": 1,
                                "maximum": 5000
                            }
                        },
                        "required": ["agent_id"]
                    }
                ),
                types.Tool(
                    name="get_wazuh_stats",
                    description="Query specific statistical information from the Wazuh manager or a specific agent.",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "component": {
                                "type": "string",
                                "description": "The component to query.",
                                "enum": ["manager", "agent"]
                            },
                            "stat_type": {
                                "type": "string",
                                "description": "The type of statistics to retrieve.",
                                "enum": ["weekly", "log_collector", "remoted"]
                            },
                            "agent_id": {
                                "type": "string",
                                "description": "Required if component is agent."
                            }
                        },
                        "required": ["component", "stat_type"]
                    }
                ),
                types.Tool(
                    name="search_wazuh_logs",
                    description="Search the Wazuh manager or agent logs for specific patterns.",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "log_source": {
                                "type": "string",
                                "description": "The source of the logs.",
                                "enum": ["manager", "agent"]
                            },
                            "agent_id": {
                                "type": "string",
                                "description": "Required if log_source is agent."
                            },
                            "query": {
                                "type": "string",
                                "description": "The search query or pattern."
                            },
                            "limit": {
                                "type": "integer",
                                "description": "The maximum number of log entries to return.",
                                "default": 100
                            }
                        },
                        "required": ["log_source", "query"]
                    }
                ),
                types.Tool(
                    name="search_wazuh_manager_logs",
                    description="Deep forensic investigation and timeline reconstruction of Wazuh manager logs with regex support and correlation analysis",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "search_pattern": {
                                "type": "string",
                                "description": "Search pattern (regex supported for advanced searches)",
                                "default": ".*"
                            },
                            "log_types": {
                                "type": "array",
                                "description": "Types of logs to search",
                                "items": {
                                    "type": "string",
                                    "enum": ["ossec", "api", "cluster", "analysisd", "remoted", "all"]
                                },
                                "default": ["ossec"]
                            },
                            "time_range": {
                                "type": "string",
                                "description": "Time range for log search (1h, 24h, 7d, custom)",
                                "default": "24h"
                            },
                            "start_time": {
                                "type": "string",
                                "description": "Start time for custom range (ISO 8601 format)"
                            },
                            "end_time": {
                                "type": "string",
                                "description": "End time for custom range (ISO 8601 format)"
                            },
                            "context_lines": {
                                "type": "integer",
                                "description": "Number of lines before/after match for context",
                                "default": 3,
                                "minimum": 0,
                                "maximum": 20
                            },
                            "case_sensitive": {
                                "type": "boolean",
                                "description": "Whether search should be case sensitive",
                                "default": false
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "Maximum number of matching lines to return",
                                "default": 100,
                                "minimum": 1,
                                "maximum": 10000
                            },
                            "include_forensics": {
                                "type": "boolean",
                                "description": "Include forensic analysis (timeline, correlations, IoC matching)",
                                "default": true
                            },
                            "correlation_window": {
                                "type": "integer",
                                "description": "Time window in minutes for event correlation",
                                "default": 30,
                                "minimum": 1,
                                "maximum": 1440
                            }
                        },
                        "required": ["search_pattern"]
                    }
                ),
                types.Tool(
                    name="get_wazuh_manager_error_logs",
                    description="System compromise and failure detection through comprehensive error log analysis with root cause detection",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "error_level": {
                                "type": "array",
                                "description": "Error levels to include",
                                "items": {
                                    "type": "string",
                                    "enum": ["ERROR", "CRITICAL", "WARNING", "FATAL"]
                                },
                                "default": ["ERROR", "CRITICAL"]
                            },
                            "time_range": {
                                "type": "string",
                                "description": "Time range for error analysis",
                                "default": "24h"
                            },
                            "start_time": {
                                "type": "string",
                                "description": "Start time for custom range (ISO 8601 format)"
                            },
                            "end_time": {
                                "type": "string",
                                "description": "End time for custom range (ISO 8601 format)"
                            },
                            "component_filter": {
                                "type": "array",
                                "description": "Wazuh components to focus on",
                                "items": {
                                    "type": "string",
                                    "enum": ["analysisd", "remoted", "manager", "api", "cluster", "logcollector"]
                                }
                            },
                            "pattern_filter": {
                                "type": "string",
                                "description": "Additional pattern filter for error messages"
                            },
                            "include_analysis": {
                                "type": "boolean",
                                "description": "Include root cause analysis and remediation suggestions",
                                "default": true
                            },
                            "include_trends": {
                                "type": "boolean",
                                "description": "Include error trend analysis and spike detection",
                                "default": true
                            },
                            "correlation_analysis": {
                                "type": "boolean",
                                "description": "Enable error correlation and impact assessment",
                                "default": true
                            },
                            "max_errors": {
                                "type": "integer",
                                "description": "Maximum number of errors to analyze",
                                "default": 500,
                                "minimum": 1,
                                "maximum": 5000
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_cluster_health",
                    description="Retrieve the overall health and status of the Wazuh cluster, including node information.",
                    inputSchema={
                        "type": "object",
                        "properties": {}
                    }
                ),
                types.Tool(
                    name="get_wazuh_alert_summary",
                    description="Comprehensive alert analysis with full temporal context, statistical analysis, and trend detection",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "time_range": {
                                "type": "string",
                                "description": "Time range for alert analysis",
                                "enum": ["1h", "6h", "12h", "24h", "7d", "30d", "custom"],
                                "default": "24h"
                            },
                            "custom_start": {
                                "type": "string",
                                "description": "Custom start time (ISO format, required if time_range is 'custom')",
                                "pattern": "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}"
                            },
                            "custom_end": {
                                "type": "string",
                                "description": "Custom end time (ISO format, required if time_range is 'custom')",
                                "pattern": "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}"
                            },
                            "severity_filter": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["critical", "high", "medium", "low"]
                                },
                                "description": "Filter by alert severity levels"
                            },
                            "agent_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific agent IDs or names"
                            },
                            "rule_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by rule IDs or patterns"
                            },
                            "group_by": {
                                "type": "string",
                                "description": "Group alerts by specific field",
                                "enum": ["rule", "agent", "severity", "time", "source_ip"],
                                "default": "severity"
                            },
                            "include_stats": {
                                "type": "boolean",
                                "description": "Include statistical analysis (mean, std dev, outliers)",
                                "default": True
                            },
                            "include_trends": {
                                "type": "boolean",
                                "description": "Include trend detection and pattern analysis",
                                "default": True
                            },
                            "max_alerts": {
                                "type": "integer",
                                "description": "Maximum number of alerts to analyze",
                                "default": 1000,
                                "minimum": 100,
                                "maximum": 10000
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_vulnerability_summary",
                    description="Comprehensive vulnerability analysis across entire infrastructure with risk prioritization",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "cvss_threshold": {
                                "type": "number",
                                "description": "Minimum CVSS score (0.0-10.0)",
                                "default": 0.0,
                                "minimum": 0.0,
                                "maximum": 10.0
                            },
                            "severity_filter": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["critical", "high", "medium", "low"]
                                },
                                "description": "Filter by vulnerability severity levels"
                            },
                            "cve_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific CVE identifiers"
                            },
                            "os_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by operating systems"
                            },
                            "package_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by package names"
                            },
                            "exploitability": {
                                "type": "boolean",
                                "description": "Filter to vulnerabilities with known exploits only",
                                "default": false
                            },
                            "group_by": {
                                "type": "string",
                                "description": "Group vulnerabilities by specific field",
                                "enum": ["agent", "severity", "package", "cve", "os"],
                                "default": "severity"
                            },
                            "include_remediation": {
                                "type": "boolean",
                                "description": "Include remediation recommendations",
                                "default": true
                            },
                            "include_analytics": {
                                "type": "boolean",
                                "description": "Include risk analytics and trends",
                                "default": true
                            },
                            "max_agents": {
                                "type": "integer",
                                "description": "Maximum number of agents to analyze",
                                "default": 100,
                                "minimum": 1,
                                "maximum": 1000
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_critical_vulnerabilities",
                    description="Focus on highest-risk vulnerabilities with exploit availability and exposure analysis",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "min_cvss": {
                                "type": "number",
                                "description": "Minimum CVSS score (default: 9.0 for critical)",
                                "default": 9.0,
                                "minimum": 0.0,
                                "maximum": 10.0
                            },
                            "exploit_required": {
                                "type": "boolean",
                                "description": "Only show vulnerabilities with known exploits",
                                "default": true
                            },
                            "internet_exposed": {
                                "type": "boolean",
                                "description": "Filter by internet exposure status",
                                "default": false
                            },
                            "patch_available": {
                                "type": "boolean",
                                "description": "Only show vulnerabilities with available patches",
                                "default": false
                            },
                            "age_days": {
                                "type": "integer",
                                "description": "Maximum vulnerability age in days",
                                "minimum": 0
                            },
                            "affected_services": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by critical services (web, database, api, payment, etc.)"
                            },
                            "include_context": {
                                "type": "boolean",
                                "description": "Include network and process context for each vulnerability",
                                "default": true
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "Maximum results to return",
                                "default": 100,
                                "minimum": 1,
                                "maximum": 500
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_running_agents",
                    description="Real-time visibility of active security infrastructure with comprehensive agent analysis",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "status_filter": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["active", "disconnected", "never_connected", "pending"]
                                },
                                "description": "Filter by agent status"
                            },
                            "os_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by operating systems"
                            },
                            "version_filter": {
                                "type": "string",
                                "description": "Filter by agent version"
                            },
                            "group_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by agent groups"
                            },
                            "inactive_threshold": {
                                "type": "integer",
                                "description": "Threshold in seconds to consider agent inactive",
                                "default": 300,
                                "minimum": 60,
                                "maximum": 3600
                            },
                            "include_disconnected": {
                                "type": "boolean",
                                "description": "Include disconnected agents in analysis",
                                "default": false
                            },
                            "include_health_metrics": {
                                "type": "boolean",
                                "description": "Include health and performance metrics",
                                "default": true
                            },
                            "include_last_activity": {
                                "type": "boolean",
                                "description": "Include last activity analysis",
                                "default": true
                            },
                            "group_by": {
                                "type": "string",
                                "description": "Group agents by specific field",
                                "enum": ["status", "os", "version", "group", "node", "location"],
                                "default": "status"
                            },
                            "max_agents": {
                                "type": "integer",
                                "description": "Maximum agents to analyze",
                                "default": 1000,
                                "minimum": 1,
                                "maximum": 5000
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_rules_summary",
                    description="Comprehensive rules analysis with usage statistics and coverage assessment",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "rule_level_filter": {
                                "type": "array",
                                "items": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "maximum": 16
                                },
                                "description": "Filter by rule levels (0-16)"
                            },
                            "rule_group_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by rule groups (authentication, firewall, etc.)"
                            },
                            "rule_id_filter": {
                                "type": "array",
                                "items": {"type": "integer"},
                                "description": "Filter by specific rule IDs"
                            },
                            "category_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by rule categories"
                            },
                            "status_filter": {
                                "type": "string",
                                "enum": ["enabled", "disabled", "all"],
                                "default": "enabled",
                                "description": "Rule status filter"
                            },
                            "include_disabled": {
                                "type": "boolean",
                                "default": false,
                                "description": "Include disabled rules in analysis"
                            },
                            "include_usage_stats": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include rule usage statistics"
                            },
                            "include_coverage_analysis": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include security coverage analysis"
                            },
                            "group_by": {
                                "type": "string",
                                "enum": ["level", "group", "category", "file", "status"],
                                "default": "level",
                                "description": "Group rules by specific field"
                            },
                            "sort_by": {
                                "type": "string",
                                "enum": ["level", "id", "group", "frequency", "file"],
                                "default": "level",
                                "description": "Sort rules by specific field"
                            },
                            "max_rules": {
                                "type": "integer",
                                "default": 1000,
                                "minimum": 10,
                                "maximum": 10000,
                                "description": "Maximum rules to analyze"
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_weekly_stats",
                    description="Advanced weekly security metrics with statistical analysis, anomaly detection, and behavioral pattern recognition",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "weeks": {
                                "type": "integer",
                                "description": "Number of weeks to analyze (1-12)",
                                "default": 1,
                                "minimum": 1,
                                "maximum": 12
                            },
                            "start_date": {
                                "type": "string",
                                "description": "Custom start date (YYYY-MM-DD format)",
                                "pattern": "^\\d{4}-\\d{2}-\\d{2}$"
                            },
                            "metrics": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["alerts", "events", "agents", "rules", "compliance", 
                                            "vulnerabilities", "authentication", "network", "files"]
                                },
                                "description": "Specific metrics to include"
                            },
                            "include_trends": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include trend analysis"
                            },
                            "include_comparison": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include week-over-week comparison"
                            },
                            "include_forecasting": {
                                "type": "boolean",
                                "default": false,
                                "description": "Include basic forecasting"
                            },
                            "include_predictions": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include predictive analysis and future trend modeling"
                            },
                            "anomaly_detection": {
                                "type": "boolean",
                                "default": true,
                                "description": "Enable anomaly detection algorithms"
                            },
                            "seasonality_analysis": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include seasonality pattern detection"
                            },
                            "behavioral_analysis": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include behavioral pattern analysis and change detection"
                            },
                            "statistical_analysis": {
                                "type": "boolean",
                                "default": true,
                                "description": "Include statistical metrics (mean, std dev, outliers)"
                            },
                            "compare_weeks": {
                                "type": "integer",
                                "description": "Number of weeks for comparison baseline",
                                "default": 4,
                                "minimum": 1,
                                "maximum": 52
                            },
                            "anomaly_threshold": {
                                "type": "number",
                                "description": "Anomaly detection threshold (standard deviations)",
                                "default": 2.0,
                                "minimum": 1.0,
                                "maximum": 5.0
                            },
                            "group_by": {
                                "type": "string",
                                "enum": ["hour", "day", "week"],
                                "default": "day",
                                "description": "Grouping granularity"
                            },
                            "agent_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific agent IDs or names"
                            },
                            "rule_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific rule IDs"
                            },
                            "output_format": {
                                "type": "string",
                                "enum": ["detailed", "summary", "minimal"],
                                "default": "detailed",
                                "description": "Output format"
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_remoted_stats",
                    description="Advanced remote daemon statistics with communication health metrics, performance monitoring, and reliability analysis",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "time_range": {
                                "type": "string",
                                "description": "Time range for analysis",
                                "enum": ["1h", "6h", "12h", "24h", "7d", "30d"],
                                "default": "24h"
                            },
                            "node_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific node names"
                            },
                            "include_performance": {
                                "type": "boolean",
                                "description": "Include performance metrics (CPU, memory)",
                                "default": true
                            },
                            "include_connections": {
                                "type": "boolean",
                                "description": "Include connection statistics",
                                "default": true
                            },
                            "include_events": {
                                "type": "boolean",
                                "description": "Include event processing metrics",
                                "default": true
                            },
                            "include_queues": {
                                "type": "boolean",
                                "description": "Include queue statistics and analysis",
                                "default": true
                            },
                            "include_errors": {
                                "type": "boolean",
                                "description": "Include error analysis and troubleshooting",
                                "default": true
                            },
                            "include_trends": {
                                "type": "boolean",
                                "description": "Include trend analysis over time",
                                "default": true
                            },
                            "include_communication_metrics": {
                                "type": "boolean",
                                "description": "Include communication health metrics (throughput, latency, timeouts)",
                                "default": true
                            },
                            "include_health_monitoring": {
                                "type": "boolean",
                                "description": "Include health monitoring and diagnostic analysis",
                                "default": true
                            },
                            "include_throughput_analysis": {
                                "type": "boolean",
                                "description": "Include throughput and latency analysis",
                                "default": true
                            },
                            "include_reliability_scoring": {
                                "type": "boolean",
                                "description": "Include reliability and availability scoring",
                                "default": true
                            },
                            "include_diagnostics": {
                                "type": "boolean",
                                "description": "Include troubleshooting diagnostics and optimization suggestions",
                                "default": true
                            },
                            "include_capacity_planning": {
                                "type": "boolean",
                                "description": "Include capacity planning metrics and scaling recommendations",
                                "default": true
                            },
                            "group_by": {
                                "type": "string",
                                "description": "Grouping field for analysis",
                                "enum": ["node", "connection_type", "event_type", "status"],
                                "default": "node"
                            },
                            "output_format": {
                                "type": "string",
                                "description": "Output format",
                                "enum": ["detailed", "summary", "minimal"],
                                "default": "detailed"
                            },
                            "threshold_cpu": {
                                "type": "number",
                                "description": "CPU usage threshold for alerting",
                                "minimum": 0.0,
                                "maximum": 100.0,
                                "default": 80.0
                            },
                            "threshold_memory": {
                                "type": "number",
                                "description": "Memory usage threshold for alerting",
                                "minimum": 0.0,
                                "maximum": 100.0,
                                "default": 80.0
                            },
                            "threshold_queue": {
                                "type": "integer",
                                "description": "Queue size threshold for alerting",
                                "minimum": 0,
                                "default": 1000
                            },
                            "threshold_latency": {
                                "type": "number",
                                "description": "Latency threshold in seconds for alerting",
                                "minimum": 0.0,
                                "maximum": 60.0,
                                "default": 5.0
                            },
                            "threshold_error_rate": {
                                "type": "number",
                                "description": "Error rate threshold percentage for alerting",
                                "minimum": 0.0,
                                "maximum": 100.0,
                                "default": 5.0
                            },
                            "alert_on_anomalies": {
                                "type": "boolean",
                                "description": "Generate alerts for detected anomalies",
                                "default": true
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_log_collector_stats",
                    description="Log collector performance and file monitoring analysis across Wazuh agents and managers",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "time_range": {
                                "type": "string",
                                "description": "Time range for analysis",
                                "enum": ["1h", "6h", "12h", "24h", "7d", "30d"],
                                "default": "24h"
                            },
                            "node_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific node names"
                            },
                            "agent_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by specific agent IDs or names"
                            },
                            "log_type_filter": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Filter by log types (apache, nginx, syslog, etc.)"
                            },
                            "include_performance": {
                                "type": "boolean",
                                "description": "Include performance metrics and processing rates",
                                "default": True
                            },
                            "include_file_monitoring": {
                                "type": "boolean",
                                "description": "Include file monitoring statistics and lag analysis",
                                "default": True
                            },
                            "include_processing_stats": {
                                "type": "boolean",
                                "description": "Include log processing and parsing metrics",
                                "default": True
                            },
                            "include_error_analysis": {
                                "type": "boolean",
                                "description": "Include error analysis and troubleshooting",
                                "default": True
                            },
                            "include_efficiency": {
                                "type": "boolean",
                                "description": "Include efficiency analysis and optimization suggestions",
                                "default": True
                            },
                            "include_trends": {
                                "type": "boolean",
                                "description": "Include trend analysis over time",
                                "default": True
                            },
                            "include_coverage_analysis": {
                                "type": "boolean",
                                "description": "Include coverage analysis with compliance mapping",
                                "default": True
                            },
                            "include_resource_monitoring": {
                                "type": "boolean",
                                "description": "Include resource usage tracking and monitoring",
                                "default": True
                            },
                            "include_bottleneck_detection": {
                                "type": "boolean",
                                "description": "Include bottleneck detection and optimization hints",
                                "default": True
                            },
                            "include_capacity_planning": {
                                "type": "boolean",
                                "description": "Include capacity planning metrics and scaling recommendations",
                                "default": True
                            },
                            "compliance_frameworks": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Compliance frameworks to analyze (PCI, SOX, HIPAA, GDPR, etc.)"
                            },
                            "group_by": {
                                "type": "string",
                                "description": "Grouping field for analysis",
                                "enum": ["node", "agent", "log_type", "status", "performance"],
                                "default": "node"
                            },
                            "output_format": {
                                "type": "string",
                                "description": "Output format",
                                "enum": ["detailed", "summary", "minimal"],
                                "default": "detailed"
                            },
                            "threshold_processing_rate": {
                                "type": "integer",
                                "description": "Minimum logs per second threshold for alerting",
                                "minimum": 0,
                                "default": 1000
                            },
                            "threshold_error_rate": {
                                "type": "number",
                                "description": "Maximum error rate threshold (percentage)",
                                "minimum": 0.0,
                                "maximum": 100.0,
                                "default": 5.0
                            },
                            "threshold_file_lag": {
                                "type": "integer",
                                "description": "Maximum file monitoring lag threshold in seconds",
                                "minimum": 0,
                                "default": 300
                            },
                            "threshold_resource_usage": {
                                "type": "number",
                                "description": "Resource usage threshold for alerting (percentage)",
                                "minimum": 0.0,
                                "maximum": 100.0,
                                "default": 80.0
                            },
                            "coverage_threshold": {
                                "type": "number",
                                "description": "Minimum coverage threshold percentage",
                                "minimum": 0.0,
                                "maximum": 100.0,
                                "default": 90.0
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_cluster_health",
                    description="Comprehensive cluster health analysis including node status, performance metrics, and connectivity",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "include_node_details": {
                                "type": "boolean",
                                "description": "Include detailed node information and specifications",
                                "default": True
                            },
                            "include_performance": {
                                "type": "boolean",
                                "description": "Include performance metrics (CPU, memory, disk)",
                                "default": True
                            },
                            "include_connectivity": {
                                "type": "boolean",
                                "description": "Include connectivity analysis between nodes",
                                "default": True
                            },
                            "include_resource_usage": {
                                "type": "boolean",
                                "description": "Include resource utilization analysis",
                                "default": True
                            },
                            "include_service_status": {
                                "type": "boolean",
                                "description": "Include service status checks for all components",
                                "default": True
                            },
                            "include_disk_usage": {
                                "type": "boolean",
                                "description": "Include disk usage analysis and alerts",
                                "default": True
                            },
                            "include_network_stats": {
                                "type": "boolean",
                                "description": "Include network statistics and performance",
                                "default": True
                            },
                            "include_recommendations": {
                                "type": "boolean",
                                "description": "Include health recommendations and optimization suggestions",
                                "default": True
                            },
                            "include_diagnostics": {
                                "type": "boolean", 
                                "description": "Include comprehensive diagnostics and troubleshooting",
                                "default": True
                            },
                            "include_failure_prediction": {
                                "type": "boolean",
                                "description": "Include failure prediction and trending analysis",
                                "default": True
                            },
                            "include_sync_monitoring": {
                                "type": "boolean",
                                "description": "Include synchronization monitoring and analysis",
                                "default": True
                            },
                            "include_root_cause_analysis": {
                                "type": "boolean",
                                "description": "Include root cause analysis for detected issues",
                                "default": True
                            },
                            "include_remediation_steps": {
                                "type": "boolean",
                                "description": "Include detailed remediation steps and guides",
                                "default": True
                            },
                            "include_health_trending": {
                                "type": "boolean",
                                "description": "Include health trending analysis over time",
                                "default": True
                            },
                            "include_predictive_alerts": {
                                "type": "boolean",
                                "description": "Include predictive alerts and early warnings",
                                "default": True
                            },
                            "health_threshold_cpu": {
                                "type": "number",
                                "description": "CPU usage threshold for health alerts",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 80.0
                            },
                            "health_threshold_memory": {
                                "type": "number",
                                "description": "Memory usage threshold for health alerts",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 85.0
                            },
                            "health_threshold_disk": {
                                "type": "number",
                                "description": "Disk usage threshold for health alerts",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 90.0
                            },
                            "sync_lag_threshold": {
                                "type": "integer",
                                "description": "Sync lag threshold in seconds for alerts",
                                "minimum": 1,
                                "maximum": 300,
                                "default": 30
                            },
                            "connectivity_timeout": {
                                "type": "integer",
                                "description": "Timeout for connectivity checks in seconds",
                                "minimum": 1,
                                "maximum": 60,
                                "default": 5
                            },
                            "prediction_window_hours": {
                                "type": "integer",
                                "description": "Prediction window in hours for failure prediction",
                                "minimum": 1,
                                "maximum": 168,
                                "default": 24
                            },
                            "alert_escalation_threshold": {
                                "type": "integer",
                                "description": "Alert escalation threshold count",
                                "minimum": 1,
                                "maximum": 10,
                                "default": 3
                            },
                            "output_format": {
                                "type": "string",
                                "description": "Output format level",
                                "enum": ["detailed", "summary", "minimal"],
                                "default": "detailed"
                            }
                        }
                    }
                ),
                types.Tool(
                    name="get_wazuh_cluster_nodes",
                    description="Individual node monitoring and management with comprehensive performance tracking",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "node_type": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["master", "worker", "all"]
                                },
                                "description": "Filter by node type",
                                "default": ["all"]
                            },
                            "status_filter": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["active", "inactive", "disconnected", "all"]
                                },
                                "description": "Filter by node status",
                                "default": ["all"]
                            },
                            "node_name": {
                                "type": "string",
                                "description": "Filter by specific node name"
                            },
                            "include_performance": {
                                "type": "boolean",
                                "description": "Include performance metrics (CPU, memory, disk)",
                                "default": true
                            },
                            "include_configuration": {
                                "type": "boolean",
                                "description": "Include node configuration details",
                                "default": false
                            },
                            "include_sync_status": {
                                "type": "boolean",
                                "description": "Include synchronization status and integrity checks",
                                "default": true
                            },
                            "include_load_metrics": {
                                "type": "boolean",
                                "description": "Include load balancing and capacity metrics",
                                "default": true
                            },
                            "include_agent_distribution": {
                                "type": "boolean",
                                "description": "Include agent distribution across nodes",
                                "default": true
                            },
                            "performance_threshold_cpu": {
                                "type": "number",
                                "description": "CPU usage threshold for performance alerts",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 80.0
                            },
                            "performance_threshold_memory": {
                                "type": "number",
                                "description": "Memory usage threshold for performance alerts",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 85.0
                            },
                            "sync_lag_threshold": {
                                "type": "integer",
                                "description": "Synchronization lag threshold in seconds",
                                "minimum": 0,
                                "maximum": 600,
                                "default": 30
                            },
                            "output_format": {
                                "type": "string",
                                "description": "Output format level",
                                "enum": ["detailed", "summary", "minimal"],
                                "default": "detailed"
                            }
                        }
                    }
                )
            ]
        
        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
            """Execute Wazuh tools with comprehensive validation and error handling."""
            request_id = str(uuid.uuid4())
            start_time = datetime.utcnow()
            
            try:
                with LogContext(request_id):
                    self.logger.info(f"Executing tool: {name}", extra={"details": arguments})
                    
                    # Apply timeout to all tool executions
                    timeout = self.config.request_timeout_seconds * 2  # Double timeout for complex operations
                    
                    # First check if the tool can be handled by the new tool factory
                    if self.tool_factory and self.tool_factory.is_tool_available(name):
                        result = await asyncio.wait_for(
                            self.tool_factory.handle_tool_call(name, arguments), 
                            timeout=timeout
                        )
                    # Legacy tool handling for backward compatibility
                    elif name == "get_alerts":
                        result = await asyncio.wait_for(self._handle_get_alerts(arguments), timeout=timeout)
                    elif name == "analyze_threats":
                        result = await asyncio.wait_for(self._handle_analyze_threats(arguments), timeout=timeout)
                    elif name == "check_agent_health":
                        result = await asyncio.wait_for(self._handle_check_agent_health(arguments), timeout=timeout)
                    elif name == "compliance_check":
                        result = await asyncio.wait_for(self._handle_compliance_check(arguments), timeout=timeout)
                    elif name == "check_ioc":
                        result = await asyncio.wait_for(self._handle_check_ioc(arguments), timeout=timeout)
                    elif name == "risk_assessment":
                        result = await asyncio.wait_for(self._handle_risk_assessment(arguments), timeout=timeout)
                    elif name == "get_agent_processes":
                        result = await asyncio.wait_for(self._handle_get_agent_processes(arguments), timeout=timeout)
                    elif name == "get_agent_ports":
                        result = await asyncio.wait_for(self._handle_get_agent_ports(arguments), timeout=timeout)
                    elif name == "get_wazuh_stats":
                        result = await asyncio.wait_for(self._handle_get_wazuh_stats(arguments), timeout=timeout)
                    elif name == "search_wazuh_logs":
                        result = await asyncio.wait_for(self._handle_search_wazuh_logs(arguments), timeout=timeout)
                    elif name == "search_wazuh_manager_logs":
                        result = await asyncio.wait_for(self._handle_search_wazuh_manager_logs(arguments), timeout=timeout)
                    elif name == "get_wazuh_manager_error_logs":
                        result = await asyncio.wait_for(self._handle_get_wazuh_manager_error_logs(arguments), timeout=timeout)
                    elif name == "get_cluster_health":
                        result = await asyncio.wait_for(self._handle_get_cluster_health(arguments), timeout=timeout)
                    else:
                        raise ValueError(f"Unknown tool: {name}")
                    
                    # Apply context enhancement if enabled (Phase 5)
                    if self.context_aggregator and self.context_aggregator.is_enabled():
                        try:
                            # Extract prompt from arguments if available (would need to be passed from client)
                            prompt = arguments.get('_prompt', '')
                            enhanced_result = await self.context_aggregator.enhance_response(
                                name, arguments, result, prompt
                            )
                            result = enhanced_result
                        except Exception as e:
                            # Enhancement failures should not break the tool execution
                            self.logger.debug(f"Context enhancement failed for {name}: {str(e)}")
                    
                    execution_time = (datetime.utcnow() - start_time).total_seconds()
                    self.logger.info(f"Tool {name} completed in {execution_time:.2f}s")
                    return result
                        
            except asyncio.TimeoutError:
                execution_time = (datetime.utcnow() - start_time).total_seconds()
                self.logger.error(f"Tool {name} timed out after {execution_time:.2f}s")
                return [types.TextContent(
                    type="text",
                    text=self._format_error_response(
                        asyncio.TimeoutError(f"Tool execution timed out after {timeout} seconds"),
                        request_id,
                        execution_time
                    )
                )]
            except ValidationError as e:
                self.logger.warning(f"Validation error in tool {name}: {str(e)}")
                return [types.TextContent(
                    type="text",
                    text=self._format_error_response(e, request_id)
                )]
            except Exception as e:
                execution_time = (datetime.utcnow() - start_time).total_seconds()
                self.logger.error(f"Error executing tool {name} after {execution_time:.2f}s: {str(e)}")
                return [types.TextContent(
                    type="text",
                    text=self._format_error_response(e, request_id, execution_time)
                )]
    
    async def _handle_get_alerts(self, arguments: dict) -> list[types.TextContent]:
        """Handle get_alerts tool execution."""
        # Validate parameters
        validated_query = validate_alert_query(arguments)
        
        data = await self.api_client.get_alerts(
            limit=validated_query.limit,
            level=arguments.get("level"),
            time_range=arguments.get("time_range"),
            agent_id=arguments.get("agent_id")
        )
        
        # Check for critical alerts and send notifications
        alerts = data.get("data", {}).get("affected_items", [])
        for alert in alerts:
            await self._send_critical_alert_notification(alert)
        
        formatted = self._format_alerts(data)
        
        return [types.TextContent(
            type="text",
            text=json.dumps(formatted, indent=2)
        )]
    
    async def _handle_analyze_threats(self, arguments: dict) -> list[types.TextContent]:
        """Handle threat analysis tool execution."""
        validated_query = validate_threat_analysis(arguments)
        
        # Get alerts for analysis
        alerts_data = await self.api_client.get_alerts(
            limit=1000, 
            time_range=validated_query.time_range
        )
        alerts = alerts_data.get("data", {}).get("affected_items", [])
        
        # Perform risk assessment
        risk_assessment = self.security_analyzer.calculate_comprehensive_risk_score(
            alerts, time_window_hours=validated_query.time_range // 3600
        )
        
        analysis = {
            "category": validated_query.category,
            "total_alerts": len(alerts),
            "time_range_seconds": validated_query.time_range,
            "risk_assessment": {
                "overall_score": risk_assessment.overall_score,
                "risk_level": risk_assessment.risk_level.value,
                "confidence": risk_assessment.confidence,
                "factors": [
                    {
                        "name": factor.name,
                        "score": factor.score,
                        "weight": factor.weight,
                        "description": factor.description
                    }
                    for factor in risk_assessment.factors
                ],
                "recommendations": risk_assessment.recommendations
            },
            "timestamp": risk_assessment.timestamp.isoformat()
        }
        
        # Include attack patterns if requested
        if arguments.get("include_patterns", False):
            patterns = self.security_analyzer.detect_attack_patterns(alerts)
            analysis["attack_patterns"] = patterns
        
        return [types.TextContent(
            type="text",
            text=json.dumps(analysis, indent=2)
        )]
    
    async def _handle_check_agent_health(self, arguments: dict) -> list[types.TextContent]:
        """Handle agent health check tool execution."""
        agent_id = arguments.get("agent_id")
        include_stats = arguments.get("include_stats", False)
        
        if agent_id:
            # Validate agent query
            validated_query = validate_agent_query({"agent_id": agent_id})
            
            # Get specific agent data
            agents_data = await self.api_client.get_agents()
            agents = agents_data.get("data", {}).get("affected_items", [])
            agent = next((a for a in agents if a.get("id") == agent_id), None)
            
            if not agent:
                return [types.TextContent(
                    type="text",
                    text=self._format_error_response(ValueError(f"Agent ID {agent_id} not found"))
                )]
            
            health = self._assess_agent_health(agent)
            
            if include_stats:
                try:
                    stats = await self.api_client.get_agent_stats(agent_id)
                    health["statistics"] = stats.get("data", {})
                except Exception as e:
                    health["statistics_error"] = str(e)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(health, indent=2)
            )]
        else:
            # Get all agents health
            data = await self.api_client.get_agents()
            health_report = self._assess_all_agents_health(data)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(health_report, indent=2)
            )]
    
    async def _handle_compliance_check(self, arguments: dict) -> list[types.TextContent]:
        """Handle compliance check tool execution."""
        framework_str = arguments.get("framework", "pci_dss")
        include_evidence = arguments.get("include_evidence", True)
        
        # Map string to enum
        framework_map = {
            "pci_dss": ComplianceFramework.PCI_DSS,
            "hipaa": ComplianceFramework.HIPAA,
            "gdpr": ComplianceFramework.GDPR,
            "nist": ComplianceFramework.NIST,
            "iso27001": ComplianceFramework.ISO27001
        }
        
        framework = framework_map.get(framework_str, ComplianceFramework.PCI_DSS)
        
        # Gather data for compliance assessment
        alerts_data = await self.api_client.get_alerts(limit=1000)
        alerts = alerts_data.get("data", {}).get("affected_items", [])
        
        agents_data = await self.api_client.get_agents()
        agents = agents_data.get("data", {}).get("affected_items", [])
        
        # Get vulnerabilities for a sample of agents
        vulnerabilities = []
        active_agents = [a for a in agents if a.get("status") == "active"][:5]  # Sample 5 agents
        
        for agent in active_agents:
            try:
                vuln_data = await self.api_client.get_agent_vulnerabilities(agent["id"])
                agent_vulns = vuln_data.get("data", {}).get("affected_items", [])
                vulnerabilities.extend(agent_vulns)
            except Exception as e:
                self.logger.warning(f"Could not get vulnerabilities for agent {agent['id']}: {str(e)}")
        
        # Perform compliance assessment
        report = self.compliance_analyzer.assess_compliance(
            framework, alerts, agents, vulnerabilities
        )
        
        # Format response
        compliance_result = {
            "framework": framework.value,
            "overall_score": report.overall_score,
            "status": report.status.value,
            "summary": report.summary,
            "timestamp": report.timestamp.isoformat()
        }
        
        if include_evidence:
            compliance_result["requirements"] = [
                {
                    "id": req.id,
                    "title": req.title,
                    "description": req.description,
                    "status": req.status.value,
                    "score": req.score,
                    "evidence": req.evidence,
                    "gaps": req.gaps,
                    "recommendations": req.recommendations
                }
                for req in report.requirements
            ]
            compliance_result["recommendations"] = report.recommendations
        
        return [types.TextContent(
            type="text",
            text=json.dumps(compliance_result, indent=2)
        )]
    
    async def _handle_check_ioc(self, arguments: dict) -> list[types.TextContent]:
        """Handle IOC checking tool execution."""
        ip_address = arguments.get("ip_address")
        file_hash = arguments.get("file_hash")
        
        results = {"indicators": []}
        
        if ip_address:
            try:
                validated_ip = validate_ip_address(ip_address)
                results["indicators"].append({
                    "type": "ip_address",
                    "value": validated_ip.ip,
                    "status": "validated",
                    "message": "IP address validated successfully"
                })
            except ValidationError as e:
                results["indicators"].append({
                    "type": "ip_address",
                    "value": ip_address,
                    "status": "error",
                    "message": str(e)
                })
        
        if file_hash:
            try:
                validated_hash = validate_file_hash(file_hash)
                results["indicators"].append({
                    "type": "file_hash",
                    "value": validated_hash.hash_value,
                    "hash_type": validated_hash.hash_type,
                    "status": "validated",
                    "message": "File hash validated successfully"
                })
            except ValidationError as e:
                results["indicators"].append({
                    "type": "file_hash",
                    "value": file_hash,
                    "status": "error",
                    "message": str(e)
                })
        
        if not results["indicators"]:
            results["error"] = "No valid indicators provided."
        
        return [types.TextContent(
            type="text",
            text=json.dumps(results, indent=2)
        )]
    
    async def _handle_risk_assessment(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive risk assessment tool execution."""
        time_window_hours = arguments.get("time_window_hours", 24)
        include_vulnerabilities = arguments.get("include_vulnerabilities", True)
        
        # Initialize progress tracking
        total_steps = 4 if include_vulnerabilities else 2
        current_step = 0
        
        # Step 1: Get alerts for the time window
        current_step += 1
        await self._report_progress(current_step, total_steps, "Fetching security alerts...")
        
        time_range_seconds = time_window_hours * 3600
        alerts_data = await self.api_client.get_alerts(
            limit=2000, 
            time_range=time_range_seconds
        )
        alerts = alerts_data.get("data", {}).get("affected_items", [])
        
        vulnerabilities = []
        if include_vulnerabilities:
            # Step 2: Get agents
            current_step += 1
            await self._report_progress(current_step, total_steps, "Fetching agent information...")
            
            agents_data = await self.api_client.get_agents(status="active")
            active_agents = agents_data.get("data", {}).get("affected_items", [])[:10]  # Sample 10 agents
            
            # Step 3: Get vulnerabilities
            current_step += 1
            await self._report_progress(current_step, total_steps, "Analyzing vulnerabilities...")
            
            for i, agent in enumerate(active_agents):
                try:
                    vuln_data = await self.api_client.get_agent_vulnerabilities(agent["id"])
                    agent_vulns = vuln_data.get("data", {}).get("affected_items", [])
                    vulnerabilities.extend(agent_vulns)
                    
                    # Report sub-progress
                    if i % 2 == 0:  # Report every 2nd agent
                        await self._report_progress(
                            current_step, 
                            total_steps, 
                            f"Analyzing vulnerabilities... ({i+1}/{len(active_agents)} agents processed)"
                        )
                except Exception as e:
                    self.logger.warning(f"Could not get vulnerabilities for agent {agent['id']}: {str(e)}")
        
        # Final step: Perform comprehensive risk assessment
        current_step += 1
        await self._report_progress(current_step, total_steps, "Calculating risk assessment...")
        
        risk_assessment = self.security_analyzer.calculate_comprehensive_risk_score(
            alerts, vulnerabilities, time_window_hours
        )
        
        # Complete progress
        await self._report_progress(total_steps, total_steps, "Risk assessment completed!")
        
        # Format result
        result = {
            "assessment_period": {
                "time_window_hours": time_window_hours,
                "alerts_analyzed": len(alerts),
                "vulnerabilities_analyzed": len(vulnerabilities)
            },
            "risk_score": risk_assessment.overall_score,
            "risk_level": risk_assessment.risk_level.value,
            "confidence": risk_assessment.confidence,
            "factors": [
                {
                    "name": factor.name,
                    "score": factor.score,
                    "weight": factor.weight,
                    "description": factor.description
                }
                for factor in risk_assessment.factors
            ],
            "recommendations": risk_assessment.recommendations,
            "timestamp": risk_assessment.timestamp.isoformat()
        }
        
        return [types.TextContent(
            type="text",
            text=json.dumps(result, indent=2)
        )]

    async def _handle_get_agent_processes(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive agent process analysis with threat detection and behavior analysis."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_agent_processes_query(arguments)
            
            # Fetch process data from agent
            process_analysis = await self._perform_comprehensive_process_analysis(
                validated_query.agent_id,
                validated_query.process_filter,
                validated_query.include_children,
                validated_query.include_hashes,
                validated_query.threat_detection,
                validated_query.include_network_activity,
                validated_query.baseline_comparison,
                validated_query.max_processes
            )
            
            # Generate comprehensive process report
            process_report = await self._generate_process_analysis_report(
                process_analysis, validated_query, start_time
            )
            
            # Filter results if suspicious_only is requested
            if validated_query.suspicious_only:
                process_report = self._filter_suspicious_processes(process_report)
            
            # Sort results by specified metric
            process_report = self._sort_process_results(process_report, validated_query.sort_by)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(process_report, indent=2, default=str)
            )]
            
        except Exception as e:
            self.logger.error(f"Error in agent process analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]

    async def _handle_get_agent_ports(self, arguments: dict) -> list[types.TextContent]:
        """Handle enhanced agent ports analysis with exposure and backdoor detection."""
        try:
            start_time = datetime.utcnow()
            
            # Validate query parameters
            validated_query = validate_agent_ports_query(arguments)
            
            # Perform comprehensive port analysis
            port_report = await self._perform_comprehensive_port_analysis(
                validated_query.agent_id,
                validated_query.port_state,
                validated_query.protocol,
                validated_query.include_process,
                validated_query.known_services_only,
                validated_query.exposure_analysis,
                validated_query.backdoor_detection,
                validated_query.baseline_comparison,
                validated_query.include_firewall_analysis,
                validated_query.threat_intelligence,
                validated_query.max_ports
            )
            
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            port_report["analysis_metadata"]["execution_time_seconds"] = execution_time
            
            return [types.TextContent(
                type="text",
                text=json.dumps(port_report, indent=2, default=str)
            )]
            
        except Exception as e:
            self.logger.error(f"Error in agent ports analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]

    async def _handle_get_wazuh_stats(self, arguments: dict) -> list[types.TextContent]:
        """Handle get_wazuh_stats tool execution."""
        component = arguments.get("component")
        stat_type = arguments.get("stat_type")
        agent_id = arguments.get("agent_id")

        data = await self.api_client.get_wazuh_stats(component, stat_type, agent_id)

        return [types.TextContent(
            type="text",
            text=json.dumps(data, indent=2)
        )]

    async def _handle_search_wazuh_logs(self, arguments: dict) -> list[types.TextContent]:
        """Handle search_wazuh_logs tool execution."""
        log_source = arguments.get("log_source")
        query = arguments.get("query")
        limit = arguments.get("limit", 100)
        agent_id = arguments.get("agent_id")
        
        # Validate required parameters
        if not log_source:
            return [types.TextContent(
                type="text",
                text=self._format_error_response(ValueError("log_source is required"))
            )]
        
        if not query:
            return [types.TextContent(
                type="text",
                text=self._format_error_response(ValueError("query is required"))
            )]

        data = await self.api_client.search_wazuh_logs(log_source, query, limit, agent_id)

        return [types.TextContent(
            type="text",
            text=json.dumps(data, indent=2)
        )]

    async def _handle_search_wazuh_manager_logs(self, arguments: dict) -> list[types.TextContent]:
        """Handle advanced manager logs search with forensic analysis capabilities."""
        import re
        import os
        from datetime import datetime, timedelta
        
        start_time = datetime.utcnow()
        
        try:
            # Extract and validate parameters
            search_pattern = arguments.get("search_pattern", ".*")
            log_types = arguments.get("log_types", ["ossec"])
            time_range = arguments.get("time_range", "24h")
            start_time_str = arguments.get("start_time")
            end_time_str = arguments.get("end_time")
            context_lines = arguments.get("context_lines", 3)
            case_sensitive = arguments.get("case_sensitive", False)
            max_results = arguments.get("max_results", 100)
            include_forensics = arguments.get("include_forensics", True)
            correlation_window = arguments.get("correlation_window", 30)
            
            # Parse time range
            if start_time_str and end_time_str:
                start_dt = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(end_time_str.replace('Z', '+00:00'))
            else:
                end_dt = datetime.utcnow()
                if time_range == "1h":
                    start_dt = end_dt - timedelta(hours=1)
                elif time_range == "24h":
                    start_dt = end_dt - timedelta(hours=24)
                elif time_range == "7d":
                    start_dt = end_dt - timedelta(days=7)
                else:
                    start_dt = end_dt - timedelta(hours=24)  # default
            
            # Compile regex pattern
            try:
                regex_flags = 0 if case_sensitive else re.IGNORECASE
                pattern = re.compile(search_pattern, regex_flags)
            except re.error as e:
                return [types.TextContent(
                    type="text",
                    text=self._format_error_response(ValueError(f"Invalid regex pattern: {str(e)}"))
                )]
            
            # Perform the enhanced log search
            search_results = await self._perform_forensic_log_search(
                pattern, log_types, start_dt, end_dt, context_lines, 
                max_results, include_forensics, correlation_window
            )
            
            # Generate comprehensive forensic report
            forensic_report = await self._generate_forensic_analysis(
                search_results, search_pattern, start_dt, end_dt,
                include_forensics, correlation_window, start_time
            )
            
            return [types.TextContent(
                type="text",
                text=json.dumps(forensic_report, indent=2, default=str)
            )]
            
        except Exception as e:
            self.logger.error(f"Error in manager logs search: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]

    async def _handle_get_wazuh_manager_error_logs(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive error log analysis with root cause detection."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_manager_error_logs_query(arguments)
            
            # Extract validated parameters
            error_levels = validated_query.error_level
            time_range = validated_query.time_range
            start_time_str = validated_query.start_time
            end_time_str = validated_query.end_time
            component_filter = validated_query.component_filter
            pattern_filter = validated_query.pattern_filter
            include_analysis = validated_query.include_analysis
            include_trends = validated_query.include_trends
            correlation_analysis = validated_query.correlation_analysis
            max_errors = validated_query.max_errors
            
            # Parse time range
            if start_time_str and end_time_str:
                start_dt = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
                end_dt = datetime.fromisoformat(end_time_str.replace('Z', '+00:00'))
            else:
                end_dt = datetime.utcnow()
                if time_range == "1h":
                    start_dt = end_dt - timedelta(hours=1)
                elif time_range == "24h":
                    start_dt = end_dt - timedelta(hours=24)
                elif time_range == "7d":
                    start_dt = end_dt - timedelta(days=7)
                else:
                    start_dt = end_dt - timedelta(hours=24)
            
            # Fetch and analyze error logs
            error_analysis = await self._perform_error_log_analysis(
                error_levels, start_dt, end_dt, component_filter, 
                pattern_filter, max_errors, include_analysis,
                include_trends, correlation_analysis
            )
            
            # Generate comprehensive error report
            error_report = await self._generate_error_analysis_report(
                error_analysis, start_dt, end_dt, error_levels,
                include_analysis, include_trends, correlation_analysis, start_time
            )
            
            return [types.TextContent(
                type="text",
                text=json.dumps(error_report, indent=2, default=str)
            )]
            
        except Exception as e:
            self.logger.error(f"Error in manager error logs analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]

    async def _handle_get_cluster_health(self, arguments: dict) -> list[types.TextContent]:
        """Handle get_cluster_health tool execution."""
        data = await self.api_client.get_cluster_info()
        nodes_data = await self.api_client.get_cluster_nodes()

        # Combine cluster info and node info
        cluster_health = {
            "cluster_info": data.get("data", {}),
            "nodes": nodes_data.get("data", {}).get("affected_items", [])
        }

        return [types.TextContent(
            type="text",
            text=json.dumps(cluster_health, indent=2)
        )]
    
    async def _handle_get_wazuh_alert_summary(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive alert summary with advanced analytics."""
        # Validate input parameters
        validated_query = validate_alert_summary_query(arguments)
        
        # Calculate time range parameters
        time_params = self._calculate_time_range(validated_query)
        
        # Fetch alerts with pagination support
        alerts_data = await self._fetch_alerts_with_pagination(
            validated_query, time_params
        )
        
        # Process and analyze alerts
        summary = await self._analyze_alert_data(
            alerts_data, validated_query, time_params
        )
        
        return [types.TextContent(
            type="text",
            text=json.dumps(summary, indent=2, default=str)
        )]
    
    def _calculate_time_range(self, query) -> Dict[str, Any]:
        """Calculate time range parameters from query."""
        from datetime import datetime, timedelta
        
        now = datetime.utcnow()
        
        if query.time_range == "custom":
            # Parse custom times
            start_time = datetime.fromisoformat(query.custom_start.replace('Z', '+00:00'))
            end_time = datetime.fromisoformat(query.custom_end.replace('Z', '+00:00'))
        else:
            # Calculate relative time range
            time_mapping = {
                "1h": timedelta(hours=1),
                "6h": timedelta(hours=6), 
                "12h": timedelta(hours=12),
                "24h": timedelta(days=1),
                "7d": timedelta(days=7),
                "30d": timedelta(days=30)
            }
            start_time = now - time_mapping[query.time_range]
            end_time = now
        
        return {
            "start_time": start_time,
            "end_time": end_time,
            "duration_seconds": int((end_time - start_time).total_seconds()),
            "duration_hours": (end_time - start_time).total_seconds() / 3600
        }
    
    async def _fetch_alerts_with_pagination(self, query, time_params) -> List[Dict[str, Any]]:
        """Fetch alerts with smart pagination to handle large datasets."""
        all_alerts = []
        offset = 0
        batch_size = min(1000, query.max_alerts)
        
        try:
            while len(all_alerts) < query.max_alerts:
                # Calculate remaining alerts needed
                remaining = query.max_alerts - len(all_alerts)
                current_limit = min(batch_size, remaining)
                
                # Fetch batch of alerts
                alerts_response = await self.api_client.get_alerts(
                    limit=current_limit,
                    offset=offset,
                    # Convert time to seconds for API
                    time_range=time_params["duration_seconds"]
                )
                
                batch_alerts = alerts_response.get("data", {}).get("affected_items", [])
                
                # Filter alerts based on criteria
                filtered_alerts = self._filter_alerts(batch_alerts, query, time_params)
                all_alerts.extend(filtered_alerts)
                
                # Check if we got fewer alerts than requested (end of data)
                if len(batch_alerts) < current_limit:
                    break
                    
                offset += current_limit
                
                # Progress tracking for large datasets
                if len(all_alerts) % 1000 == 0:
                    self.logger.info(f"Processed {len(all_alerts)} alerts...")
        
        except Exception as e:
            self.logger.error(f"Error fetching alerts: {str(e)}")
            # Return partial results if available
            if all_alerts:
                self.logger.warning(f"Returning partial results: {len(all_alerts)} alerts")
            else:
                raise
        
        return all_alerts[:query.max_alerts]
    
    def _filter_alerts(self, alerts: List[Dict], query, time_params) -> List[Dict]:
        """Apply additional filtering to alerts."""
        filtered = []
        
        for alert in alerts:
            # Time-based filtering (additional validation)
            alert_time = alert.get("timestamp")
            if alert_time:
                try:
                    alert_dt = datetime.fromisoformat(alert_time.replace('Z', '+00:00'))
                    if not (time_params["start_time"] <= alert_dt <= time_params["end_time"]):
                        continue
                except:
                    pass  # Skip alerts with invalid timestamps
            
            # Severity filtering
            if query.severity_filter:
                alert_level = alert.get("rule", {}).get("level", 0)
                severity = self._map_level_to_severity(alert_level)
                if severity not in query.severity_filter:
                    continue
            
            # Agent filtering
            if query.agent_filter:
                agent_id = alert.get("agent", {}).get("id")
                agent_name = alert.get("agent", {}).get("name", "")
                if not any(filter_val in [agent_id, agent_name] for filter_val in query.agent_filter):
                    continue
            
            # Rule filtering
            if query.rule_filter:
                rule_id = str(alert.get("rule", {}).get("id", ""))
                if not any(filter_val in rule_id for filter_val in query.rule_filter):
                    continue
            
            filtered.append(alert)
        
        return filtered
    
    def _map_level_to_severity(self, level: int) -> str:
        """Map Wazuh alert levels to severity categories."""
        if level >= 13:
            return "critical"
        elif level >= 10:
            return "high"
        elif level >= 7:
            return "medium"
        else:
            return "low"
    
    async def _analyze_alert_data(self, alerts: List[Dict], query, time_params) -> Dict[str, Any]:
        """Perform comprehensive analysis of alert data."""
        from collections import defaultdict, Counter
        import statistics
        
        analysis_start = datetime.utcnow()
        
        # Base summary
        summary = {
            "query_parameters": {
                "time_range": query.time_range,
                "period": f"{time_params['start_time'].isoformat()} to {time_params['end_time'].isoformat()}",
                "duration_hours": round(time_params["duration_hours"], 2),
                "max_alerts_requested": query.max_alerts,
                "filters_applied": {
                    "severity": query.severity_filter,
                    "agents": query.agent_filter,
                    "rules": query.rule_filter
                }
            },
            "summary": {
                "total_alerts": len(alerts),
                "analysis_timestamp": analysis_start.isoformat(),
                "data_completeness": min(100, (len(alerts) / query.max_alerts) * 100) if query.max_alerts > 0 else 100
            }
        }
        
        if not alerts:
            summary["summary"]["message"] = "No alerts found for the specified criteria"
            return summary
        
        # Group alerts based on query parameter
        grouped_data = self._group_alerts(alerts, query.group_by)
        summary["grouped_analysis"] = grouped_data
        
        # Statistical analysis
        if query.include_stats:
            summary["statistical_analysis"] = self._calculate_statistics(alerts, time_params)
        
        # Trend analysis
        if query.include_trends:
            summary["trend_analysis"] = self._analyze_trends(alerts, time_params)
        
        # Top insights
        summary["key_insights"] = self._generate_insights(alerts, grouped_data, time_params)
        
        # Performance metrics
        analysis_time = (datetime.utcnow() - analysis_start).total_seconds()
        summary["analysis_metadata"] = {
            "processing_time_seconds": round(analysis_time, 3),
            "alerts_per_second": round(len(alerts) / max(analysis_time, 0.001), 2),
            "memory_efficient": len(alerts) <= 5000
        }
        
        return summary
    
    def _group_alerts(self, alerts: List[Dict], group_by: str) -> Dict[str, Any]:
        """Group alerts by specified field with detailed analysis."""
        from collections import defaultdict, Counter
        
        groups = defaultdict(list)
        
        for alert in alerts:
            if group_by == "severity":
                level = alert.get("rule", {}).get("level", 0)
                key = self._map_level_to_severity(level)
            elif group_by == "agent":
                agent = alert.get("agent", {})
                key = f"{agent.get('name', 'unknown')} ({agent.get('id', 'N/A')})"
            elif group_by == "rule":
                rule = alert.get("rule", {})
                key = f"Rule {rule.get('id', 'N/A')}: {rule.get('description', 'Unknown')[:50]}"
            elif group_by == "source_ip":
                key = alert.get("data", {}).get("srcip", "unknown")
            elif group_by == "time":
                # Group by hour
                timestamp = alert.get("timestamp", "")
                try:
                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    key = dt.strftime("%Y-%m-%d %H:00")
                except:
                    key = "unknown"
            else:
                key = "all"
            
            groups[key].append(alert)
        
        # Create detailed group analysis
        group_analysis = {}
        total_alerts = len(alerts)
        
        for group_name, group_alerts in groups.items():
            count = len(group_alerts)
            percentage = (count / total_alerts) * 100 if total_alerts > 0 else 0
            
            # Calculate severity distribution for this group
            severity_dist = Counter()
            for alert in group_alerts:
                level = alert.get("rule", {}).get("level", 0)
                severity = self._map_level_to_severity(level)
                severity_dist[severity] += 1
            
            group_analysis[group_name] = {
                "count": count,
                "percentage": round(percentage, 2),
                "severity_distribution": dict(severity_dist),
                "sample_alerts": [
                    {
                        "id": alert.get("id"),
                        "timestamp": alert.get("timestamp"),
                        "rule_description": alert.get("rule", {}).get("description", "")[:100]
                    }
                    for alert in group_alerts[:3]  # Show first 3 as samples
                ]
            }
        
        # Sort groups by count (descending)
        sorted_groups = dict(sorted(group_analysis.items(), 
                                   key=lambda x: x[1]["count"], reverse=True))
        
        return {
            "grouping_field": group_by,
            "total_groups": len(sorted_groups),
            "groups": sorted_groups
        }
    
    def _calculate_statistics(self, alerts: List[Dict], time_params) -> Dict[str, Any]:
        """Calculate comprehensive statistical analysis."""
        import statistics
        from collections import Counter
        
        if not alerts:
            return {"error": "No alerts available for statistical analysis"}
        
        # Alert levels
        levels = [alert.get("rule", {}).get("level", 0) for alert in alerts]
        
        # Time distribution
        timestamps = []
        for alert in alerts:
            try:
                ts = alert.get("timestamp", "")
                dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                timestamps.append(dt)
            except:
                continue
        
        stats = {
            "alert_levels": {
                "mean": round(statistics.mean(levels), 2) if levels else 0,
                "median": round(statistics.median(levels), 2) if levels else 0,
                "std_dev": round(statistics.stdev(levels), 2) if len(levels) > 1 else 0,
                "min": min(levels) if levels else 0,
                "max": max(levels) if levels else 0,
                "distribution": dict(Counter(levels))
            },
            "temporal_analysis": {
                "alerts_per_hour": round(len(alerts) / max(time_params["duration_hours"], 1), 2),
                "peak_detection": self._detect_peaks(timestamps, time_params) if timestamps else None
            }
        }
        
        # Outlier detection
        if len(levels) > 10:
            q1 = statistics.quantiles(levels, n=4)[0]
            q3 = statistics.quantiles(levels, n=4)[2]
            iqr = q3 - q1
            outlier_threshold = q3 + 1.5 * iqr
            outliers = [l for l in levels if l > outlier_threshold]
            stats["outlier_analysis"] = {
                "outlier_threshold": round(outlier_threshold, 2),
                "outlier_count": len(outliers),
                "outlier_percentage": round((len(outliers) / len(levels)) * 100, 2)
            }
        
        return stats
    
    def _detect_peaks(self, timestamps: List[datetime], time_params) -> Dict[str, Any]:
        """Detect time-based peaks in alert activity."""
        if len(timestamps) < 10:
            return {"note": "Insufficient data for peak detection"}
        
        # Create hourly buckets
        from collections import defaultdict
        hourly_counts = defaultdict(int)
        
        for ts in timestamps:
            hour_key = ts.strftime("%Y-%m-%d %H:00")
            hourly_counts[hour_key] += 1
        
        if not hourly_counts:
            return {"note": "No valid timestamps for peak analysis"}
        
        # Find peaks (hours with significantly higher alert counts)
        counts = list(hourly_counts.values())
        if len(counts) < 3:
            return {"note": "Insufficient time periods for peak detection"}
        
        mean_count = statistics.mean(counts)
        std_count = statistics.stdev(counts) if len(counts) > 1 else 0
        
        peak_threshold = mean_count + (1.5 * std_count)
        
        peaks = []
        for hour, count in hourly_counts.items():
            if count > peak_threshold:
                peaks.append({
                    "time_period": hour,
                    "alert_count": count,
                    "deviation_from_mean": round(count - mean_count, 2)
                })
        
        peaks.sort(key=lambda x: x["alert_count"], reverse=True)
        
        return {
            "mean_alerts_per_hour": round(mean_count, 2),
            "peak_threshold": round(peak_threshold, 2),
            "peaks_detected": len(peaks),
            "top_peaks": peaks[:5]  # Show top 5 peaks
        }
    
    def _analyze_trends(self, alerts: List[Dict], time_params) -> Dict[str, Any]:
        """Analyze trends and patterns in alert data."""
        if not alerts:
            return {"note": "No alerts available for trend analysis"}
        
        # Pattern detection
        patterns = {
            "severity_trends": self._analyze_severity_trends(alerts),
            "agent_patterns": self._analyze_agent_patterns(alerts),
            "rule_patterns": self._analyze_rule_patterns(alerts),
            "temporal_patterns": self._analyze_temporal_patterns(alerts)
        }
        
        return patterns
    
    def _analyze_severity_trends(self, alerts: List[Dict]) -> Dict[str, Any]:
        """Analyze severity distribution trends."""
        from collections import Counter
        
        severity_counts = Counter()
        for alert in alerts:
            level = alert.get("rule", {}).get("level", 0)
            severity = self._map_level_to_severity(level)
            severity_counts[severity] += 1
        
        total = sum(severity_counts.values())
        trends = {}
        
        for severity, count in severity_counts.items():
            percentage = (count / total) * 100 if total > 0 else 0
            trends[severity] = {
                "count": count,
                "percentage": round(percentage, 2)
            }
        
        # Identify concerning trends
        concerns = []
        if severity_counts.get("critical", 0) > total * 0.1:
            concerns.append("High proportion of critical alerts detected")
        if severity_counts.get("high", 0) + severity_counts.get("critical", 0) > total * 0.3:
            concerns.append("Elevated high-severity alert activity")
        
        return {
            "distribution": trends,
            "concerns": concerns
        }
    
    def _analyze_agent_patterns(self, alerts: List[Dict]) -> Dict[str, Any]:
        """Analyze patterns across agents."""
        from collections import Counter, defaultdict
        
        agent_counts = Counter()
        agent_severities = defaultdict(Counter)
        
        for alert in alerts:
            agent = alert.get("agent", {})
            agent_id = agent.get("id", "unknown")
            agent_name = agent.get("name", "unknown")
            agent_key = f"{agent_name} ({agent_id})"
            
            agent_counts[agent_key] += 1
            
            level = alert.get("rule", {}).get("level", 0)
            severity = self._map_level_to_severity(level)
            agent_severities[agent_key][severity] += 1
        
        # Find agents with unusual activity
        if not agent_counts:
            return {"note": "No agent data available"}
        
        total_alerts = sum(agent_counts.values())
        mean_alerts_per_agent = total_alerts / len(agent_counts)
        
        high_activity_agents = []
        for agent, count in agent_counts.most_common(10):
            if count > mean_alerts_per_agent * 2:  # Agents with >2x average activity
                severity_dist = dict(agent_severities[agent])
                high_activity_agents.append({
                    "agent": agent,
                    "alert_count": count,
                    "percentage_of_total": round((count / total_alerts) * 100, 2),
                    "severity_distribution": severity_dist
                })
        
        return {
            "total_agents": len(agent_counts),
            "mean_alerts_per_agent": round(mean_alerts_per_agent, 2),
            "high_activity_agents": high_activity_agents
        }
    
    def _analyze_rule_patterns(self, alerts: List[Dict]) -> Dict[str, Any]:
        """Analyze rule triggering patterns."""
        from collections import Counter
        
        rule_counts = Counter()
        for alert in alerts:
            rule = alert.get("rule", {})
            rule_id = rule.get("id", "unknown")
            rule_desc = rule.get("description", "Unknown")[:50]
            rule_key = f"Rule {rule_id}: {rule_desc}"
            rule_counts[rule_key] += 1
        
        if not rule_counts:
            return {"note": "No rule data available"}
        
        total_alerts = sum(rule_counts.values())
        
        # Top triggered rules
        top_rules = []
        for rule, count in rule_counts.most_common(10):
            percentage = (count / total_alerts) * 100
            top_rules.append({
                "rule": rule,
                "trigger_count": count,
                "percentage": round(percentage, 2)
            })
        
        # Check for rule concentration (few rules generating many alerts)
        top_5_percent = sum(count for _, count in rule_counts.most_common(5)) / total_alerts * 100
        
        return {
            "total_unique_rules": len(rule_counts),
            "top_rules": top_rules,
            "concentration_analysis": {
                "top_5_rules_percentage": round(top_5_percent, 2),
                "rule_diversity": "low" if top_5_percent > 80 else "medium" if top_5_percent > 60 else "high"
            }
        }
    
    def _analyze_temporal_patterns(self, alerts: List[Dict]) -> Dict[str, Any]:
        """Analyze time-based patterns."""
        from collections import defaultdict
        
        hourly_counts = defaultdict(int)
        daily_counts = defaultdict(int)
        
        for alert in alerts:
            try:
                timestamp = alert.get("timestamp", "")
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                
                hour = dt.hour
                day = dt.strftime("%A")
                
                hourly_counts[hour] += 1
                daily_counts[day] += 1
            except:
                continue
        
        if not hourly_counts and not daily_counts:
            return {"note": "No valid timestamps for temporal analysis"}
        
        # Find peak hours and days
        peak_hour = max(hourly_counts.items(), key=lambda x: x[1]) if hourly_counts else None
        peak_day = max(daily_counts.items(), key=lambda x: x[1]) if daily_counts else None
        
        return {
            "hourly_distribution": dict(hourly_counts),
            "daily_distribution": dict(daily_counts),
            "patterns": {
                "peak_hour": f"{peak_hour[0]}:00" if peak_hour else None,
                "peak_hour_count": peak_hour[1] if peak_hour else 0,
                "peak_day": peak_day[0] if peak_day else None,
                "peak_day_count": peak_day[1] if peak_day else 0
            }
        }
    
    def _generate_insights(self, alerts: List[Dict], grouped_data: Dict, time_params) -> List[str]:
        """Generate actionable insights from the analysis."""
        insights = []
        
        if not alerts:
            insights.append("No alerts found for the specified time period and filters.")
            return insights
        
        total_alerts = len(alerts)
        duration_hours = time_params["duration_hours"]
        
        # Rate-based insights
        alert_rate = total_alerts / max(duration_hours, 1)
        if alert_rate > 100:
            insights.append(f"High alert volume detected: {total_alerts:,} alerts in {duration_hours:.1f} hours ({alert_rate:.1f} alerts/hour)")
        elif alert_rate < 1 and duration_hours > 24:
            insights.append(f"Low alert activity: Only {total_alerts} alerts in {duration_hours:.1f} hours")
        
        # Severity insights
        severity_groups = grouped_data.get("groups", {}) if grouped_data.get("grouping_field") == "severity" else {}
        critical_count = sum(group["count"] for name, group in severity_groups.items() if "critical" in name.lower())
        high_count = sum(group["count"] for name, group in severity_groups.items() if "high" in name.lower())
        
        if critical_count > 0:
            insights.append(f"  {critical_count} critical alerts require immediate attention")
        if high_count > total_alerts * 0.2:
            insights.append(f"  {high_count} high-severity alerts detected ({(high_count/total_alerts)*100:.1f}% of total)")
        
        # Concentration insights
        if grouped_data.get("grouping_field") == "agent":
            groups = grouped_data.get("groups", {})
            if groups:
                top_agent = next(iter(groups.items()))
                if top_agent[1]["percentage"] > 50:
                    insights.append(f"Agent concentration detected: '{top_agent[0]}' generated {top_agent[1]['percentage']:.1f}% of all alerts")
        
        # Add baseline comparison insight
        baseline_rate = 10  # alerts per hour as baseline
        if alert_rate > baseline_rate * 3:
            increase_pct = ((alert_rate - baseline_rate) / baseline_rate) * 100
            insights.append(f"Alert rate {increase_pct:.0f}% above baseline ({baseline_rate} alerts/hour)")
        
        # Time-based insights
        if duration_hours >= 24:
            daily_rate = total_alerts / (duration_hours / 24)
            insights.append(f"Average daily alert volume: {daily_rate:.0f} alerts per day")
        
        if not insights:
            insights.append(f"Analyzed {total_alerts:,} alerts over {duration_hours:.1f} hours - no significant patterns detected")
        
        return insights
    
    async def _handle_get_wazuh_vulnerability_summary(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive vulnerability summary across entire infrastructure."""
        # Validate input parameters
        validated_query = validate_vulnerability_summary_query(arguments)
        
        # Get all agents first
        agents_data = await self._fetch_agents_for_vulnerability_analysis(validated_query)
        
        # Fetch vulnerability data for all agents
        vulnerability_data = await self._fetch_vulnerability_data_for_agents(
            agents_data, validated_query
        )
        
        # Analyze and aggregate vulnerability data
        summary = await self._analyze_vulnerability_data(
            vulnerability_data, validated_query, agents_data
        )
        
        return [types.TextContent(
            type="text",
            text=json.dumps(summary, indent=2, default=str)
        )]
    
    async def _fetch_agents_for_vulnerability_analysis(self, query) -> List[Dict[str, Any]]:
        """Fetch agents for vulnerability analysis with proper filtering."""
        try:
            # Get all active agents
            agents_response = await self.api_client.get_agents(
                status="active",
                limit=query.max_agents
            )
            
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            # Filter by OS if specified
            if query.os_filter:
                filtered_agents = []
                for agent in agents:
                    agent_os = agent.get("os", {}).get("platform", "").lower()
                    if any(os_filter.lower() in agent_os for os_filter in query.os_filter):
                        filtered_agents.append(agent)
                agents = filtered_agents
            
            self.logger.info(f"Found {len(agents)} agents for vulnerability analysis")
            return agents
            
        except Exception as e:
            self.logger.error(f"Error fetching agents: {str(e)}")
            raise
    
    async def _fetch_vulnerability_data_for_agents(self, agents: List[Dict], query) -> Dict[str, Any]:
        """Fetch vulnerability data for all agents with parallel processing."""
        vulnerability_data = {
            "agents": {},
            "total_vulnerabilities": 0,
            "processing_errors": [],
            "coverage": {
                "agents_processed": 0,
                "agents_with_vulnerabilities": 0,
                "agents_with_errors": 0
            }
        }
        
        # Process agents in batches to avoid overwhelming the API
        batch_size = 10
        for i in range(0, len(agents), batch_size):
            batch = agents[i:i + batch_size]
            await self._process_agent_batch_for_vulnerabilities(
                batch, query, vulnerability_data
            )
            
            # Progress tracking
            progress = min(i + batch_size, len(agents))
            if progress % 50 == 0 or progress == len(agents):
                self.logger.info(f"Processed {progress}/{len(agents)} agents for vulnerabilities")
        
        return vulnerability_data
    
    async def _process_agent_batch_for_vulnerabilities(self, agents: List[Dict], query, vulnerability_data):
        """Process a batch of agents for vulnerability data."""
        import asyncio
        
        # Create tasks for parallel processing
        tasks = []
        for agent in agents:
            task = self._fetch_agent_vulnerabilities_safe(agent, query)
            tasks.append(task)
        
        # Execute batch in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for agent, result in zip(agents, results):
            agent_id = agent.get("id", "unknown")
            
            if isinstance(result, Exception):
                vulnerability_data["processing_errors"].append({
                    "agent_id": agent_id,
                    "agent_name": agent.get("name", "unknown"),
                    "error": str(result)
                })
                vulnerability_data["coverage"]["agents_with_errors"] += 1
            else:
                vulnerability_data["agents"][agent_id] = {
                    "agent_info": agent,
                    "vulnerabilities": result,
                    "vulnerability_count": len(result)
                }
                vulnerability_data["total_vulnerabilities"] += len(result)
                
                if result:  # Agent has vulnerabilities
                    vulnerability_data["coverage"]["agents_with_vulnerabilities"] += 1
            
            vulnerability_data["coverage"]["agents_processed"] += 1
    
    async def _fetch_agent_vulnerabilities_safe(self, agent: Dict, query) -> List[Dict]:
        """Safely fetch vulnerabilities for a single agent."""
        try:
            agent_id = agent.get("id")
            if not agent_id:
                return []
            
            # Use the client manager to get vulnerabilities
            vuln_response = await self.api_client.get_agent_vulnerabilities(agent_id)
            vulnerabilities = vuln_response.get("data", {}).get("affected_items", [])
            
            # Apply filters
            filtered_vulnerabilities = self._filter_vulnerabilities(vulnerabilities, query)
            
            return filtered_vulnerabilities
            
        except Exception as e:
            # Log error but don't fail the entire process
            self.logger.warning(f"Failed to fetch vulnerabilities for agent {agent.get('id', 'unknown')}: {str(e)}")
            raise e  # Re-raise to be caught by caller
    
    def _filter_vulnerabilities(self, vulnerabilities: List[Dict], query) -> List[Dict]:
        """Apply filters to vulnerability data."""
        filtered = []
        
        for vuln in vulnerabilities:
            # CVSS threshold filter
            cvss_score = self._extract_cvss_score(vuln)
            if cvss_score < query.cvss_threshold:
                continue
            
            # Severity filter
            if query.severity_filter:
                severity = self._map_cvss_to_severity(cvss_score)
                if severity not in query.severity_filter:
                    continue
            
            # CVE filter
            if query.cve_filter:
                cve_id = vuln.get("cve", "")
                if not any(cve_filter in cve_id for cve_filter in query.cve_filter):
                    continue
            
            # Package filter
            if query.package_filter:
                package_name = vuln.get("name", "")
                if not any(pkg_filter.lower() in package_name.lower() for pkg_filter in query.package_filter):
                    continue
            
            # Exploitability filter
            if query.exploitability:
                if not self._has_known_exploit(vuln):
                    continue
            
            filtered.append(vuln)
        
        return filtered
    
    def _extract_cvss_score(self, vulnerability: Dict) -> float:
        """Extract CVSS score from vulnerability data."""
        # Try different possible fields for CVSS score
        cvss_fields = ["cvss2_score", "cvss3_score", "cvss_score", "score"]
        
        for field in cvss_fields:
            score = vulnerability.get(field)
            if score is not None:
                try:
                    return float(score)
                except (ValueError, TypeError):
                    continue
        
        # Fallback: try to derive from severity
        severity = vulnerability.get("severity", "").lower()
        severity_mapping = {
            "critical": 9.0,
            "high": 7.5,
            "medium": 5.0,
            "low": 2.5
        }
        return severity_mapping.get(severity, 0.0)
    
    def _map_cvss_to_severity(self, cvss_score: float) -> str:
        """Map CVSS score to severity level."""
        if cvss_score >= 9.0:
            return "critical"
        elif cvss_score >= 7.0:
            return "high"
        elif cvss_score >= 4.0:
            return "medium"
        else:
            return "low"
    
    def _has_known_exploit(self, vulnerability: Dict) -> bool:
        """Check if vulnerability has known exploits."""
        # Check various fields that might indicate known exploits
        exploit_indicators = [
            vulnerability.get("exploit_available", False),
            vulnerability.get("metasploit_module", False),
            vulnerability.get("exploit_code_maturity", "") == "functional",
            "exploit" in vulnerability.get("references", []),
            "exploitdb" in str(vulnerability.get("references", [])).lower()
        ]
        
        return any(exploit_indicators)
    
    async def _analyze_vulnerability_data(self, vulnerability_data: Dict, query, agents: List[Dict]) -> Dict[str, Any]:
        """Perform comprehensive analysis of vulnerability data."""
        from collections import defaultdict, Counter
        import statistics
        
        analysis_start = datetime.utcnow()
        
        # Base summary
        summary = {
            "query_parameters": {
                "cvss_threshold": query.cvss_threshold,
                "severity_filter": query.severity_filter,
                "os_filter": query.os_filter,
                "package_filter": query.package_filter,
                "exploitability_filter": query.exploitability,
                "max_agents": query.max_agents,
                "group_by": query.group_by
            },
            "summary": {
                "total_agents_analyzed": len(agents),
                "agents_with_vulnerabilities": vulnerability_data["coverage"]["agents_with_vulnerabilities"],
                "agents_with_errors": vulnerability_data["coverage"]["agents_with_errors"],
                "total_vulnerabilities": vulnerability_data["total_vulnerabilities"],
                "analysis_timestamp": analysis_start.isoformat(),
                "coverage_percentage": round(
                    (vulnerability_data["coverage"]["agents_processed"] / max(len(agents), 1)) * 100, 2
                )
            }
        }
        
        if vulnerability_data["total_vulnerabilities"] == 0:
            summary["summary"]["message"] = "No vulnerabilities found matching the specified criteria"
            return summary
        
        # Collect all vulnerabilities for analysis
        all_vulnerabilities = []
        agent_vuln_map = {}
        
        for agent_id, agent_data in vulnerability_data["agents"].items():
            vulns = agent_data["vulnerabilities"]
            all_vulnerabilities.extend(vulns)
            agent_vuln_map[agent_id] = {
                "agent_info": agent_data["agent_info"],
                "vulnerabilities": vulns,
                "count": len(vulns)
            }
        
        # Group vulnerabilities
        grouped_data = self._group_vulnerabilities(all_vulnerabilities, agent_vuln_map, query.group_by)
        summary["grouped_analysis"] = grouped_data
        
        # Risk analytics
        if query.include_analytics:
            summary["risk_analytics"] = self._calculate_vulnerability_risk_analytics(
                all_vulnerabilities, agent_vuln_map
            )
        
        # Remediation recommendations
        if query.include_remediation:
            summary["remediation"] = self._generate_vulnerability_remediation(
                all_vulnerabilities, grouped_data
            )
        
        # Key insights
        summary["key_insights"] = self._generate_vulnerability_insights(
            all_vulnerabilities, agent_vuln_map, grouped_data
        )
        
        # Processing errors
        if vulnerability_data["processing_errors"]:
            summary["processing_errors"] = vulnerability_data["processing_errors"][:10]  # Show first 10 errors
        
        # Performance metrics
        analysis_time = (datetime.utcnow() - analysis_start).total_seconds()
        summary["analysis_metadata"] = {
            "processing_time_seconds": round(analysis_time, 3),
            "vulnerabilities_per_second": round(len(all_vulnerabilities) / max(analysis_time, 0.001), 2),
            "agents_per_second": round(len(agents) / max(analysis_time, 0.001), 2)
        }
        
        return summary
    
    def _group_vulnerabilities(self, vulnerabilities: List[Dict], agent_vuln_map: Dict, group_by: str) -> Dict[str, Any]:
        """Group vulnerabilities by specified field."""
        from collections import defaultdict, Counter
        
        groups = defaultdict(list)
        
        for vuln in vulnerabilities:
            if group_by == "severity":
                cvss_score = self._extract_cvss_score(vuln)
                key = self._map_cvss_to_severity(cvss_score)
            elif group_by == "cve":
                key = vuln.get("cve", "unknown")
            elif group_by == "package":
                key = vuln.get("name", "unknown")
            elif group_by == "os":
                # Find which agent this vulnerability belongs to
                agent_id = vuln.get("agent_id")  # This might not be available
                if agent_id and agent_id in agent_vuln_map:
                    agent_info = agent_vuln_map[agent_id]["agent_info"]
                    key = agent_info.get("os", {}).get("platform", "unknown")
                else:
                    key = "unknown"
            elif group_by == "agent":
                # Similar to OS, need to find the agent
                agent_id = vuln.get("agent_id")
                if agent_id and agent_id in agent_vuln_map:
                    agent_info = agent_vuln_map[agent_id]["agent_info"]
                    key = f"{agent_info.get('name', 'unknown')} ({agent_id})"
                else:
                    key = "unknown"
            else:
                key = "all"
            
            groups[key].append(vuln)
        
        # Create detailed group analysis
        group_analysis = {}
        total_vulnerabilities = len(vulnerabilities)
        
        for group_name, group_vulns in groups.items():
            count = len(group_vulns)
            percentage = (count / total_vulnerabilities) * 100 if total_vulnerabilities > 0 else 0
            
            # Calculate CVSS statistics for this group
            cvss_scores = [self._extract_cvss_score(v) for v in group_vulns]
            cvss_scores = [s for s in cvss_scores if s > 0]  # Remove zero scores
            
            group_analysis[group_name] = {
                "count": count,
                "percentage": round(percentage, 2),
                "cvss_statistics": {
                    "mean": round(statistics.mean(cvss_scores), 2) if cvss_scores else 0,
                    "max": round(max(cvss_scores), 2) if cvss_scores else 0,
                    "min": round(min(cvss_scores), 2) if cvss_scores else 0
                },
                "exploitable_count": sum(1 for v in group_vulns if self._has_known_exploit(v)),
                "sample_vulnerabilities": [
                    {
                        "cve": vuln.get("cve", "unknown"),
                        "name": vuln.get("name", "unknown")[:50],
                        "cvss_score": self._extract_cvss_score(vuln)
                    }
                    for vuln in group_vulns[:3]  # Show first 3 as samples
                ]
            }
        
        # Sort groups by count (descending)
        sorted_groups = dict(sorted(group_analysis.items(), 
                                   key=lambda x: x[1]["count"], reverse=True))
        
        return {
            "grouping_field": group_by,
            "total_groups": len(sorted_groups),
            "groups": sorted_groups
        }
    
    def _calculate_vulnerability_risk_analytics(self, vulnerabilities: List[Dict], agent_vuln_map: Dict) -> Dict[str, Any]:
        """Calculate comprehensive risk analytics."""
        import statistics
        from collections import Counter
        
        if not vulnerabilities:
            return {"error": "No vulnerabilities available for risk analysis"}
        
        # CVSS score analysis
        cvss_scores = [self._extract_cvss_score(v) for v in vulnerabilities]
        cvss_scores = [s for s in cvss_scores if s > 0]
        
        # Severity distribution
        severity_counts = Counter()
        exploit_counts = Counter()
        
        for vuln in vulnerabilities:
            cvss = self._extract_cvss_score(vuln)
            severity = self._map_cvss_to_severity(cvss)
            severity_counts[severity] += 1
            
            if self._has_known_exploit(vuln):
                exploit_counts[severity] += 1
        
        analytics = {
            "cvss_analysis": {
                "mean_score": round(statistics.mean(cvss_scores), 2) if cvss_scores else 0,
                "median_score": round(statistics.median(cvss_scores), 2) if cvss_scores else 0,
                "std_deviation": round(statistics.stdev(cvss_scores), 2) if len(cvss_scores) > 1 else 0,
                "score_distribution": dict(Counter([round(s, 1) for s in cvss_scores]))
            },
            "severity_distribution": dict(severity_counts),
            "exploitability_analysis": {
                "total_exploitable": sum(exploit_counts.values()),
                "exploitable_by_severity": dict(exploit_counts),
                "exploitation_risk_score": self._calculate_exploitation_risk_score(
                    severity_counts, exploit_counts
                )
            },
            "risk_concentration": self._analyze_vulnerability_concentration(agent_vuln_map)
        }
        
        return analytics
    
    def _calculate_exploitation_risk_score(self, severity_counts: Counter, exploit_counts: Counter) -> float:
        """Calculate overall exploitation risk score (0-100)."""
        if not severity_counts:
            return 0.0
        
        # Weight severities differently
        severity_weights = {"critical": 4.0, "high": 3.0, "medium": 2.0, "low": 1.0}
        
        total_weighted_vulns = sum(
            count * severity_weights.get(severity, 1.0) 
            for severity, count in severity_counts.items()
        )
        
        total_weighted_exploits = sum(
            count * severity_weights.get(severity, 1.0) 
            for severity, count in exploit_counts.items()
        )
        
        # Calculate risk score
        if total_weighted_vulns == 0:
            return 0.0
        
        exploit_ratio = total_weighted_exploits / total_weighted_vulns
        severity_factor = min(total_weighted_vulns / 100, 1.0)  # Cap at 100 vulns
        
        risk_score = (exploit_ratio * 70) + (severity_factor * 30)
        return round(min(risk_score, 100.0), 2)
    
    def _analyze_vulnerability_concentration(self, agent_vuln_map: Dict) -> Dict[str, Any]:
        """Analyze how vulnerabilities are concentrated across agents."""
        if not agent_vuln_map:
            return {"note": "No agent data available for concentration analysis"}
        
        vuln_counts = [data["count"] for data in agent_vuln_map.values()]
        total_agents = len(agent_vuln_map)
        
        # Find agents with high vulnerability counts
        if vuln_counts:
            mean_vulns = statistics.mean(vuln_counts)
            high_vuln_agents = []
            
            for agent_id, data in agent_vuln_map.items():
                if data["count"] > mean_vulns * 2:  # Agents with >2x average
                    high_vuln_agents.append({
                        "agent_id": agent_id,
                        "agent_name": data["agent_info"].get("name", "unknown"),
                        "vulnerability_count": data["count"],
                        "os": data["agent_info"].get("os", {}).get("platform", "unknown")
                    })
            
            return {
                "total_agents": total_agents,
                "mean_vulnerabilities_per_agent": round(mean_vulns, 2),
                "max_vulnerabilities_per_agent": max(vuln_counts),
                "high_vulnerability_agents": sorted(high_vuln_agents, 
                                                   key=lambda x: x["vulnerability_count"], 
                                                   reverse=True)[:10]
            }
        
        return {"note": "No vulnerability data available"}
    
    def _generate_vulnerability_remediation(self, vulnerabilities: List[Dict], grouped_data: Dict) -> Dict[str, Any]:
        """Generate remediation recommendations."""
        from collections import Counter
        
        recommendations = {
            "immediate_actions": [],
            "patch_priorities": [],
            "system_hardening": [],
            "monitoring_recommendations": []
        }
        
        # Analyze critical vulnerabilities
        critical_vulns = [v for v in vulnerabilities 
                         if self._map_cvss_to_severity(self._extract_cvss_score(v)) == "critical"]
        
        if critical_vulns:
            recommendations["immediate_actions"].append({
                "priority": "URGENT",
                "action": f"Patch {len(critical_vulns)} critical vulnerabilities immediately",
                "affected_components": list(set(v.get("name", "unknown") for v in critical_vulns[:5]))
            })
        
        # Analyze exploitable vulnerabilities
        exploitable_vulns = [v for v in vulnerabilities if self._has_known_exploit(v)]
        if exploitable_vulns:
            recommendations["immediate_actions"].append({
                "priority": "HIGH",
                "action": f"Address {len(exploitable_vulns)} vulnerabilities with known exploits",
                "affected_components": list(set(v.get("name", "unknown") for v in exploitable_vulns[:5]))
            })
        
        # Package-based recommendations
        package_vulns = Counter(v.get("name", "unknown") for v in vulnerabilities)
        top_vulnerable_packages = package_vulns.most_common(5)
        
        for package, count in top_vulnerable_packages:
            if count > 5:  # Packages with many vulnerabilities
                recommendations["patch_priorities"].append({
                    "package": package,
                    "vulnerability_count": count,
                    "recommendation": f"Update {package} - {count} vulnerabilities found"
                })
        
        # System hardening recommendations
        if len(vulnerabilities) > 50:
            recommendations["system_hardening"].append(
                "Consider implementing automated patch management"
            )
        
        if any(self._has_known_exploit(v) for v in vulnerabilities):
            recommendations["monitoring_recommendations"].append(
                "Implement enhanced monitoring for exploitation attempts"
            )
        
        return recommendations
    
    def _generate_vulnerability_insights(self, vulnerabilities: List[Dict], agent_vuln_map: Dict, grouped_data: Dict) -> List[str]:
        """Generate actionable insights from vulnerability analysis."""
        insights = []
        
        if not vulnerabilities:
            insights.append("No vulnerabilities found matching the specified criteria.")
            return insights
        
        total_vulns = len(vulnerabilities)
        total_agents = len(agent_vuln_map)
        
        # Overall statistics
        insights.append(f"Found {total_vulns:,} vulnerabilities across {total_agents} agents")
        
        # Critical vulnerability insights
        critical_count = sum(1 for v in vulnerabilities 
                           if self._map_cvss_to_severity(self._extract_cvss_score(v)) == "critical")
        if critical_count > 0:
            insights.append(f"  {critical_count} critical vulnerabilities require immediate attention")
        
        # Exploitability insights
        exploitable_count = sum(1 for v in vulnerabilities if self._has_known_exploit(v))
        if exploitable_count > 0:
            exploit_percentage = (exploitable_count / total_vulns) * 100
            insights.append(f" {exploitable_count} vulnerabilities ({exploit_percentage:.1f}%) have known exploits")
        
        # Agent concentration insights
        if agent_vuln_map:
            vuln_counts = [data["count"] for data in agent_vuln_map.values()]
            if vuln_counts:
                max_vulns = max(vuln_counts)
                max_agent = max(agent_vuln_map.items(), key=lambda x: x[1]["count"])
                agent_name = max_agent[1]["agent_info"].get("name", "unknown")
                
                if max_vulns > 20:
                    insights.append(f"Agent '{agent_name}' has {max_vulns} vulnerabilities - consider prioritizing")
        
        # Package insights
        if grouped_data.get("grouping_field") == "package":
            groups = grouped_data.get("groups", {})
            if groups:
                top_package = next(iter(groups.items()))
                if top_package[1]["count"] > 10:
                    insights.append(f"Package '{top_package[0]}' has {top_package[1]['count']} vulnerabilities - update recommended")
        
        # Coverage insights
        agents_with_vulns = sum(1 for data in agent_vuln_map.values() if data["count"] > 0)
        if agents_with_vulns < total_agents:
            clean_agents = total_agents - agents_with_vulns
            insights.append(f"{clean_agents} agents have no vulnerabilities matching criteria")
        
        if not insights[1:]:  # Only the first general insight
            insights.append(f"Vulnerability distribution appears normal - continue regular patching schedule")
        
        return insights
    
    async def _handle_get_wazuh_critical_vulnerabilities(self, arguments: dict) -> list[types.TextContent]:
        """Handle critical vulnerabilities detection with exposure and exploit analysis."""
        # Validate input parameters
        validated_query = validate_critical_vulnerabilities_query(arguments)
        
        # Get all agents for analysis
        agents_data = await self._fetch_agents_for_critical_analysis(validated_query)
        
        # Fetch critical vulnerability data with context
        critical_vulns = await self._fetch_critical_vulnerabilities(
            agents_data, validated_query
        )
        
        # Analyze and prioritize critical vulnerabilities
        analysis = await self._analyze_critical_vulnerabilities(
            critical_vulns, validated_query
        )
        
        return [types.TextContent(
            type="text",
            text=json.dumps(analysis, indent=2, default=str)
        )]
    
    async def _fetch_agents_for_critical_analysis(self, query) -> List[Dict[str, Any]]:
        """Fetch agents focusing on critical systems and exposed services."""
        try:
            # Get all active agents
            agents_response = await self.api_client.get_agents(
                status="active",
                limit=500  # More agents for critical analysis
            )
            
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            # If filtering by critical services, prioritize those agents
            if query.affected_services:
                prioritized_agents = []
                other_agents = []
                
                for agent in agents:
                    agent_name = agent.get("name", "").lower()
                    agent_labels = agent.get("labels", {})
                    
                    # Check if agent is associated with critical services
                    is_critical = any(
                        service.lower() in agent_name or 
                        service.lower() in str(agent_labels).lower()
                        for service in query.affected_services
                    )
                    
                    if is_critical:
                        prioritized_agents.append(agent)
                    else:
                        other_agents.append(agent)
                
                # Return prioritized agents first, up to max limit
                agents = (prioritized_agents + other_agents)[:query.max_results]
            
            self.logger.info(f"Analyzing {len(agents)} agents for critical vulnerabilities")
            return agents
            
        except Exception as e:
            self.logger.error(f"Error fetching agents for critical analysis: {str(e)}")
            raise
    
    async def _fetch_critical_vulnerabilities(self, agents: List[Dict], query) -> Dict[str, Any]:
        """Fetch critical vulnerabilities with additional context."""
        critical_data = {
            "vulnerabilities": [],
            "agents_analyzed": len(agents),
            "total_critical_vulns": 0,
            "exploitable_count": 0,
            "internet_exposed_count": 0,
            "context_data": {},
            "processing_errors": []
        }
        
        # Process agents to find critical vulnerabilities
        for agent in agents:
            agent_id = agent.get("id")
            if not agent_id:
                continue
            
            try:
                # Get vulnerabilities for this agent
                vuln_response = await self.api_client.get_agent_vulnerabilities(agent_id)
                vulnerabilities = vuln_response.get("data", {}).get("affected_items", [])
                
                # Filter for critical vulnerabilities
                critical_vulns = self._filter_critical_vulnerabilities(
                    vulnerabilities, query, agent
                )
                
                # If we have critical vulnerabilities and need context
                if critical_vulns and query.include_context:
                    # Fetch additional context (ports, processes)
                    context = await self._fetch_vulnerability_context(agent_id)
                    critical_data["context_data"][agent_id] = context
                
                # Check for internet exposure if requested
                if query.internet_exposed and critical_vulns:
                    exposure_data = await self._check_internet_exposure(
                        agent_id, critical_vulns, 
                        critical_data.get("context_data", {}).get(agent_id, {})
                    )
                    
                    # Update vulnerabilities with exposure data
                    for vuln in critical_vulns:
                        vuln["internet_exposed"] = exposure_data.get(
                            vuln.get("name", ""), False
                        )
                        if vuln["internet_exposed"]:
                            critical_data["internet_exposed_count"] += 1
                
                # Add agent information to each vulnerability
                for vuln in critical_vulns:
                    vuln["agent_id"] = agent_id
                    vuln["agent_name"] = agent.get("name", "unknown")
                    vuln["agent_ip"] = agent.get("ip", "unknown")
                    vuln["agent_os"] = agent.get("os", {}).get("platform", "unknown")
                    
                    # Count exploitable vulnerabilities
                    if self._has_known_exploit(vuln):
                        critical_data["exploitable_count"] += 1
                
                critical_data["vulnerabilities"].extend(critical_vulns)
                critical_data["total_critical_vulns"] += len(critical_vulns)
                
            except Exception as e:
                self.logger.warning(f"Error processing agent {agent_id}: {str(e)}")
                critical_data["processing_errors"].append({
                    "agent_id": agent_id,
                    "agent_name": agent.get("name", "unknown"),
                    "error": str(e)
                })
        
        # Sort vulnerabilities by risk score
        critical_data["vulnerabilities"] = sorted(
            critical_data["vulnerabilities"],
            key=lambda v: self._calculate_vulnerability_risk_score(v),
            reverse=True
        )[:query.max_results]
        
        return critical_data
    
    def _filter_critical_vulnerabilities(self, vulnerabilities: List[Dict], query, agent: Dict) -> List[Dict]:
        """Filter vulnerabilities based on critical criteria."""
        filtered = []
        current_date = datetime.utcnow()
        
        for vuln in vulnerabilities:
            # CVSS score check
            cvss_score = self._extract_cvss_score(vuln)
            if cvss_score < query.min_cvss:
                continue
            
            # Exploit requirement check
            if query.exploit_required and not self._has_known_exploit(vuln):
                continue
            
            # Patch availability check
            if query.patch_available:
                if not self._has_available_patch(vuln):
                    continue
            
            # Age check
            if query.age_days is not None:
                vuln_age = self._calculate_vulnerability_age(vuln, current_date)
                if vuln_age > query.age_days:
                    continue
            
            # Service filter check
            if query.affected_services:
                service_match = any(
                    service.lower() in vuln.get("name", "").lower() or
                    service.lower() in vuln.get("package", "").lower()
                    for service in query.affected_services
                )
                if not service_match:
                    continue
            
            filtered.append(vuln)
        
        return filtered
    
    def _has_available_patch(self, vulnerability: Dict) -> bool:
        """Check if a patch is available for the vulnerability."""
        patch_indicators = [
            vulnerability.get("patch_available", False),
            vulnerability.get("fixed_version") is not None,
            vulnerability.get("solution") is not None,
            "patch" in vulnerability.get("remediation", "").lower(),
            "update" in vulnerability.get("remediation", "").lower()
        ]
        
        return any(patch_indicators)
    
    def _calculate_vulnerability_age(self, vulnerability: Dict, current_date: datetime) -> int:
        """Calculate the age of a vulnerability in days."""
        # Try to get publication date
        pub_date_str = vulnerability.get("published_date") or vulnerability.get("detection_date")
        
        if pub_date_str:
            try:
                pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
                age_days = (current_date - pub_date).days
                return age_days
            except:
                pass
        
        # If no date available, consider it old
        return 9999
    
    async def _fetch_vulnerability_context(self, agent_id: str) -> Dict[str, Any]:
        """Fetch network and process context for vulnerability analysis."""
        context = {
            "open_ports": [],
            "running_processes": [],
            "network_interfaces": []
        }
        
        try:
            # Get open ports
            ports_response = await self.api_client.get_agent_ports(agent_id)
            context["open_ports"] = ports_response.get("data", {}).get("affected_items", [])
        except Exception as e:
            self.logger.debug(f"Could not fetch ports for agent {agent_id}: {str(e)}")
        
        try:
            # Get running processes
            processes_response = await self.api_client.get_agent_processes(agent_id)
            context["running_processes"] = processes_response.get("data", {}).get("affected_items", [])
        except Exception as e:
            self.logger.debug(f"Could not fetch processes for agent {agent_id}: {str(e)}")
        
        return context
    
    async def _check_internet_exposure(self, agent_id: str, vulnerabilities: List[Dict], 
                                     context: Dict) -> Dict[str, bool]:
        """Check if vulnerable services are exposed to the internet."""
        exposure_map = {}
        
        # Get open ports from context
        open_ports = context.get("open_ports", [])
        
        # Common internet-facing ports
        internet_ports = {
            80, 443, 8080, 8443,  # Web
            22, 3389,             # SSH/RDP
            21, 990,              # FTP
            25, 587, 993, 995,    # Mail
            3306, 5432, 1521,     # Databases
            27017, 9200           # NoSQL
        }
        
        # Check each vulnerability for exposure
        for vuln in vulnerabilities:
            package_name = vuln.get("name", "").lower()
            
            # Check if any open port is associated with this package
            is_exposed = False
            
            for port in open_ports:
                port_num = port.get("local", {}).get("port", 0)
                process_name = port.get("process", "").lower()
                
                # Check if port is internet-facing and related to vulnerable package
                if port_num in internet_ports:
                    if package_name in process_name or self._is_service_related(package_name, port_num):
                        is_exposed = True
                        break
            
            exposure_map[vuln.get("name", "")] = is_exposed
        
        return exposure_map
    
    def _is_service_related(self, package_name: str, port: int) -> bool:
        """Check if a package is related to a specific port."""
        service_port_mapping = {
            "apache": [80, 443, 8080],
            "nginx": [80, 443, 8080],
            "iis": [80, 443],
            "ssh": [22],
            "openssh": [22],
            "mysql": [3306],
            "postgresql": [5432],
            "mongodb": [27017],
            "elastic": [9200, 9300],
            "redis": [6379],
            "tomcat": [8080, 8443]
        }
        
        for service, ports in service_port_mapping.items():
            if service in package_name and port in ports:
                return True
        
        return False
    
    def _calculate_vulnerability_risk_score(self, vulnerability: Dict) -> float:
        """Calculate a risk score for vulnerability prioritization."""
        base_score = self._extract_cvss_score(vulnerability)
        
        # Risk multipliers
        multipliers = 1.0
        
        # Exploit available increases risk
        if self._has_known_exploit(vulnerability):
            multipliers *= 1.5
        
        # Internet exposure significantly increases risk
        if vulnerability.get("internet_exposed", False):
            multipliers *= 2.0
        
        # Age factor (newer vulnerabilities might be more actively exploited)
        age_days = self._calculate_vulnerability_age(vulnerability, datetime.utcnow())
        if age_days <= 30:
            multipliers *= 1.3
        elif age_days <= 90:
            multipliers *= 1.1
        
        # Critical service factor
        critical_services = ["database", "payment", "authentication", "api"]
        package_name = vulnerability.get("name", "").lower()
        if any(service in package_name for service in critical_services):
            multipliers *= 1.4
        
        return min(base_score * multipliers, 10.0)
    
    async def _analyze_critical_vulnerabilities(self, critical_data: Dict, query) -> Dict[str, Any]:
        """Analyze and prioritize critical vulnerabilities."""
        analysis_start = datetime.utcnow()
        
        vulnerabilities = critical_data["vulnerabilities"]
        
        # Base analysis structure
        analysis = {
            "query_parameters": {
                "min_cvss": query.min_cvss,
                "exploit_required": query.exploit_required,
                "internet_exposed_filter": query.internet_exposed,
                "patch_available_filter": query.patch_available,
                "age_days": query.age_days,
                "affected_services": query.affected_services,
                "max_results": query.max_results
            },
            "summary": {
                "total_critical_vulnerabilities": critical_data["total_critical_vulns"],
                "agents_analyzed": critical_data["agents_analyzed"],
                "exploitable_vulnerabilities": critical_data["exploitable_count"],
                "internet_exposed_vulnerabilities": critical_data["internet_exposed_count"],
                "analysis_timestamp": analysis_start.isoformat()
            }
        }
        
        if not vulnerabilities:
            analysis["summary"]["message"] = "No critical vulnerabilities found matching the specified criteria"
            return analysis
        
        # Group by severity for overview
        severity_groups = self._group_by_severity(vulnerabilities)
        analysis["severity_breakdown"] = severity_groups
        
        # Identify top risks
        analysis["top_risks"] = self._identify_top_risks(vulnerabilities, critical_data)
        
        # Attack surface analysis
        analysis["attack_surface"] = self._analyze_attack_surface(vulnerabilities, critical_data)
        
        # Immediate actions required
        analysis["immediate_actions"] = self._generate_immediate_actions(vulnerabilities)
        
        # Detailed vulnerability list (limited to top results)
        analysis["critical_vulnerabilities"] = self._format_critical_vulnerabilities(
            vulnerabilities[:query.max_results]
        )
        
        # Risk metrics
        analysis["risk_metrics"] = self._calculate_risk_metrics(vulnerabilities)
        
        # Processing information
        if critical_data["processing_errors"]:
            analysis["processing_errors"] = critical_data["processing_errors"][:5]
        
        # Performance metrics
        analysis_time = (datetime.utcnow() - analysis_start).total_seconds()
        analysis["analysis_metadata"] = {
            "processing_time_seconds": round(analysis_time, 3),
            "vulnerabilities_per_second": round(len(vulnerabilities) / max(analysis_time, 0.001), 2)
        }
        
        return analysis
    
    def _group_by_severity(self, vulnerabilities: List[Dict]) -> Dict[str, Any]:
        """Group vulnerabilities by severity with counts."""
        from collections import Counter
        
        severity_counts = Counter()
        for vuln in vulnerabilities:
            cvss = self._extract_cvss_score(vuln)
            severity = self._map_cvss_to_severity(cvss)
            severity_counts[severity] += 1
        
        return dict(severity_counts)
    
    def _identify_top_risks(self, vulnerabilities: List[Dict], critical_data: Dict) -> List[Dict]:
        """Identify the highest risk vulnerabilities."""
        top_risks = []
        
        # Get top 10 highest risk vulnerabilities
        for vuln in vulnerabilities[:10]:
            risk_score = self._calculate_vulnerability_risk_score(vuln)
            
            risk_info = {
                "cve": vuln.get("cve", "unknown"),
                "package": vuln.get("name", "unknown"),
                "cvss_score": self._extract_cvss_score(vuln),
                "risk_score": round(risk_score, 2),
                "agent": f"{vuln.get('agent_name', 'unknown')} ({vuln.get('agent_id', 'N/A')})",
                "risk_factors": []
            }
            
            # Add risk factors
            if self._has_known_exploit(vuln):
                risk_info["risk_factors"].append("Known exploit available")
            
            if vuln.get("internet_exposed", False):
                risk_info["risk_factors"].append("Internet exposed")
            
            age_days = self._calculate_vulnerability_age(vuln, datetime.utcnow())
            if age_days <= 30:
                risk_info["risk_factors"].append(f"Recently disclosed ({age_days} days)")
            
            if self._has_available_patch(vuln):
                risk_info["risk_factors"].append("Patch available")
            
            top_risks.append(risk_info)
        
        return top_risks
    
    def _analyze_attack_surface(self, vulnerabilities: List[Dict], critical_data: Dict) -> Dict[str, Any]:
        """Analyze the attack surface based on vulnerabilities and exposure."""
        from collections import defaultdict
        
        attack_surface = {
            "exposed_services": defaultdict(int),
            "vulnerable_ports": set(),
            "affected_agents": defaultdict(list),
            "risk_summary": {}
        }
        
        for vuln in vulnerabilities:
            # Track exposed services
            if vuln.get("internet_exposed", False):
                service = vuln.get("name", "unknown")
                attack_surface["exposed_services"][service] += 1
            
            # Track affected agents
            agent_name = vuln.get("agent_name", "unknown")
            attack_surface["affected_agents"][agent_name].append(vuln.get("cve", "unknown"))
        
        # Convert sets to lists for JSON serialization
        attack_surface["exposed_services"] = dict(attack_surface["exposed_services"])
        attack_surface["vulnerable_ports"] = list(attack_surface["vulnerable_ports"])
        attack_surface["affected_agents"] = dict(attack_surface["affected_agents"])
        
        # Calculate risk summary
        total_agents = len(attack_surface["affected_agents"])
        exposed_count = sum(attack_surface["exposed_services"].values())
        
        attack_surface["risk_summary"] = {
            "agents_at_risk": total_agents,
            "internet_exposed_services": exposed_count,
            "exposure_rate": round((exposed_count / max(len(vulnerabilities), 1)) * 100, 2)
        }
        
        return attack_surface
    
    def _generate_immediate_actions(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Generate immediate action items based on critical vulnerabilities."""
        actions = []
        
        # Check for exploitable, internet-exposed vulnerabilities
        critical_exposed = [
            v for v in vulnerabilities 
            if v.get("internet_exposed", False) and self._has_known_exploit(v)
        ]
        
        if critical_exposed:
            actions.append({
                "priority": "CRITICAL",
                "action": f"Immediately isolate or patch {len(critical_exposed)} internet-exposed vulnerabilities with active exploits",
                "affected_systems": list(set(v.get("agent_name", "unknown") for v in critical_exposed[:5])),
                "vulnerabilities": [v.get("cve", "unknown") for v in critical_exposed[:5]]
            })
        
        # Check for critical vulnerabilities with patches
        patchable = [v for v in vulnerabilities if self._has_available_patch(v)]
        if patchable:
            actions.append({
                "priority": "HIGH",
                "action": f"Apply available patches for {len(patchable)} critical vulnerabilities",
                "affected_packages": list(set(v.get("name", "unknown") for v in patchable[:5]))
            })
        
        # Check for database/payment system vulnerabilities
        critical_services = [
            v for v in vulnerabilities
            if any(s in v.get("name", "").lower() for s in ["database", "payment", "auth"])
        ]
        
        if critical_services:
            actions.append({
                "priority": "HIGH",
                "action": f"Review and secure {len(critical_services)} vulnerabilities in critical services",
                "services": list(set(v.get("name", "unknown") for v in critical_services[:5]))
            })
        
        # Network isolation recommendation
        if any(v.get("internet_exposed", False) for v in vulnerabilities):
            actions.append({
                "priority": "MEDIUM",
                "action": "Implement network segmentation to reduce internet exposure",
                "recommendation": "Move vulnerable services behind firewall or VPN"
            })
        
        return actions
    
    def _format_critical_vulnerabilities(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Format critical vulnerabilities for output."""
        formatted = []
        
        for vuln in vulnerabilities:
            formatted_vuln = {
                "cve": vuln.get("cve", "unknown"),
                "package": vuln.get("name", "unknown"),
                "version": vuln.get("version", "unknown"),
                "cvss_score": self._extract_cvss_score(vuln),
                "severity": self._map_cvss_to_severity(self._extract_cvss_score(vuln)),
                "agent": {
                    "id": vuln.get("agent_id", "unknown"),
                    "name": vuln.get("agent_name", "unknown"),
                    "ip": vuln.get("agent_ip", "unknown"),
                    "os": vuln.get("agent_os", "unknown")
                },
                "risk_factors": {
                    "exploitable": self._has_known_exploit(vuln),
                    "internet_exposed": vuln.get("internet_exposed", False),
                    "patch_available": self._has_available_patch(vuln),
                    "age_days": self._calculate_vulnerability_age(vuln, datetime.utcnow())
                },
                "risk_score": round(self._calculate_vulnerability_risk_score(vuln), 2)
            }
            
            # Add remediation if available
            if vuln.get("remediation") or vuln.get("solution"):
                formatted_vuln["remediation"] = vuln.get("remediation") or vuln.get("solution")
            
            formatted.append(formatted_vuln)
        
        return formatted
    
    def _calculate_risk_metrics(self, vulnerabilities: List[Dict]) -> Dict[str, Any]:
        """Calculate overall risk metrics."""
        if not vulnerabilities:
            return {}
        
        risk_scores = [self._calculate_vulnerability_risk_score(v) for v in vulnerabilities]
        cvss_scores = [self._extract_cvss_score(v) for v in vulnerabilities]
        
        metrics = {
            "average_risk_score": round(sum(risk_scores) / len(risk_scores), 2),
            "max_risk_score": round(max(risk_scores), 2),
            "average_cvss": round(sum(cvss_scores) / len(cvss_scores), 2),
            "exploitation_rate": round(
                sum(1 for v in vulnerabilities if self._has_known_exploit(v)) / len(vulnerabilities) * 100, 2
            ),
            "exposure_rate": round(
                sum(1 for v in vulnerabilities if v.get("internet_exposed", False)) / len(vulnerabilities) * 100, 2
            ),
            "patch_availability_rate": round(
                sum(1 for v in vulnerabilities if self._has_available_patch(v)) / len(vulnerabilities) * 100, 2
            )
        }
        
        return metrics
    
    def _format_alerts(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format alerts for better readability."""
        alerts = data.get("data", {}).get("affected_items", [])
        
        formatted_alerts = []
        for alert in alerts:
            formatted_alerts.append({
                "id": alert.get("id"),
                "timestamp": alert.get("timestamp"),
                "rule": {
                    "id": alert.get("rule", {}).get("id"),
                    "description": alert.get("rule", {}).get("description"),
                    "level": alert.get("rule", {}).get("level"),
                    "groups": alert.get("rule", {}).get("groups", [])
                },
                "agent": {
                    "id": alert.get("agent", {}).get("id"),
                    "name": alert.get("agent", {}).get("name"),
                    "ip": alert.get("agent", {}).get("ip")
                },
                "location": alert.get("location")
            })
        
        return {
            "total_alerts": data.get("data", {}).get("total_affected_items", 0),
            "alerts": formatted_alerts,
            "query_time": datetime.utcnow().isoformat()
        }
    
    def _format_agents(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format agent data with enhanced metrics."""
        agents = data.get("data", {}).get("affected_items", [])
        
        status_summary = {
            "active": 0,
            "disconnected": 0,
            "never_connected": 0,
            "pending": 0
        }
        
        formatted_agents = []
        for agent in agents:
            status = agent.get("status", "unknown")
            if status in status_summary:
                status_summary[status] += 1
            
            formatted_agents.append({
                "id": agent.get("id"),
                "name": agent.get("name"),
                "ip": agent.get("ip"),
                "status": status,
                "os": agent.get("os", {}).get("platform"),
                "version": agent.get("version"),
                "last_keep_alive": agent.get("lastKeepAlive")
            })
        
        return {
            "summary": status_summary,
            "total_agents": len(agents),
            "agents": formatted_agents
        }
    
    def _assess_agent_health(self, agent: Dict[str, Any]) -> Dict[str, Any]:
        """Assess health of a single agent."""
        status = agent.get("status", "unknown")
        health_status = "healthy" if status == "active" else "unhealthy"
        
        return {
            "agent_id": agent.get("id"),
            "agent_name": agent.get("name"),
            "health_status": health_status,
            "status": status,
            "details": {
                "ip": agent.get("ip"),
                "os": agent.get("os", {}).get("platform"),
                "version": agent.get("version"),
                "last_keep_alive": agent.get("lastKeepAlive")
            }
        }
    
    def _assess_all_agents_health(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess health of all agents."""
        agents = data.get("data", {}).get("affected_items", [])
        
        health_report = {
            "total_agents": len(agents),
            "healthy": 0,
            "unhealthy": 0,
            "agents": []
        }
        
        for agent in agents:
            agent_health = self._assess_agent_health(agent)
            health_report["agents"].append(agent_health)
            
            if agent_health["health_status"] == "healthy":
                health_report["healthy"] += 1
            else:
                health_report["unhealthy"] += 1
        
        health_report["health_percentage"] = (
            (health_report["healthy"] / len(agents) * 100) if agents else 0
        )
        
        return health_report
    
    def _generate_alert_summary(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate statistical summary of alerts."""
        alerts = data.get("data", {}).get("affected_items", [])
        
        if not alerts:
            return {"message": "No alerts to summarize"}
        
        # Basic statistics
        total_alerts = len(alerts)
        levels = [alert.get("rule", {}).get("level", 0) for alert in alerts]
        
        summary = {
            "total_alerts": total_alerts,
            "severity_distribution": {
                "low": len([l for l in levels if 1 <= l <= 5]),
                "medium": len([l for l in levels if 6 <= l <= 10]),
                "high": len([l for l in levels if 11 <= l <= 15])
            },
            "top_rules": {},
            "top_agents": {},
            "time_analysis": {}
        }
        
        # Top rules
        rule_counter = {}
        for alert in alerts:
            rule_id = alert.get("rule", {}).get("id")
            if rule_id:
                rule_counter[rule_id] = rule_counter.get(rule_id, 0) + 1
        
        summary["top_rules"] = dict(sorted(rule_counter.items(), key=lambda x: x[1], reverse=True)[:10])
        
        # Top agents
        agent_counter = {}
        for alert in alerts:
            agent_id = alert.get("agent", {}).get("id")
            if agent_id:
                agent_counter[agent_id] = agent_counter.get(agent_id, 0) + 1
        
        summary["top_agents"] = dict(sorted(agent_counter.items(), key=lambda x: x[1], reverse=True)[:10])
        
        return summary
    
    async def _get_critical_vulnerabilities(self, agents_data: Dict[str, Any]) -> Dict[str, Any]:
        """Get critical vulnerabilities across agents."""
        agents = agents_data.get("data", {}).get("affected_items", [])
        active_agents = [a for a in agents if a.get("status") == "active"][:10]  # Limit to 10 agents
        
        critical_vulns = []
        
        # Fetch vulnerabilities concurrently for better performance
        async def fetch_agent_vulnerabilities(agent):
            try:
                vuln_data = await self.api_client.get_agent_vulnerabilities(agent["id"])
                vulns = vuln_data.get("data", {}).get("affected_items", [])
                
                agent_critical_vulns = []
                for vuln in vulns:
                    severity = vuln.get("severity", "").lower()
                    if severity in ["critical", "high"]:
                        agent_critical_vulns.append({
                            "agent_id": agent["id"],
                            "agent_name": agent.get("name"),
                            "vulnerability": vuln.get("title"),
                            "severity": severity,
                            "cve": vuln.get("cve", "N/A")
                        })
                return agent_critical_vulns
            except Exception as e:
                self.logger.warning(f"Could not get vulnerabilities for agent {agent['id']}: {str(e)}")
                return []
        
        # Execute all vulnerability fetches concurrently
        tasks = [fetch_agent_vulnerabilities(agent) for agent in active_agents]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Flatten results and filter out exceptions
        for result in results:
            if isinstance(result, list):
                critical_vulns.extend(result)
        
        return {
            "total_critical_vulnerabilities": len(critical_vulns),
            "vulnerabilities": critical_vulns[:50],  # Limit response size
            "agents_checked": len(active_agents)
        }
    
    async def _get_compliance_overview(self, agents_data: Dict[str, Any], alerts_data: Dict[str, Any]) -> Dict[str, Any]:
        """Get quick compliance overview."""
        agents = agents_data.get("data", {}).get("affected_items", [])
        alerts = alerts_data.get("data", {}).get("affected_items", [])
        
        # Simple compliance metrics
        total_agents = len(agents)
        active_agents = len([a for a in agents if a.get("status") == "active"])
        
        # Alert severity analysis
        high_severity_alerts = len([a for a in alerts if a.get("rule", {}).get("level", 0) >= 10])
        
        coverage_score = (active_agents / total_agents * 100) if total_agents > 0 else 0
        
        return {
            "agent_coverage": {
                "total_agents": total_agents,
                "active_agents": active_agents,
                "coverage_percentage": round(coverage_score, 1)
            },
            "alert_analysis": {
                "total_alerts": len(alerts),
                "high_severity_alerts": high_severity_alerts
            },
            "compliance_score": round(max(0, 100 - (high_severity_alerts * 2) - (100 - coverage_score)), 1),
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def _get_active_threats(self, alerts_data: Dict[str, Any]) -> Dict[str, Any]:
        """Get active threat summary."""
        alerts = alerts_data.get("data", {}).get("affected_items", [])
        
        if not alerts:
            return {"message": "No recent threats detected"}
        
        # Analyze patterns
        patterns = self.security_analyzer.detect_attack_patterns(alerts)
        
        # Get threat categories
        threat_categories = {}
        for alert in alerts:
            groups = alert.get("rule", {}).get("groups", [])
            for group in groups:
                if any(keyword in group.lower() for keyword in 
                      ["attack", "intrusion", "malware", "exploit", "breach"]):
                    threat_categories[group] = threat_categories.get(group, 0) + 1
        
        return {
            "total_recent_alerts": len(alerts),
            "detected_patterns": patterns.get("detected_patterns", {}),
            "threat_categories": dict(sorted(threat_categories.items(), key=lambda x: x[1], reverse=True)[:10]),
            "analysis_window": "Last hour",
            "timestamp": datetime.utcnow().isoformat()
        }
    
    async def _get_security_incident_analysis_prompt(self, arguments: dict) -> types.GetPromptResult:
        """Generate security incident analysis prompt with context."""
        alert_id = arguments.get("alert_id")
        include_context = arguments.get("include_context", "true").lower() == "true"
        
        # Fetch alert data for context
        context_data = ""
        if alert_id:
            try:
                alerts_data = await self.api_client.get_alerts(limit=100)
                alerts = alerts_data.get("data", {}).get("affected_items", [])
                target_alert = next((a for a in alerts if str(a.get("id")) == str(alert_id)), None)
                
                if target_alert:
                    context_data = f"""
### Alert Details:
- **ID**: {target_alert.get('id')}
- **Timestamp**: {target_alert.get('timestamp')}
- **Rule**: {target_alert.get('rule', {}).get('description')} (Level: {target_alert.get('rule', {}).get('level')})
- **Agent**: {target_alert.get('agent', {}).get('name')} ({target_alert.get('agent', {}).get('ip')})
- **Location**: {target_alert.get('location')}
"""
                    
                    if include_context:
                        # Get related alerts
                        agent_id = target_alert.get('agent', {}).get('id')
                        related_alerts = [a for a in alerts if a.get('agent', {}).get('id') == agent_id and a.get('id') != alert_id][:5]
                        
                        if related_alerts:
                            context_data += "\n### Related Alerts on Same Agent:\n"
                            for alert in related_alerts:
                                context_data += f"- {alert.get('timestamp')}: {alert.get('rule', {}).get('description')} (Level: {alert.get('rule', {}).get('level')})\n"
                else:
                    context_data = f"\n### Alert ID {alert_id} not found in recent alerts."
            except Exception as e:
                context_data = f"\n### Error fetching alert context: {str(e)}"
        
        prompt_text = f"""You are a cybersecurity analyst investigating a security incident from Wazuh SIEM. Please analyze the following security alert and provide a comprehensive incident analysis.

{context_data}

### Analysis Framework:
Please provide a structured analysis covering:

1. **Incident Summary**
   - Brief description of what happened
   - Severity assessment and business impact
   - Initial classification (malware, intrusion, policy violation, etc.)

2. **Technical Analysis**
   - Detailed breakdown of the alert and its indicators
   - Attack vectors and techniques used (map to MITRE ATT&CK if applicable)
   - Affected systems and potential lateral movement

3. **Timeline Reconstruction**
   - Chronological sequence of events
   - Key timestamps and their significance
   - Potential attack progression

4. **Risk Assessment**
   - Immediate risks and threats
   - Potential for further compromise
   - Business impact evaluation

5. **Containment Recommendations**
   - Immediate actions to contain the threat
   - Isolation procedures for affected systems
   - Evidence preservation steps

6. **Investigation Steps**
   - Additional data sources to examine
   - Forensic artifacts to collect
   - Queries to run for deeper analysis

7. **Remediation Plan**
   - Step-by-step remediation actions
   - System hardening recommendations
   - Prevention measures for future incidents

Please provide actionable insights and prioritize recommendations based on risk level."""

        return types.GetPromptResult(
            description="Comprehensive security incident analysis with investigation framework",
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=prompt_text
                    )
                )
            ]
        )
    
    async def _get_threat_hunting_query_prompt(self, arguments: dict) -> types.GetPromptResult:
        """Generate threat hunting query prompt."""
        threat_type = arguments.get("threat_type", "general")
        time_range = arguments.get("time_range", "24h")
        target_agents = arguments.get("target_agents", "")
        
        # Get recent alerts for context
        context_data = ""
        try:
            alerts_data = await self.api_client.get_alerts(limit=50)
            alerts = alerts_data.get("data", {}).get("affected_items", [])
            
            if alerts:
                # Analyze recent threat patterns
                rule_counts = {}
                for alert in alerts:
                    rule_desc = alert.get('rule', {}).get('description', 'Unknown')
                    rule_counts[rule_desc] = rule_counts.get(rule_desc, 0) + 1
                
                top_rules = sorted(rule_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                context_data = f"""
### Recent Threat Landscape (Last 50 alerts):
{chr(10).join([f"- {rule}: {count} occurrences" for rule, count in top_rules])}
"""
        except Exception as e:
            context_data = f"\n### Error fetching threat context: {str(e)}"
        
        prompt_text = f"""You are a threat hunter developing proactive hunting queries for a Wazuh SIEM environment. Generate comprehensive threat hunting queries and strategies.

### Hunting Parameters:
- **Threat Type**: {threat_type}
- **Time Range**: {time_range}
- **Target Agents**: {target_agents if target_agents else "All agents"}

{context_data}

### Generate Threat Hunting Strategy:

1. **Threat Hypothesis**
   - What specific threats are we hunting for?
   - Threat actor behaviors and TTPs to look for
   - MITRE ATT&CK techniques to focus on

2. **Hunting Queries**
   - Specific Wazuh queries to identify suspicious activity
   - Log sources and data types to examine
   - Correlation rules to create or modify

3. **Indicators of Compromise (IOCs)**
   - File hashes, IP addresses, domains to investigate
   - Process names and command line patterns
   - Network traffic patterns and anomalies

4. **Behavioral Analysis**
   - User behavior anomalies to detect
   - System behavior patterns that indicate compromise
   - Temporal patterns and frequency analysis

5. **Detection Logic**
   - Boolean logic for combining indicators
   - Thresholds and baselines for anomaly detection
   - Statistical analysis approaches

6. **Validation Steps**
   - How to validate potential findings
   - False positive reduction techniques
   - Escalation criteria for confirmed threats

7. **Automation Opportunities**
   - Automated hunting rules to implement
   - Orchestration and response workflows
   - Continuous monitoring improvements

Please provide actionable hunting queries and methodologies tailored to the {threat_type} threat landscape."""

        return types.GetPromptResult(
            description="Comprehensive threat hunting strategy and query generation",
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=prompt_text
                    )
                )
            ]
        )
    
    async def _get_compliance_assessment_prompt(self, arguments: dict) -> types.GetPromptResult:
        """Generate compliance assessment prompt."""
        framework = arguments.get("framework", "pci_dss")
        scope = arguments.get("scope", "full")
        
        # Get system context
        context_data = ""
        try:
            agents_data = await self.api_client.get_agents()
            agents = agents_data.get("data", {}).get("affected_items", [])
            
            alerts_data = await self.api_client.get_alerts(limit=100)
            alerts = alerts_data.get("data", {}).get("affected_items", [])
            
            context_data = f"""
### Current Environment Status:
- **Total Agents**: {len(agents)}
- **Active Agents**: {len([a for a in agents if a.get('status') == 'active'])}
- **Recent Alerts**: {len(alerts)}
- **High Severity Alerts**: {len([a for a in alerts if a.get('rule', {}).get('level', 0) >= 10])}
"""
        except Exception as e:
            context_data = f"\n### Error fetching environment context: {str(e)}"
        
        framework_details = {
            "pci_dss": {
                "name": "Payment Card Industry Data Security Standard",
                "focus": "protecting cardholder data",
                "key_areas": ["network security", "access control", "monitoring", "encryption"]
            },
            "hipaa": {
                "name": "Health Insurance Portability and Accountability Act",
                "focus": "protecting healthcare information",
                "key_areas": ["administrative safeguards", "physical safeguards", "technical safeguards"]
            },
            "gdpr": {
                "name": "General Data Protection Regulation",
                "focus": "data privacy and protection",
                "key_areas": ["data processing", "consent", "breach notification", "data subject rights"]
            },
            "nist": {
                "name": "NIST Cybersecurity Framework",
                "focus": "cybersecurity risk management",
                "key_areas": ["identify", "protect", "detect", "respond", "recover"]
            },
            "iso27001": {
                "name": "ISO 27001 Information Security Management",
                "focus": "information security management systems",
                "key_areas": ["risk assessment", "security controls", "monitoring", "improvement"]
            }
        }
        
        framework_info = framework_details.get(framework, framework_details["pci_dss"])
        
        prompt_text = f"""You are a compliance officer conducting a comprehensive assessment against {framework_info['name']} standards. Analyze the current security posture and provide detailed compliance recommendations.

### Assessment Parameters:
- **Framework**: {framework_info['name']}
- **Scope**: {scope}
- **Focus Area**: {framework_info['focus']}
- **Key Areas**: {', '.join(framework_info['key_areas'])}

{context_data}

### Compliance Assessment Framework:

1. **Current Compliance Status**
   - Overall compliance posture assessment
   - Strengths and weaknesses identification
   - Gap analysis against framework requirements

2. **Control Assessment**
   - Technical controls evaluation
   - Administrative controls review
   - Physical controls assessment (where applicable)

3. **Risk Analysis**
   - Compliance risks and their impact
   - Regulatory penalties and consequences
   - Business risks from non-compliance

4. **Evidence Collection**
   - Required documentation and evidence
   - Audit trail and logging requirements
   - Monitoring and reporting mechanisms

5. **Remediation Roadmap**
   - Prioritized action items
   - Timeline and resource requirements
   - Quick wins and long-term improvements

6. **Monitoring and Maintenance**
   - Continuous compliance monitoring
   - Regular assessment schedules
   - Key performance indicators (KPIs)

7. **Reporting and Documentation**
   - Compliance report structure
   - Stakeholder communication
   - Audit preparation guidance

Please provide a detailed compliance assessment with actionable recommendations prioritized by risk and regulatory importance."""

        return types.GetPromptResult(
            description=f"Comprehensive {framework_info['name']} compliance assessment",
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=prompt_text
                    )
                )
            ]
        )
    
    async def _get_security_report_generation_prompt(self, arguments: dict) -> types.GetPromptResult:
        """Generate security report generation prompt."""
        report_type = arguments.get("report_type", "executive")
        time_period = arguments.get("time_period", "weekly")
        audience = arguments.get("audience", "executives")
        
        # Get comprehensive metrics
        context_data = ""
        try:
            # Get alerts for reporting period
            time_range_map = {"daily": 86400, "weekly": 604800, "monthly": 2592000, "quarterly": 7776000}
            time_range = time_range_map.get(time_period, 604800)
            
            alerts_data = await self.api_client.get_alerts(limit=1000, time_range=time_range)
            alerts = alerts_data.get("data", {}).get("affected_items", [])
            
            agents_data = await self.api_client.get_agents()
            agents = agents_data.get("data", {}).get("affected_items", [])
            
            # Calculate metrics
            total_alerts = len(alerts)
            high_severity = len([a for a in alerts if a.get('rule', {}).get('level', 0) >= 10])
            critical_alerts = len([a for a in alerts if a.get('rule', {}).get('level', 0) >= 12])
            
            context_data = f"""
### Security Metrics ({time_period.title()} Period):
- **Total Alerts**: {total_alerts}
- **High Severity Alerts**: {high_severity}
- **Critical Alerts**: {critical_alerts}
- **Total Agents**: {len(agents)}
- **Active Agents**: {len([a for a in agents if a.get('status') == 'active'])}
- **Agent Coverage**: {len([a for a in agents if a.get('status') == 'active']) / len(agents) * 100 if agents else 0:.1f}%
"""
        except Exception as e:
            context_data = f"\n### Error fetching metrics: {str(e)}"
        
        audience_details = {
            "executives": {
                "focus": "business impact, risk levels, and strategic recommendations",
                "style": "high-level, business-focused, with executive summary"
            },
            "security_team": {
                "focus": "technical details, incident analysis, and operational metrics",
                "style": "detailed technical analysis with actionable insights"
            },
            "compliance_team": {
                "focus": "regulatory compliance, audit findings, and control effectiveness",
                "style": "compliance-focused with regulatory mapping"
            }
        }
        
        audience_info = audience_details.get(audience, audience_details["executives"])
        
        prompt_text = f"""You are a security analyst generating a comprehensive {report_type} security report for {audience}. Create a professional, actionable report that communicates security posture effectively.

### Report Parameters:
- **Report Type**: {report_type.title()}
- **Time Period**: {time_period.title()}
- **Target Audience**: {audience.title()}
- **Focus**: {audience_info['focus']}
- **Style**: {audience_info['style']}

{context_data}

### Security Report Structure:

1. **Executive Summary**
   - Key findings and security posture overview
   - Critical issues requiring immediate attention
   - Overall risk assessment and trending

2. **Threat Landscape Analysis**
   - Attack patterns and threat actor activity
   - Emerging threats and vulnerabilities
   - Industry-specific threat intelligence

3. **Incident Analysis**
   - Significant security incidents and their impact
   - Response effectiveness and lessons learned
   - Trend analysis and pattern recognition

4. **Metrics and KPIs**
   - Security operations metrics
   - Incident response performance
   - Compliance and audit metrics

5. **Risk Assessment**
   - Current risk levels and classifications
   - Risk trend analysis
   - Mitigation effectiveness

6. **Recommendations**
   - Strategic security improvements
   - Tactical operational enhancements
   - Resource and investment priorities

7. **Appendices**
   - Detailed technical findings
   - Compliance mapping
   - Methodology and data sources

Please generate a comprehensive, professional security report that is appropriate for the {audience} audience and provides actionable insights for improving security posture."""

        return types.GetPromptResult(
            description=f"Professional {report_type} security report for {audience}",
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=prompt_text
                    )
                )
            ]
        )
    
    async def _get_vulnerability_prioritization_prompt(self, arguments: dict) -> types.GetPromptResult:
        """Generate vulnerability prioritization prompt."""
        severity_threshold = arguments.get("severity_threshold", "medium")
        asset_criticality = arguments.get("asset_criticality", "high")
        
        # Get vulnerability data
        context_data = ""
        try:
            agents_data = await self.api_client.get_agents(status="active")
            agents = agents_data.get("data", {}).get("affected_items", [])[:10]  # Sample 10 agents
            
            total_vulns = 0
            critical_vulns = 0
            high_vulns = 0
            
            for agent in agents:
                try:
                    vuln_data = await self.api_client.get_agent_vulnerabilities(agent["id"])
                    vulns = vuln_data.get("data", {}).get("affected_items", [])
                    total_vulns += len(vulns)
                    
                    for vuln in vulns:
                        severity = vuln.get("severity", "").lower()
                        if severity == "critical":
                            critical_vulns += 1
                        elif severity == "high":
                            high_vulns += 1
                except Exception:
                    continue
            
            context_data = f"""
### Vulnerability Landscape:
- **Total Vulnerabilities**: {total_vulns}
- **Critical Vulnerabilities**: {critical_vulns}
- **High Vulnerabilities**: {high_vulns}
- **Agents Analyzed**: {len(agents)}
"""
        except Exception as e:
            context_data = f"\n### Error fetching vulnerability data: {str(e)}"
        
        prompt_text = f"""You are a vulnerability management specialist developing a risk-based prioritization strategy for security vulnerabilities. Create a comprehensive framework for prioritizing remediation efforts.

### Prioritization Parameters:
- **Severity Threshold**: {severity_threshold}
- **Asset Criticality**: {asset_criticality}
- **Risk-Based Approach**: Business impact and exploitability focus

{context_data}

### Vulnerability Prioritization Framework:

1. **Risk Scoring Matrix**
   - CVSS score integration with business context
   - Threat intelligence and exploit availability
   - Asset criticality and business impact weighting

2. **Threat Context Analysis**
   - Active exploitation in the wild
   - Threat actor capabilities and intentions
   - Industry-specific threat landscape

3. **Business Impact Assessment**
   - Critical business processes affected
   - Data sensitivity and regulatory implications
   - Financial and operational impact potential

4. **Remediation Feasibility**
   - Patch availability and testing requirements
   - System dependencies and change windows
   - Resource requirements and technical complexity

5. **Compensating Controls**
   - Existing security controls effectiveness
   - Network segmentation and access controls
   - Monitoring and detection capabilities

6. **Prioritization Methodology**
   - Scoring algorithm and weighting factors
   - Decision matrices and automated scoring
   - Regular re-evaluation criteria

7. **Remediation Roadmap**
   - Immediate actions (0-30 days)
   - Short-term remediation (1-3 months)
   - Long-term strategic improvements (3-12 months)

8. **Metrics and Tracking**
   - Key performance indicators
   - Remediation progress tracking
   - Risk reduction measurement

Please provide a comprehensive vulnerability prioritization strategy that balances technical risk with business impact and operational constraints."""

        return types.GetPromptResult(
            description="Risk-based vulnerability prioritization framework",
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=prompt_text
                    )
                )
            ]
        )
    
    async def _get_forensic_analysis_prompt(self, arguments: dict) -> types.GetPromptResult:
        """Generate forensic analysis prompt."""
        incident_id = arguments.get("incident_id")
        analysis_depth = arguments.get("analysis_depth", "detailed")
        
        # Get incident context
        context_data = ""
        try:
            # Try to fetch the specific incident/alert
            alerts_data = await self.api_client.get_alerts(limit=200)
            alerts = alerts_data.get("data", {}).get("affected_items", [])
            
            target_incident = None
            if incident_id:
                target_incident = next((a for a in alerts if str(a.get("id")) == str(incident_id)), None)
            
            if target_incident:
                agent_id = target_incident.get('agent', {}).get('id')
                agent_alerts = [a for a in alerts if a.get('agent', {}).get('id') == agent_id]
                
                context_data = f"""
### Incident Context:
- **Incident ID**: {incident_id}
- **Timestamp**: {target_incident.get('timestamp')}
- **Agent**: {target_incident.get('agent', {}).get('name')} ({target_incident.get('agent', {}).get('ip')})
- **Rule**: {target_incident.get('rule', {}).get('description')}
- **Severity**: Level {target_incident.get('rule', {}).get('level')}

### Related Events on Same Agent:
{chr(10).join([f"- {a.get('timestamp')}: {a.get('rule', {}).get('description')} (Level {a.get('rule', {}).get('level')})" for a in agent_alerts[:10]])}
"""
            else:
                context_data = f"\n### Incident ID {incident_id} not found in recent alerts. Proceeding with general forensic analysis framework."
        except Exception as e:
            context_data = f"\n### Error fetching incident context: {str(e)}"
        
        analysis_levels = {
            "surface": "high-level overview with key findings",
            "detailed": "comprehensive analysis with technical details",
            "comprehensive": "exhaustive examination with all available evidence"
        }
        
        analysis_description = analysis_levels.get(analysis_depth, "detailed analysis")
        
        prompt_text = f"""You are a digital forensics investigator conducting a {analysis_description} of a security incident. Perform systematic forensic analysis to reconstruct the incident timeline and identify all relevant evidence.

### Investigation Parameters:
- **Incident ID**: {incident_id}
- **Analysis Depth**: {analysis_depth.title()}
- **Investigation Type**: {analysis_description}

{context_data}

### Forensic Analysis Framework:

1. **Initial Assessment**
   - Incident scope and affected systems
   - Evidence preservation status
   - Investigation objectives and priorities

2. **Timeline Reconstruction**
   - Chronological sequence of events
   - Event correlation across systems
   - Attack progression and lateral movement

3. **Evidence Collection**
   - Log files and system artifacts
   - Network traffic analysis
   - File system examination
   - Memory dump analysis (if available)

4. **Artifact Analysis**
   - Malware analysis and IOC extraction
   - User activity reconstruction
   - System changes and modifications
   - Network connections and data exfiltration

5. **Attack Vector Analysis**
   - Initial compromise method
   - Privilege escalation techniques
   - Persistence mechanisms
   - Command and control communications

6. **Attribution Assessment**
   - Threat actor indicators
   - Tactics, techniques, and procedures (TTPs)
   - Campaign indicators and similarities
   - Geolocation and infrastructure analysis

7. **Impact Assessment**
   - Data accessed or compromised
   - System integrity status
   - Business process disruption
   - Regulatory and compliance implications

8. **Recovery Recommendations**
   - Evidence-based remediation steps
   - System restoration procedures
   - Security improvements
   - Monitoring enhancements

9. **Legal and Reporting**
   - Chain of custody documentation
   - Executive summary for stakeholders
   - Legal implications and requirements
   - Lessons learned and process improvements

Please conduct a thorough forensic analysis that follows industry best practices and provides actionable insights for incident response and security improvement."""

        return types.GetPromptResult(
            description=f"Comprehensive {analysis_depth} forensic analysis of security incident",
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=prompt_text
                    )
                )
            ]
        )
    
    async def _send_critical_alert_notification(self, alert: dict):
        """Send notification for critical security alerts."""
        try:
            severity_level = alert.get('rule', {}).get('level', 0)
            if severity_level >= 12:  # Critical alerts
                await self.server.send_notification(
                    "security/critical-alert",
                    {
                        "alert_id": alert.get('id'),
                        "timestamp": alert.get('timestamp'),
                        "rule_description": alert.get('rule', {}).get('description'),
                        "severity": severity_level,
                        "agent": {
                            "id": alert.get('agent', {}).get('id'),
                            "name": alert.get('agent', {}).get('name'),
                            "ip": alert.get('agent', {}).get('ip')
                        },
                        "location": alert.get('location'),
                        "message": f"Critical security alert: {alert.get('rule', {}).get('description')}"
                    }
                )
                self.logger.info(f"Sent critical alert notification for alert {alert.get('id')}")
        except Exception as e:
            self.logger.error(f"Failed to send critical alert notification: {str(e)}")
    
    async def _send_agent_status_notification(self, agent_id: str, status: str, previous_status: str):
        """Send notification for agent status changes."""
        try:
            if status != previous_status:
                await self.server.send_notification(
                    "agents/status-change",
                    {
                        "agent_id": agent_id,
                        "new_status": status,
                        "previous_status": previous_status,
                        "timestamp": datetime.utcnow().isoformat(),
                        "message": f"Agent {agent_id} status changed from {previous_status} to {status}"
                    }
                )
                self.logger.info(f"Sent agent status notification for agent {agent_id}: {previous_status} -> {status}")
        except Exception as e:
            self.logger.error(f"Failed to send agent status notification: {str(e)}")
    
    async def _send_system_health_notification(self, health_status: str, details: dict):
        """Send notification for system health changes."""
        try:
            await self.server.send_notification(
                "system/health-status",
                {
                    "health_status": health_status,
                    "timestamp": datetime.utcnow().isoformat(),
                    "details": details,
                    "message": f"System health status: {health_status}"
                }
            )
            self.logger.info(f"Sent system health notification: {health_status}")
        except Exception as e:
            self.logger.error(f"Failed to send system health notification: {str(e)}")
    
    async def _report_progress(self, current: int, total: int, message: str):
        """Report progress for long-running operations."""
        try:
            progress = (current / total) * 100
            await self.server.send_notification(
                "operations/progress",
                {
                    "progress": progress,
                    "current_step": current,
                    "total_steps": total,
                    "message": message,
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
            self.logger.debug(f"Progress reported: {progress:.1f}% - {message}")
        except Exception as e:
            self.logger.error(f"Failed to report progress: {str(e)}")
    
    async def _handle_get_wazuh_running_agents(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive running agents analysis with real-time infrastructure visibility."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_running_agents_query(arguments)
            
            # Fetch agents data
            agents_data = await self._fetch_agents_for_running_analysis(validated_query)
            
            # Analyze agents and generate comprehensive report
            analysis = await self._analyze_running_agents(agents_data, validated_query, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except ValidationError as e:
            self.logger.error(f"Validation error in running agents analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
        except Exception as e:
            self.logger.error(f"Error in running agents analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
    
    async def _fetch_agents_for_running_analysis(self, query) -> Dict[str, Any]:
        """Fetch agents data for running analysis with comprehensive filtering."""
        try:
            # Build query parameters
            query_params = {"limit": min(query.max_agents, 1000)}
            
            # Apply status filter
            if query.status_filter:
                if not query.include_disconnected and "disconnected" in query.status_filter:
                    query.status_filter.remove("disconnected")
                if query.status_filter:
                    query_params["status"] = ",".join(query.status_filter)
            elif not query.include_disconnected:
                query_params["status"] = "active"
            
            # Apply OS filter
            if query.os_filter:
                query_params["os.platform"] = ",".join(query.os_filter)
            
            # Apply version filter
            if query.version_filter:
                query_params["version"] = query.version_filter
            
            # Apply group filter
            if query.group_filter:
                query_params["group"] = ",".join(query.group_filter)
            
            # Fetch agents
            agents_response = await self.api_client.get_agents(**query_params)
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            self.logger.info(f"Fetched {len(agents)} agents for running analysis")
            
            # Enhance agents with additional metadata
            enhanced_agents = []
            for agent in agents:
                enhanced_agent = dict(agent)
                enhanced_agent["analysis_timestamp"] = datetime.utcnow().isoformat()
                enhanced_agents.append(enhanced_agent)
            
            return {
                "agents": enhanced_agents,
                "total_count": len(enhanced_agents),
                "query_params": query_params
            }
            
        except Exception as e:
            self.logger.error(f"Error fetching agents for running analysis: {str(e)}")
            raise
    
    async def _analyze_running_agents(self, agents_data: Dict, query, start_time: datetime) -> Dict[str, Any]:
        """Perform comprehensive analysis of running agents."""
        agents = agents_data.get("agents", [])
        current_time = datetime.utcnow()
        
        # Initialize analysis structure
        analysis = {
            "query_parameters": {
                "status_filter": query.status_filter,
                "os_filter": query.os_filter,
                "version_filter": query.version_filter,
                "group_filter": query.group_filter,
                "inactive_threshold": query.inactive_threshold,
                "include_disconnected": query.include_disconnected,
                "include_health_metrics": query.include_health_metrics,
                "include_last_activity": query.include_last_activity,
                "group_by": query.group_by,
                "max_agents": query.max_agents
            },
            "summary": {},
            "grouped_analysis": {},
            "infrastructure_health": {},
            "agent_details": [],
            "health_metrics": {},
            "activity_analysis": {},
            "recommendations": [],
            "analysis_metadata": {
                "timestamp": current_time.isoformat(),
                "processing_time_seconds": (current_time - start_time).total_seconds(),
                "total_agents_analyzed": len(agents)
            }
        }
        
        if not agents:
            analysis["summary"] = {
                "total_agents": 0,
                "active_agents": 0,
                "disconnected_agents": 0,
                "never_connected_agents": 0,
                "pending_agents": 0,
                "message": "No agents found matching the specified criteria"
            }
            return analysis
        
        # Analyze agent status distribution
        status_counts = {}
        os_counts = {}
        version_counts = {}
        group_counts = {}
        node_counts = {}
        
        active_agents = []
        inactive_agents = []
        health_issues = []
        
        for agent in agents:
            # Status analysis
            status = agent.get("status", "unknown")
            status_counts[status] = status_counts.get(status, 0) + 1
            
            # OS analysis
            os_info = agent.get("os", {})
            os_platform = os_info.get("platform", "unknown")
            os_counts[os_platform] = os_counts.get(os_platform, 0) + 1
            
            # Version analysis
            version = agent.get("version", "unknown")
            version_counts[version] = version_counts.get(version, 0) + 1
            
            # Group analysis
            group = agent.get("group", ["default"])[0] if agent.get("group") else "default"
            group_counts[group] = group_counts.get(group, 0) + 1
            
            # Node analysis
            node = agent.get("node_name", "unknown")
            node_counts[node] = node_counts.get(node, 0) + 1
            
            # Activity analysis
            if status == "active":
                last_keep_alive = agent.get("last_keep_alive")
                if last_keep_alive:
                    try:
                        last_seen = datetime.fromisoformat(last_keep_alive.replace("Z", "+00:00"))
                        seconds_since_last_seen = (current_time - last_seen.replace(tzinfo=None)).total_seconds()
                        
                        if seconds_since_last_seen <= query.inactive_threshold:
                            active_agents.append(agent)
                        else:
                            inactive_agents.append(agent)
                            health_issues.append({
                                "agent_id": agent.get("id"),
                                "agent_name": agent.get("name"),
                                "issue": "inactive",
                                "details": f"Last seen {int(seconds_since_last_seen)} seconds ago"
                            })
                    except Exception as e:
                        self.logger.warning(f"Error parsing last_keep_alive for agent {agent.get('id')}: {str(e)}")
                        active_agents.append(agent)  # Assume active if we can't parse
                else:
                    active_agents.append(agent)
        
        # Build summary
        analysis["summary"] = {
            "total_agents": len(agents),
            "active_agents": status_counts.get("active", 0),
            "disconnected_agents": status_counts.get("disconnected", 0),
            "never_connected_agents": status_counts.get("never_connected", 0),
            "pending_agents": status_counts.get("pending", 0),
            "truly_active_agents": len(active_agents),
            "inactive_agents": len(inactive_agents),
            "health_issues_count": len(health_issues),
            "infrastructure_coverage": f"{len(agents)} endpoints monitored",
            "analysis_timestamp": current_time.isoformat()
        }
        
        # Build grouped analysis
        analysis["grouped_analysis"] = {
            "grouping_field": query.group_by,
            "groups": {}
        }
        
        if query.group_by == "status":
            analysis["grouped_analysis"]["groups"] = {
                status: {
                    "count": count,
                    "percentage": round((count / len(agents)) * 100, 2)
                }
                for status, count in status_counts.items()
            }
        elif query.group_by == "os":
            analysis["grouped_analysis"]["groups"] = {
                os_platform: {
                    "count": count,
                    "percentage": round((count / len(agents)) * 100, 2)
                }
                for os_platform, count in os_counts.items()
            }
        elif query.group_by == "version":
            analysis["grouped_analysis"]["groups"] = {
                version: {
                    "count": count,
                    "percentage": round((count / len(agents)) * 100, 2)
                }
                for version, count in version_counts.items()
            }
        elif query.group_by == "group":
            analysis["grouped_analysis"]["groups"] = {
                group: {
                    "count": count,
                    "percentage": round((count / len(agents)) * 100, 2)
                }
                for group, count in group_counts.items()
            }
        elif query.group_by == "node":
            analysis["grouped_analysis"]["groups"] = {
                node: {
                    "count": count,
                    "percentage": round((count / len(agents)) * 100, 2)
                }
                for node, count in node_counts.items()
            }
        
        # Infrastructure health analysis
        health_score = self._calculate_infrastructure_health_score(agents, active_agents, health_issues)
        analysis["infrastructure_health"] = {
            "overall_health_score": health_score,
            "health_rating": self._get_health_rating(health_score),
            "active_percentage": round((len(active_agents) / len(agents)) * 100, 2) if agents else 0,
            "coverage_analysis": {
                "operating_systems": len(os_counts),
                "agent_versions": len(version_counts),
                "groups": len(group_counts),
                "nodes": len(node_counts)
            },
            "health_issues": health_issues[:10]  # Top 10 issues
        }
        
        # Add health metrics if requested
        if query.include_health_metrics:
            analysis["health_metrics"] = await self._gather_health_metrics(agents[:20])  # Sample for performance
        
        # Add activity analysis if requested
        if query.include_last_activity:
            analysis["activity_analysis"] = self._analyze_agent_activity(agents, query.inactive_threshold)
        
        # Add detailed agent information (top 50 for performance)
        analysis["agent_details"] = [
            {
                "id": agent.get("id"),
                "name": agent.get("name"),
                "status": agent.get("status"),
                "os": f"{agent.get('os', {}).get('platform', 'unknown')} {agent.get('os', {}).get('version', '')}".strip(),
                "version": agent.get("version"),
                "group": agent.get("group", ["default"])[0] if agent.get("group") else "default",
                "node": agent.get("node_name", "unknown"),
                "last_keep_alive": agent.get("last_keep_alive"),
                "ip": agent.get("ip"),
                "register_ip": agent.get("register_ip")
            }
            for agent in agents[:50]
        ]
        
        # Generate recommendations
        analysis["recommendations"] = self._generate_infrastructure_recommendations(
            analysis["summary"], analysis["infrastructure_health"], health_issues
        )
        
        return analysis
    
    def _calculate_infrastructure_health_score(self, agents: List[Dict], active_agents: List[Dict], health_issues: List[Dict]) -> float:
        """Calculate overall infrastructure health score."""
        if not agents:
            return 0.0
        
        # Base score from active agents
        active_ratio = len(active_agents) / len(agents)
        base_score = active_ratio * 100
        
        # Deduct points for health issues
        issue_penalty = min(len(health_issues) * 2, 20)  # Max 20 point penalty
        
        # Consider agent version diversity (too many versions can indicate maintenance issues)
        versions = set(agent.get("version", "unknown") for agent in agents)
        if len(versions) > 5:
            version_penalty = min((len(versions) - 5) * 2, 10)  # Max 10 point penalty
        else:
            version_penalty = 0
        
        final_score = max(base_score - issue_penalty - version_penalty, 0)
        return round(final_score, 2)
    
    def _get_health_rating(self, score: float) -> str:
        """Get health rating based on score."""
        if score >= 90:
            return "Excellent"
        elif score >= 75:
            return "Good"
        elif score >= 60:
            return "Fair"
        elif score >= 40:
            return "Poor"
        else:
            return "Critical"
    
    async def _gather_health_metrics(self, sample_agents: List[Dict]) -> Dict[str, Any]:
        """Gather health metrics from a sample of agents."""
        metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "disk_usage": [],
            "network_stats": [],
            "agent_stats": [],
            "collection_errors": []
        }
        
        for agent in sample_agents:
            agent_id = agent.get("id")
            if not agent_id:
                continue
                
            try:
                # Try to get agent stats
                stats = await self.api_client.get_agent_stats(agent_id)
                if stats and "data" in stats:
                    metrics["agent_stats"].append({
                        "agent_id": agent_id,
                        "agent_name": agent.get("name"),
                        "stats": stats["data"]
                    })
            except Exception as e:
                metrics["collection_errors"].append({
                    "agent_id": agent_id,
                    "error": str(e)
                })
        
        return metrics
    
    def _analyze_agent_activity(self, agents: List[Dict], inactive_threshold: int) -> Dict[str, Any]:
        """Analyze agent activity patterns."""
        current_time = datetime.utcnow()
        activity_analysis = {
            "activity_distribution": {},
            "last_seen_analysis": {},
            "activity_patterns": []
        }
        
        # Analyze last seen times
        last_seen_buckets = {
            "last_5_minutes": 0,
            "last_15_minutes": 0,
            "last_hour": 0,
            "last_day": 0,
            "older": 0
        }
        
        for agent in agents:
            last_keep_alive = agent.get("last_keep_alive")
            if last_keep_alive:
                try:
                    last_seen = datetime.fromisoformat(last_keep_alive.replace("Z", "+00:00"))
                    seconds_since_last_seen = (current_time - last_seen.replace(tzinfo=None)).total_seconds()
                    
                    if seconds_since_last_seen <= 300:  # 5 minutes
                        last_seen_buckets["last_5_minutes"] += 1
                    elif seconds_since_last_seen <= 900:  # 15 minutes
                        last_seen_buckets["last_15_minutes"] += 1
                    elif seconds_since_last_seen <= 3600:  # 1 hour
                        last_seen_buckets["last_hour"] += 1
                    elif seconds_since_last_seen <= 86400:  # 1 day
                        last_seen_buckets["last_day"] += 1
                    else:
                        last_seen_buckets["older"] += 1
                        
                except Exception as e:
                    self.logger.warning(f"Error parsing last_keep_alive: {str(e)}")
        
        activity_analysis["last_seen_analysis"] = last_seen_buckets
        
        # Activity patterns
        if last_seen_buckets["last_5_minutes"] > len(agents) * 0.8:
            activity_analysis["activity_patterns"].append("High activity - Most agents very recent")
        elif last_seen_buckets["older"] > len(agents) * 0.2:
            activity_analysis["activity_patterns"].append("Stale agents detected - Some agents haven't reported recently")
        
        return activity_analysis
    
    def _generate_infrastructure_recommendations(self, summary: Dict, health: Dict, issues: List[Dict]) -> List[Dict]:
        """Generate infrastructure recommendations based on analysis."""
        recommendations = []
        
        # Agent connectivity recommendations
        if summary["disconnected_agents"] > 0:
            recommendations.append({
                "priority": "HIGH",
                "category": "connectivity",
                "title": "Reconnect Disconnected Agents",
                "description": f"{summary['disconnected_agents']} agents are disconnected",
                "action": "Investigate network connectivity and agent configuration for disconnected agents",
                "impact": "Security monitoring gaps"
            })
        
        # Health score recommendations
        if health["overall_health_score"] < 80:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "health",
                "title": "Improve Infrastructure Health",
                "description": f"Infrastructure health score is {health['overall_health_score']}/100",
                "action": "Address inactive agents and resolve health issues",
                "impact": "Improved monitoring reliability"
            })
        
        # Version diversity recommendations
        if health["coverage_analysis"]["agent_versions"] > 3:
            recommendations.append({
                "priority": "LOW",
                "category": "maintenance",
                "title": "Standardize Agent Versions",
                "description": f"Multiple agent versions detected ({health['coverage_analysis']['agent_versions']})",
                "action": "Plan agent version standardization to improve maintenance",
                "impact": "Easier management and consistent features"
            })
        
        # Inactive agent recommendations
        if summary["inactive_agents"] > 0:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "monitoring",
                "title": "Address Inactive Agents",
                "description": f"{summary['inactive_agents']} agents are inactive",
                "action": "Investigate why agents are not reporting regularly",
                "impact": "Improved real-time monitoring"
            })
        
        return recommendations
    
    async def _handle_get_wazuh_rules_summary(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive rules analysis with usage statistics and coverage assessment."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_rules_summary_query(arguments)
            
            # Fetch rules data
            rules_data = await self._fetch_rules_for_analysis(validated_query)
            
            # Analyze rules and generate comprehensive report
            analysis = await self._analyze_rules_summary(rules_data, validated_query, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except ValidationError as e:
            self.logger.error(f"Validation error in rules summary analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
        except Exception as e:
            self.logger.error(f"Error in rules summary analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
    
    async def _fetch_rules_for_analysis(self, query) -> Dict[str, Any]:
        """Fetch rules data for comprehensive analysis."""
        try:
            # Build query parameters
            query_params = {"limit": min(query.max_rules, 2000)}
            
            # Apply status filter
            if query.status_filter == "enabled":
                query_params["status"] = "enabled"
            elif query.status_filter == "disabled":
                query_params["status"] = "disabled"
            # For "all", don't add status filter
            
            # Apply level filter
            if query.rule_level_filter:
                query_params["level"] = ",".join(map(str, query.rule_level_filter))
            
            # Apply group filter
            if query.rule_group_filter:
                query_params["group"] = ",".join(query.rule_group_filter)
            
            # Apply rule ID filter
            if query.rule_id_filter:
                query_params["rule_ids"] = ",".join(map(str, query.rule_id_filter))
            
            # Fetch rules
            rules_response = await self.api_client.get_rules(**query_params)
            rules = rules_response.get("data", {}).get("affected_items", [])
            
            self.logger.info(f"Fetched {len(rules)} rules for analysis")
            
            # Enhance rules with additional metadata
            enhanced_rules = []
            for rule in rules:
                enhanced_rule = dict(rule)
                enhanced_rule["analysis_timestamp"] = datetime.utcnow().isoformat()
                enhanced_rules.append(enhanced_rule)
            
            # Fetch recent alerts for usage statistics if requested
            usage_stats = {}
            if query.include_usage_stats and rules:
                usage_stats = await self._fetch_rules_usage_statistics(enhanced_rules[:100])  # Sample for performance
            
            return {
                "rules": enhanced_rules,
                "total_count": len(enhanced_rules),
                "query_params": query_params,
                "usage_stats": usage_stats
            }
            
        except Exception as e:
            self.logger.error(f"Error fetching rules for analysis: {str(e)}")
            raise
    
    async def _fetch_rules_usage_statistics(self, rules: List[Dict]) -> Dict[str, Any]:
        """Fetch usage statistics for rules based on recent alerts."""
        usage_stats = {
            "rule_frequencies": {},
            "most_active_rules": [],
            "silent_rules": [],
            "analysis_period": "24h",
            "collection_errors": []
        }
        
        try:
            # Get recent alerts (last 24 hours)
            alerts_response = await self.api_client.get_alerts(
                limit=5000,
                time_range=86400  # 24 hours
            )
            
            alerts = alerts_response.get("data", {}).get("affected_items", [])
            
            # Count rule usage
            rule_counts = {}
            for alert in alerts:
                rule_id = alert.get("rule", {}).get("id")
                if rule_id:
                    rule_counts[rule_id] = rule_counts.get(rule_id, 0) + 1
            
            # Map to rule details
            for rule in rules:
                rule_id = rule.get("id")
                if rule_id:
                    frequency = rule_counts.get(rule_id, 0)
                    usage_stats["rule_frequencies"][rule_id] = {
                        "rule_id": rule_id,
                        "description": rule.get("description", ""),
                        "level": rule.get("level", 0),
                        "frequency": frequency,
                        "last_24h_hits": frequency
                    }
            
            # Identify most active rules
            sorted_rules = sorted(
                usage_stats["rule_frequencies"].values(),
                key=lambda x: x["frequency"],
                reverse=True
            )
            usage_stats["most_active_rules"] = sorted_rules[:10]
            
            # Identify silent rules (no hits in 24h)
            usage_stats["silent_rules"] = [
                rule for rule in sorted_rules if rule["frequency"] == 0
            ]
            
        except Exception as e:
            self.logger.warning(f"Error collecting rule usage statistics: {str(e)}")
            usage_stats["collection_errors"].append(str(e))
        
        return usage_stats
    
    async def _analyze_rules_summary(self, rules_data: Dict, query, start_time: datetime) -> Dict[str, Any]:
        """Perform comprehensive analysis of rules summary."""
        rules = rules_data.get("rules", [])
        usage_stats = rules_data.get("usage_stats", {})
        current_time = datetime.utcnow()
        
        # Initialize analysis structure
        analysis = {
            "query_parameters": {
                "rule_level_filter": query.rule_level_filter,
                "rule_group_filter": query.rule_group_filter,
                "rule_id_filter": query.rule_id_filter,
                "category_filter": query.category_filter,
                "status_filter": query.status_filter,
                "include_disabled": query.include_disabled,
                "include_usage_stats": query.include_usage_stats,
                "include_coverage_analysis": query.include_coverage_analysis,
                "group_by": query.group_by,
                "sort_by": query.sort_by,
                "max_rules": query.max_rules
            },
            "summary": {},
            "grouped_analysis": {},
            "coverage_analysis": {},
            "usage_analysis": {},
            "rule_details": [],
            "recommendations": [],
            "analysis_metadata": {
                "timestamp": current_time.isoformat(),
                "processing_time_seconds": (current_time - start_time).total_seconds(),
                "total_rules_analyzed": len(rules)
            }
        }
        
        if not rules:
            analysis["summary"] = {
                "total_rules": 0,
                "enabled_rules": 0,
                "disabled_rules": 0,
                "message": "No rules found matching the specified criteria"
            }
            return analysis
        
        # Analyze rule distribution
        level_counts = {}
        group_counts = {}
        file_counts = {}
        status_counts = {}
        category_counts = {}
        
        high_priority_rules = []
        custom_rules = []
        
        for rule in rules:
            # Level analysis
            level = rule.get("level", 0)
            level_counts[level] = level_counts.get(level, 0) + 1
            
            # Group analysis
            groups = rule.get("groups", [])
            for group in groups:
                group_counts[group] = group_counts.get(group, 0) + 1
                
                # Categorize by common groups
                if group.lower() in query.category_filter or [] if query.category_filter else True:
                    category_counts[group] = category_counts.get(group, 0) + 1
            
            # File analysis
            filename = rule.get("filename", "unknown")
            file_counts[filename] = file_counts.get(filename, 0) + 1
            
            # Status analysis
            status = rule.get("status", "enabled")
            status_counts[status] = status_counts.get(status, 0) + 1
            
            # Identify high priority rules (level >= 10)
            if level >= 10:
                high_priority_rules.append(rule)
            
            # Identify custom rules (not in standard files)
            if filename not in ["ossec_rules.xml", "syslog_rules.xml", "iptables_rules.xml"]:
                if not filename.startswith("0"):  # Standard Wazuh rules start with numbers
                    custom_rules.append(rule)
        
        # Build summary
        analysis["summary"] = {
            "total_rules": len(rules),
            "enabled_rules": status_counts.get("enabled", 0),
            "disabled_rules": status_counts.get("disabled", 0),
            "high_priority_rules": len(high_priority_rules),
            "custom_rules": len(custom_rules),
            "unique_levels": len(level_counts),
            "unique_groups": len(group_counts),
            "unique_files": len(file_counts),
            "coverage_score": self._calculate_rules_coverage_score(rules, group_counts),
            "analysis_timestamp": current_time.isoformat()
        }
        
        # Build grouped analysis
        analysis["grouped_analysis"] = {
            "grouping_field": query.group_by,
            "groups": {}
        }
        
        if query.group_by == "level":
            analysis["grouped_analysis"]["groups"] = {
                f"level_{level}": {
                    "count": count,
                    "percentage": round((count / len(rules)) * 100, 2),
                    "severity": self._get_level_severity(level)
                }
                for level, count in sorted(level_counts.items())
            }
        elif query.group_by == "group":
            analysis["grouped_analysis"]["groups"] = {
                group: {
                    "count": count,
                    "percentage": round((count / len(rules)) * 100, 2)
                }
                for group, count in sorted(group_counts.items(), key=lambda x: x[1], reverse=True)[:20]  # Top 20
            }
        elif query.group_by == "file":
            analysis["grouped_analysis"]["groups"] = {
                filename: {
                    "count": count,
                    "percentage": round((count / len(rules)) * 100, 2),
                    "is_custom": not filename.startswith("0") and filename not in ["ossec_rules.xml", "syslog_rules.xml"]
                }
                for filename, count in sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:15]  # Top 15
            }
        elif query.group_by == "status":
            analysis["grouped_analysis"]["groups"] = {
                status: {
                    "count": count,
                    "percentage": round((count / len(rules)) * 100, 2)
                }
                for status, count in status_counts.items()
            }
        
        # Coverage analysis
        if query.include_coverage_analysis:
            analysis["coverage_analysis"] = self._analyze_security_coverage(rules, group_counts)
        
        # Usage analysis
        if query.include_usage_stats and usage_stats:
            analysis["usage_analysis"] = self._analyze_rules_usage(usage_stats, rules)
        
        # Add detailed rule information (sorted by query.sort_by)
        sorted_rules = self._sort_rules(rules, query.sort_by, usage_stats.get("rule_frequencies", {}))
        analysis["rule_details"] = [
            {
                "id": rule.get("id"),
                "description": rule.get("description", ""),
                "level": rule.get("level", 0),
                "groups": rule.get("groups", []),
                "filename": rule.get("filename", "unknown"),
                "status": rule.get("status", "enabled"),
                "frequency": usage_stats.get("rule_frequencies", {}).get(rule.get("id"), {}).get("frequency", 0)
            }
            for rule in sorted_rules[:50]  # Top 50 for performance
        ]
        
        # Generate recommendations
        analysis["recommendations"] = self._generate_rules_recommendations(
            analysis["summary"], analysis["coverage_analysis"], usage_stats
        )
        
        return analysis
    
    def _calculate_rules_coverage_score(self, rules: List[Dict], group_counts: Dict) -> float:
        """Calculate security coverage score based on rule distribution."""
        if not rules:
            return 0.0
        
        # Essential security categories
        essential_categories = [
            "authentication", "firewall", "ids", "syscheck", "rootcheck",
            "attack", "web", "malware", "vulnerability", "compliance"
        ]
        
        # Calculate coverage
        covered_categories = 0
        for category in essential_categories:
            if any(category in group.lower() for group in group_counts.keys()):
                covered_categories += 1
        
        base_score = (covered_categories / len(essential_categories)) * 100
        
        # Bonus for high rule count (indicates comprehensive coverage)
        rule_count_bonus = min(len(rules) / 100, 10)  # Max 10 point bonus
        
        # Bonus for custom rules (indicates organization-specific coverage)
        custom_rule_bonus = min(len([r for r in rules if not r.get("filename", "").startswith("0")]) / 10, 5)
        
        final_score = min(base_score + rule_count_bonus + custom_rule_bonus, 100)
        return round(final_score, 2)
    
    def _get_level_severity(self, level: int) -> str:
        """Get severity name for rule level."""
        if level >= 12:
            return "critical"
        elif level >= 8:
            return "high"
        elif level >= 4:
            return "medium"
        else:
            return "low"
    
    def _analyze_security_coverage(self, rules: List[Dict], group_counts: Dict) -> Dict[str, Any]:
        """Analyze security coverage across different categories."""
        coverage = {
            "category_coverage": {},
            "coverage_gaps": [],
            "strength_areas": [],
            "recommendations": []
        }
        
        # Define security categories and their importance
        security_categories = {
            "authentication": {"weight": 10, "min_rules": 5},
            "firewall": {"weight": 8, "min_rules": 3},
            "ids": {"weight": 9, "min_rules": 10},
            "attack": {"weight": 10, "min_rules": 15},
            "web": {"weight": 7, "min_rules": 8},
            "malware": {"weight": 9, "min_rules": 5},
            "vulnerability": {"weight": 8, "min_rules": 3},
            "compliance": {"weight": 6, "min_rules": 5},
            "syscheck": {"weight": 7, "min_rules": 3},
            "rootcheck": {"weight": 6, "min_rules": 2}
        }
        
        # Analyze coverage for each category
        for category, specs in security_categories.items():
            category_rules = [
                rule for rule in rules
                if any(category in group.lower() for group in rule.get("groups", []))
            ]
            
            rule_count = len(category_rules)
            coverage_score = min((rule_count / specs["min_rules"]) * 100, 100)
            
            coverage["category_coverage"][category] = {
                "rule_count": rule_count,
                "min_recommended": specs["min_rules"],
                "coverage_score": round(coverage_score, 2),
                "weight": specs["weight"],
                "status": "excellent" if coverage_score >= 100 else
                         "good" if coverage_score >= 75 else
                         "adequate" if coverage_score >= 50 else "poor"
            }
            
            # Identify gaps and strengths
            if coverage_score < 50:
                coverage["coverage_gaps"].append({
                    "category": category,
                    "current_rules": rule_count,
                    "recommended_rules": specs["min_rules"],
                    "priority": "high" if specs["weight"] >= 9 else "medium"
                })
            elif coverage_score >= 100:
                coverage["strength_areas"].append({
                    "category": category,
                    "rule_count": rule_count,
                    "coverage_score": coverage_score
                })
        
        return coverage
    
    def _analyze_rules_usage(self, usage_stats: Dict, rules: List[Dict]) -> Dict[str, Any]:
        """Analyze rule usage patterns and efficiency."""
        usage_analysis = {
            "efficiency_metrics": {},
            "activity_distribution": {},
            "optimization_opportunities": [],
            "trending_rules": []
        }
        
        rule_frequencies = usage_stats.get("rule_frequencies", {})
        most_active = usage_stats.get("most_active_rules", [])
        silent_rules = usage_stats.get("silent_rules", [])
        
        # Calculate efficiency metrics
        total_rules = len(rules)
        active_rules = len([r for r in rule_frequencies.values() if r["frequency"] > 0])
        
        usage_analysis["efficiency_metrics"] = {
            "total_rules": total_rules,
            "active_rules": active_rules,
            "silent_rules": len(silent_rules),
            "utilization_rate": round((active_rules / total_rules) * 100, 2) if total_rules > 0 else 0,
            "most_active_count": len(most_active),
            "analysis_period": "24h"
        }
        
        # Activity distribution
        if rule_frequencies:
            frequencies = [r["frequency"] for r in rule_frequencies.values()]
            usage_analysis["activity_distribution"] = {
                "mean_frequency": round(sum(frequencies) / len(frequencies), 2),
                "max_frequency": max(frequencies),
                "min_frequency": min(frequencies),
                "rules_with_high_activity": len([f for f in frequencies if f > 100]),
                "rules_with_medium_activity": len([f for f in frequencies if 10 <= f <= 100]),
                "rules_with_low_activity": len([f for f in frequencies if 1 <= f < 10])
            }
        
        # Optimization opportunities
        if len(silent_rules) > total_rules * 0.3:  # More than 30% silent
            usage_analysis["optimization_opportunities"].append({
                "type": "excessive_silent_rules",
                "description": f"{len(silent_rules)} rules have no activity in 24h",
                "impact": "Consider reviewing rule relevance",
                "priority": "medium"
            })
        
        # Trending rules (high activity rules)
        if most_active:
            usage_analysis["trending_rules"] = most_active[:5]
        
        return usage_analysis
    
    def _sort_rules(self, rules: List[Dict], sort_by: str, frequencies: Dict) -> List[Dict]:
        """Sort rules based on specified criteria."""
        if sort_by == "level":
            return sorted(rules, key=lambda x: x.get("level", 0), reverse=True)
        elif sort_by == "id":
            return sorted(rules, key=lambda x: x.get("id", 0))
        elif sort_by == "group":
            return sorted(rules, key=lambda x: x.get("groups", [""])[0] if x.get("groups") else "")
        elif sort_by == "frequency":
            return sorted(rules, key=lambda x: frequencies.get(x.get("id"), {}).get("frequency", 0), reverse=True)
        elif sort_by == "file":
            return sorted(rules, key=lambda x: x.get("filename", ""))
        else:
            return rules
    
    def _generate_rules_recommendations(self, summary: Dict, coverage: Dict, usage_stats: Dict) -> List[Dict]:
        """Generate recommendations based on rules analysis."""
        recommendations = []
        
        # Coverage recommendations
        if coverage and "coverage_gaps" in coverage:
            for gap in coverage["coverage_gaps"]:
                recommendations.append({
                    "priority": gap["priority"].upper(),
                    "category": "coverage",
                    "title": f"Improve {gap['category'].title()} Rule Coverage",
                    "description": f"Only {gap['current_rules']} rules for {gap['category']}, recommended: {gap['recommended_rules']}",
                    "action": f"Add more {gap['category']} detection rules",
                    "impact": "Enhanced security monitoring coverage"
                })
        
        # Disabled rules recommendations
        if summary["disabled_rules"] > 0:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "optimization",
                "title": "Review Disabled Rules",
                "description": f"{summary['disabled_rules']} rules are disabled",
                "action": "Review disabled rules for potential re-enablement",
                "impact": "Improved detection capabilities"
            })
        
        # Usage efficiency recommendations
        if usage_stats and "rule_frequencies" in usage_stats:
            silent_count = len(usage_stats.get("silent_rules", []))
            total_rules = len(usage_stats.get("rule_frequencies", {}))
            
            if silent_count > total_rules * 0.5:  # More than 50% silent
                recommendations.append({
                    "priority": "LOW",
                    "category": "efficiency",
                    "title": "Optimize Silent Rules",
                    "description": f"{silent_count} rules have no recent activity",
                    "action": "Review silent rules for relevance and optimization",
                    "impact": "Improved rule set efficiency"
                })
        
        # Custom rules recommendations
        if summary["custom_rules"] == 0:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "customization",
                "title": "Develop Custom Rules",
                "description": "No custom rules detected",
                "action": "Consider developing organization-specific detection rules",
                "impact": "Enhanced tailored threat detection"
            })
        
        return recommendations
    
    async def _handle_get_wazuh_weekly_stats(self, arguments: dict) -> list[types.TextContent]:
        """Handle weekly statistics analysis with trends and forecasting."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_weekly_stats_query(arguments)
            
            # Calculate date ranges
            date_ranges = self._calculate_weekly_date_ranges(validated_query)
            
            # Fetch metrics data
            metrics_data = await self._fetch_weekly_metrics(date_ranges, validated_query)
            
            # Analyze metrics and generate report
            analysis = await self._analyze_weekly_stats(metrics_data, validated_query, date_ranges, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except ValidationError as e:
            self.logger.error(f"Validation error in weekly stats analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
        except Exception as e:
            self.logger.error(f"Error in weekly stats analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
    
    def _calculate_weekly_date_ranges(self, query) -> List[Dict[str, Any]]:
        """Calculate date ranges for weekly analysis."""
        ranges = []
        
        if query.start_date:
            # Use custom start date
            start = datetime.strptime(query.start_date, "%Y-%m-%d")
        else:
            # Use current date minus weeks
            start = datetime.utcnow() - timedelta(weeks=query.weeks)
        
        # Calculate weekly ranges
        for week_num in range(query.weeks):
            week_start = start + timedelta(weeks=week_num)
            week_end = week_start + timedelta(days=7)
            
            ranges.append({
                "week_number": week_num + 1,
                "start_date": week_start,
                "end_date": week_end,
                "start_iso": week_start.isoformat() + "Z",
                "end_iso": week_end.isoformat() + "Z",
                "label": f"Week {week_num + 1} ({week_start.strftime('%Y-%m-%d')} - {week_end.strftime('%Y-%m-%d')})"
            })
        
        return ranges
    
    async def _fetch_weekly_metrics(self, date_ranges: List[Dict], query) -> Dict[str, Any]:
        """Fetch comprehensive metrics for weekly analysis."""
        metrics_data = {
            "date_ranges": date_ranges,
            "metrics": {},
            "raw_data": {},
            "collection_errors": []
        }
        
        # Determine which metrics to collect
        if query.metrics:
            metrics_to_collect = query.metrics
        else:
            # Default metrics
            metrics_to_collect = ["alerts", "events", "agents", "vulnerabilities"]
        
        for week_range in date_ranges:
            week_label = week_range["label"]
            week_data = {}
            
            # Collect metrics for each type
            for metric_type in metrics_to_collect:
                try:
                    if metric_type == "alerts":
                        week_data["alerts"] = await self._fetch_weekly_alerts(
                            week_range, query.agent_filter, query.rule_filter
                        )
                    elif metric_type == "events":
                        week_data["events"] = await self._fetch_weekly_events(
                            week_range, query.agent_filter
                        )
                    elif metric_type == "agents":
                        week_data["agents"] = await self._fetch_weekly_agent_stats(
                            week_range
                        )
                    elif metric_type == "vulnerabilities":
                        week_data["vulnerabilities"] = await self._fetch_weekly_vulnerabilities(
                            week_range, query.agent_filter
                        )
                    elif metric_type == "authentication":
                        week_data["authentication"] = await self._fetch_weekly_auth_stats(
                            week_range, query.agent_filter
                        )
                    elif metric_type == "compliance":
                        week_data["compliance"] = await self._fetch_weekly_compliance_stats(
                            week_range
                        )
                    elif metric_type == "network":
                        week_data["network"] = await self._fetch_weekly_network_stats(
                            week_range, query.agent_filter
                        )
                    elif metric_type == "files":
                        week_data["files"] = await self._fetch_weekly_file_stats(
                            week_range, query.agent_filter
                        )
                except Exception as e:
                    self.logger.warning(f"Error collecting {metric_type} for {week_label}: {str(e)}")
                    metrics_data["collection_errors"].append({
                        "week": week_label,
                        "metric": metric_type,
                        "error": str(e)
                    })
                    week_data[metric_type] = {"error": str(e), "data": {}}
            
            metrics_data["raw_data"][week_label] = week_data
        
        return metrics_data
    
    async def _fetch_weekly_alerts(self, week_range: Dict, agent_filter: List[str], rule_filter: List[str]) -> Dict:
        """Fetch alert metrics for a specific week."""
        try:
            # Build query parameters
            query_params = {
                "limit": 10000,
                "pretty": True,
                "q": f"timestamp>{week_range['start_iso']};timestamp<{week_range['end_iso']}"
            }
            
            # Apply filters
            if agent_filter:
                query_params["agent_list"] = ",".join(agent_filter)
            if rule_filter:
                query_params["rule_id"] = ",".join(rule_filter)
            
            # Fetch alerts
            alerts_response = await self.api_client.get_alerts(**query_params)
            alerts = alerts_response.get("data", {}).get("affected_items", [])
            
            # Analyze alerts
            alert_stats = {
                "total_alerts": len(alerts),
                "severity_distribution": {},
                "top_rules": {},
                "top_agents": {},
                "hourly_distribution": [0] * 24,
                "daily_distribution": {}
            }
            
            # Process alerts
            for alert in alerts:
                # Severity distribution
                level = alert.get("rule", {}).get("level", 0)
                severity = self._map_level_to_severity(level)
                alert_stats["severity_distribution"][severity] = \
                    alert_stats["severity_distribution"].get(severity, 0) + 1
                
                # Top rules
                rule_id = alert.get("rule", {}).get("id")
                if rule_id:
                    if rule_id not in alert_stats["top_rules"]:
                        alert_stats["top_rules"][rule_id] = {
                            "count": 0,
                            "description": alert.get("rule", {}).get("description", "")
                        }
                    alert_stats["top_rules"][rule_id]["count"] += 1
                
                # Top agents
                agent_name = alert.get("agent", {}).get("name", "Unknown")
                alert_stats["top_agents"][agent_name] = \
                    alert_stats["top_agents"].get(agent_name, 0) + 1
                
                # Time distribution
                try:
                    timestamp = datetime.fromisoformat(alert.get("timestamp", "").replace("Z", "+00:00"))
                    hour = timestamp.hour
                    day = timestamp.strftime("%Y-%m-%d")
                    
                    alert_stats["hourly_distribution"][hour] += 1
                    alert_stats["daily_distribution"][day] = \
                        alert_stats["daily_distribution"].get(day, 0) + 1
                except Exception:
                    pass
            
            # Sort and limit top items
            alert_stats["top_rules"] = dict(sorted(
                alert_stats["top_rules"].items(),
                key=lambda x: x[1]["count"],
                reverse=True
            )[:10])
            
            alert_stats["top_agents"] = dict(sorted(
                alert_stats["top_agents"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10])
            
            return alert_stats
            
        except Exception as e:
            self.logger.error(f"Error fetching weekly alerts: {str(e)}")
            return {"error": str(e), "total_alerts": 0}
    
    async def _fetch_weekly_events(self, week_range: Dict, agent_filter: List[str]) -> Dict:
        """Fetch event metrics for a specific week."""
        # Simplified implementation - would need actual event API endpoint
        return {
            "total_events": 0,
            "event_types": {},
            "agent_distribution": {},
            "note": "Event metrics require specific API endpoint configuration"
        }
    
    async def _fetch_weekly_agent_stats(self, week_range: Dict) -> Dict:
        """Fetch agent statistics for a specific week."""
        try:
            # Get all agents
            agents_response = await self.api_client.get_agents()
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            # Analyze agent status during the week
            agent_stats = {
                "total_agents": len(agents),
                "active_agents": 0,
                "disconnected_agents": 0,
                "new_agents": 0,
                "removed_agents": 0,
                "os_distribution": {},
                "version_distribution": {}
            }
            
            for agent in agents:
                # Status
                status = agent.get("status", "unknown")
                if status == "active":
                    agent_stats["active_agents"] += 1
                elif status == "disconnected":
                    agent_stats["disconnected_agents"] += 1
                
                # Check if agent was registered during this week
                register_date = agent.get("dateAdd")
                if register_date:
                    try:
                        reg_datetime = datetime.fromisoformat(register_date.replace("Z", "+00:00"))
                        if week_range["start_date"] <= reg_datetime <= week_range["end_date"]:
                            agent_stats["new_agents"] += 1
                    except Exception:
                        pass
                
                # OS distribution
                os_platform = agent.get("os", {}).get("platform", "unknown")
                agent_stats["os_distribution"][os_platform] = \
                    agent_stats["os_distribution"].get(os_platform, 0) + 1
                
                # Version distribution
                version = agent.get("version", "unknown")
                agent_stats["version_distribution"][version] = \
                    agent_stats["version_distribution"].get(version, 0) + 1
            
            return agent_stats
            
        except Exception as e:
            self.logger.error(f"Error fetching weekly agent stats: {str(e)}")
            return {"error": str(e), "total_agents": 0}
    
    async def _fetch_weekly_vulnerabilities(self, week_range: Dict, agent_filter: List[str]) -> Dict:
        """Fetch vulnerability metrics for a specific week."""
        try:
            vuln_stats = {
                "total_vulnerabilities": 0,
                "critical_vulnerabilities": 0,
                "high_vulnerabilities": 0,
                "severity_distribution": {},
                "top_cves": {},
                "affected_agents": set()
            }
            
            # Get agents to check
            if agent_filter:
                agents_to_check = agent_filter
            else:
                agents_response = await self.api_client.get_agents(limit=50)  # Sample
                agents = agents_response.get("data", {}).get("affected_items", [])
                agents_to_check = [agent["id"] for agent in agents[:20]]  # Limit for performance
            
            # Fetch vulnerabilities for each agent
            for agent_id in agents_to_check:
                try:
                    vuln_response = await self.api_client.get_agent_vulnerabilities(agent_id)
                    vulns = vuln_response.get("data", {}).get("affected_items", [])
                    
                    for vuln in vulns:
                        vuln_stats["total_vulnerabilities"] += 1
                        vuln_stats["affected_agents"].add(agent_id)
                        
                        # Severity analysis
                        severity = vuln.get("severity", "unknown")
                        vuln_stats["severity_distribution"][severity] = \
                            vuln_stats["severity_distribution"].get(severity, 0) + 1
                        
                        if severity == "critical":
                            vuln_stats["critical_vulnerabilities"] += 1
                        elif severity == "high":
                            vuln_stats["high_vulnerabilities"] += 1
                        
                        # Top CVEs
                        cve = vuln.get("cve", "unknown")
                        if cve not in vuln_stats["top_cves"]:
                            vuln_stats["top_cves"][cve] = {
                                "count": 0,
                                "severity": severity,
                                "cvss_score": self._extract_cvss_score(vuln)
                            }
                        vuln_stats["top_cves"][cve]["count"] += 1
                        
                except Exception as e:
                    self.logger.warning(f"Error fetching vulnerabilities for agent {agent_id}: {str(e)}")
            
            # Convert set to count
            vuln_stats["affected_agents"] = len(vuln_stats["affected_agents"])
            
            # Sort and limit top CVEs
            vuln_stats["top_cves"] = dict(sorted(
                vuln_stats["top_cves"].items(),
                key=lambda x: x[1]["count"],
                reverse=True
            )[:10])
            
            return vuln_stats
            
        except Exception as e:
            self.logger.error(f"Error fetching weekly vulnerabilities: {str(e)}")
            return {"error": str(e), "total_vulnerabilities": 0}
    
    async def _fetch_weekly_auth_stats(self, week_range: Dict, agent_filter: List[str]) -> Dict:
        """Fetch authentication statistics for a specific week."""
        try:
            # Query authentication-related alerts
            query_params = {
                "limit": 5000,
                "q": f"timestamp>{week_range['start_iso']};timestamp<{week_range['end_iso']};rule.groups=authentication"
            }
            
            if agent_filter:
                query_params["agent_list"] = ",".join(agent_filter)
            
            auth_response = await self.api_client.get_alerts(**query_params)
            auth_alerts = auth_response.get("data", {}).get("affected_items", [])
            
            auth_stats = {
                "total_auth_events": len(auth_alerts),
                "successful_logins": 0,
                "failed_logins": 0,
                "authentication_methods": {},
                "top_users": {},
                "top_sources": {},
                "suspicious_activity": []
            }
            
            # Analyze authentication events
            failed_attempts = {}
            
            for alert in auth_alerts:
                rule_desc = alert.get("rule", {}).get("description", "").lower()
                
                if "success" in rule_desc or "logged" in rule_desc:
                    auth_stats["successful_logins"] += 1
                elif "fail" in rule_desc or "invalid" in rule_desc:
                    auth_stats["failed_logins"] += 1
                    
                    # Track failed attempts by source
                    src_ip = alert.get("data", {}).get("srcip", "unknown")
                    if src_ip != "unknown":
                        failed_attempts[src_ip] = failed_attempts.get(src_ip, 0) + 1
                
                # Extract user info
                user = alert.get("data", {}).get("srcuser", alert.get("data", {}).get("dstuser", "unknown"))
                if user != "unknown":
                    auth_stats["top_users"][user] = auth_stats["top_users"].get(user, 0) + 1
                
                # Extract source IPs
                src_ip = alert.get("data", {}).get("srcip", "unknown")
                if src_ip != "unknown":
                    auth_stats["top_sources"][src_ip] = auth_stats["top_sources"].get(src_ip, 0) + 1
            
            # Identify suspicious activity (multiple failed attempts)
            for src_ip, count in failed_attempts.items():
                if count >= 5:  # Threshold for suspicious
                    auth_stats["suspicious_activity"].append({
                        "source_ip": src_ip,
                        "failed_attempts": count,
                        "risk_level": "high" if count >= 10 else "medium"
                    })
            
            # Sort and limit results
            auth_stats["top_users"] = dict(sorted(
                auth_stats["top_users"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10])
            
            auth_stats["top_sources"] = dict(sorted(
                auth_stats["top_sources"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10])
            
            return auth_stats
            
        except Exception as e:
            self.logger.error(f"Error fetching weekly auth stats: {str(e)}")
            return {"error": str(e), "total_auth_events": 0}
    
    async def _fetch_weekly_compliance_stats(self, week_range: Dict) -> Dict:
        """Fetch compliance statistics for a specific week."""
        # Simplified implementation
        return {
            "compliance_score": 0,
            "compliance_checks": {},
            "failed_controls": [],
            "note": "Compliance metrics require specific configuration"
        }
    
    async def _fetch_weekly_network_stats(self, week_range: Dict, agent_filter: List[str]) -> Dict:
        """Fetch network statistics for a specific week."""
        # Simplified implementation
        return {
            "network_connections": 0,
            "top_protocols": {},
            "top_ports": {},
            "note": "Network metrics require specific configuration"
        }
    
    async def _fetch_weekly_file_stats(self, week_range: Dict, agent_filter: List[str]) -> Dict:
        """Fetch file integrity statistics for a specific week."""
        try:
            # Query file integrity monitoring alerts
            query_params = {
                "limit": 5000,
                "q": f"timestamp>{week_range['start_iso']};timestamp<{week_range['end_iso']};rule.groups=syscheck"
            }
            
            if agent_filter:
                query_params["agent_list"] = ",".join(agent_filter)
            
            fim_response = await self.api_client.get_alerts(**query_params)
            fim_alerts = fim_response.get("data", {}).get("affected_items", [])
            
            file_stats = {
                "total_file_changes": len(fim_alerts),
                "files_added": 0,
                "files_modified": 0,
                "files_deleted": 0,
                "top_changed_files": {},
                "top_directories": {},
                "critical_changes": []
            }
            
            for alert in fim_alerts:
                rule_desc = alert.get("rule", {}).get("description", "").lower()
                
                if "added" in rule_desc or "created" in rule_desc:
                    file_stats["files_added"] += 1
                elif "modified" in rule_desc or "changed" in rule_desc:
                    file_stats["files_modified"] += 1
                elif "deleted" in rule_desc or "removed" in rule_desc:
                    file_stats["files_deleted"] += 1
                
                # Extract file path
                file_path = alert.get("syscheck", {}).get("path", "unknown")
                if file_path != "unknown":
                    file_stats["top_changed_files"][file_path] = \
                        file_stats["top_changed_files"].get(file_path, 0) + 1
                    
                    # Extract directory
                    directory = "/".join(file_path.split("/")[:-1]) or "/"
                    file_stats["top_directories"][directory] = \
                        file_stats["top_directories"].get(directory, 0) + 1
                    
                    # Check for critical files
                    if any(critical in file_path.lower() for critical in 
                          ["passwd", "shadow", "sudoers", "ssh", "config"]):
                        file_stats["critical_changes"].append({
                            "file": file_path,
                            "change_type": rule_desc,
                            "timestamp": alert.get("timestamp", ""),
                            "agent": alert.get("agent", {}).get("name", "unknown")
                        })
            
            # Sort and limit results
            file_stats["top_changed_files"] = dict(sorted(
                file_stats["top_changed_files"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10])
            
            file_stats["top_directories"] = dict(sorted(
                file_stats["top_directories"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10])
            
            return file_stats
            
        except Exception as e:
            self.logger.error(f"Error fetching weekly file stats: {str(e)}")
            return {"error": str(e), "total_file_changes": 0}
    
    async def _analyze_weekly_stats(self, metrics_data: Dict, query, date_ranges: List[Dict], 
                                   start_time: datetime) -> Dict[str, Any]:
        """Analyze weekly statistics with advanced statistical analysis and anomaly detection."""
        current_time = datetime.utcnow()
        
        # Initialize enhanced analysis structure
        analysis = {
            "query_parameters": {
                "weeks": query.weeks,
                "start_date": query.start_date,
                "metrics": query.metrics,
                "include_trends": query.include_trends,
                "include_comparison": query.include_comparison,
                "include_forecasting": query.include_forecasting,
                "include_predictions": query.include_predictions,
                "anomaly_detection": query.anomaly_detection,
                "seasonality_analysis": query.seasonality_analysis,
                "behavioral_analysis": query.behavioral_analysis,
                "statistical_analysis": query.statistical_analysis,
                "compare_weeks": query.compare_weeks,
                "anomaly_threshold": query.anomaly_threshold,
                "group_by": query.group_by,
                "agent_filter": query.agent_filter,
                "rule_filter": query.rule_filter,
                "output_format": query.output_format
            },
            "summary": {},
            "weekly_metrics": {},
            "statistical_analysis": {},
            "trends": {},
            "comparisons": {},
            "anomaly_detection": {},
            "seasonality_analysis": {},
            "behavioral_analysis": {},
            "forecasting": {},
            "predictions": {},
            "insights": [],
            "recommendations": [],
            "analysis_metadata": {
                "timestamp": current_time.isoformat(),
                "processing_time_seconds": (current_time - start_time).total_seconds(),
                "date_ranges_analyzed": len(date_ranges),
                "collection_errors": len(metrics_data.get("collection_errors", []))
            }
        }
        
        # Process raw data into weekly metrics
        for week_range in date_ranges:
            week_label = week_range["label"]
            week_data = metrics_data["raw_data"].get(week_label, {})
            
            week_summary = {
                "week_number": week_range["week_number"],
                "date_range": f"{week_range['start_date'].strftime('%Y-%m-%d')} to {week_range['end_date'].strftime('%Y-%m-%d')}",
                "metrics": {}
            }
            
            # Enhanced metric summarization with statistical details
            for metric_type, metric_data in week_data.items():
                if isinstance(metric_data, dict) and "error" not in metric_data:
                    if metric_type == "alerts":
                        daily_counts = self._extract_daily_counts(metric_data.get("daily_distribution", {}))
                        week_summary["metrics"]["alerts"] = {
                            "total": metric_data.get("total_alerts", 0),
                            "daily_average": metric_data.get("total_alerts", 0) / 7,
                            "daily_counts": daily_counts,
                            "daily_variance": self._calculate_variance(daily_counts),
                            "severity_breakdown": metric_data.get("severity_distribution", {}),
                            "peak_hour": self._find_peak_hour(metric_data.get("hourly_distribution", [])),
                            "most_active_day": self._find_most_active_day(metric_data.get("daily_distribution", {}))
                        }
                    elif metric_type == "agents":
                        week_summary["metrics"]["agents"] = {
                            "total": metric_data.get("total_agents", 0),
                            "active": metric_data.get("active_agents", 0),
                            "new_this_week": metric_data.get("new_agents", 0),
                            "health_percentage": (metric_data.get("active_agents", 0) / 
                                                metric_data.get("total_agents", 1)) * 100,
                            "connectivity_stats": metric_data.get("connectivity_stats", {}),
                            "version_distribution": metric_data.get("version_distribution", {})
                        }
                    elif metric_type == "vulnerabilities":
                        week_summary["metrics"]["vulnerabilities"] = {
                            "total": metric_data.get("total_vulnerabilities", 0),
                            "critical": metric_data.get("critical_vulnerabilities", 0),
                            "high": metric_data.get("high_vulnerabilities", 0),
                            "medium": metric_data.get("medium_vulnerabilities", 0),
                            "low": metric_data.get("low_vulnerabilities", 0),
                            "affected_agents": metric_data.get("affected_agents", 0),
                            "new_this_week": metric_data.get("new_vulnerabilities", 0),
                            "patched_this_week": metric_data.get("patched_vulnerabilities", 0)
                        }
                    elif metric_type == "authentication":
                        week_summary["metrics"]["authentication"] = {
                            "total_events": metric_data.get("total_auth_events", 0),
                            "success_rate": (metric_data.get("successful_logins", 0) / 
                                           max(metric_data.get("total_auth_events", 1), 1)) * 100,
                            "failed_attempts": metric_data.get("failed_logins", 0),
                            "suspicious_sources": len(metric_data.get("suspicious_activity", [])),
                            "unique_users": metric_data.get("unique_users", 0),
                            "brute_force_attempts": metric_data.get("brute_force_attempts", 0)
                        }
                    elif metric_type == "files":
                        week_summary["metrics"]["files"] = {
                            "total_changes": metric_data.get("total_file_changes", 0),
                            "additions": metric_data.get("files_added", 0),
                            "modifications": metric_data.get("files_modified", 0),
                            "deletions": metric_data.get("files_deleted", 0),
                            "critical_changes": len(metric_data.get("critical_changes", [])),
                            "permission_changes": metric_data.get("permission_changes", 0),
                            "integrity_violations": metric_data.get("integrity_violations", 0)
                        }
            
            analysis["weekly_metrics"][week_label] = week_summary
        
        # Calculate enhanced summary statistics
        analysis["summary"] = self._calculate_enhanced_weekly_summary(analysis["weekly_metrics"])
        
        # Statistical analysis if requested
        if query.statistical_analysis:
            analysis["statistical_analysis"] = self._perform_statistical_analysis(
                analysis["weekly_metrics"], query.anomaly_threshold
            )
        
        # Anomaly detection if requested
        if query.anomaly_detection:
            analysis["anomaly_detection"] = self._detect_anomalies(
                analysis["weekly_metrics"], query.anomaly_threshold
            )
        
        # Seasonality analysis if requested
        if query.seasonality_analysis:
            analysis["seasonality_analysis"] = self._analyze_seasonality(
                analysis["weekly_metrics"], date_ranges
            )
        
        # Behavioral analysis if requested
        if query.behavioral_analysis:
            analysis["behavioral_analysis"] = self._analyze_behavioral_patterns(
                analysis["weekly_metrics"], metrics_data["raw_data"]
            )
        
        # Enhanced trend analysis if requested
        if query.include_trends:
            analysis["trends"] = self._analyze_enhanced_weekly_trends(
                analysis["weekly_metrics"], analysis.get("statistical_analysis", {})
            )
        
        # Enhanced comparisons if requested
        if query.include_comparison and len(date_ranges) > 1:
            analysis["comparisons"] = self._perform_enhanced_week_comparison(
                analysis["weekly_metrics"], query.compare_weeks
            )
        
        # Predictive analysis if requested
        if query.include_predictions:
            analysis["predictions"] = self._generate_predictions(
                analysis["weekly_metrics"], analysis.get("trends", {}), 
                analysis.get("seasonality_analysis", {})
            )
        
        # Basic forecasting if requested
        if query.include_forecasting:
            analysis["forecasting"] = self._forecast_next_week(analysis["weekly_metrics"])
        
        # Generate enhanced insights
        analysis["insights"] = self._generate_enhanced_weekly_insights(
            analysis["summary"], analysis["trends"], analysis.get("anomaly_detection", {}),
            analysis.get("behavioral_analysis", {}), metrics_data["raw_data"]
        )
        
        # Generate enhanced recommendations
        analysis["recommendations"] = self._generate_enhanced_weekly_recommendations(
            analysis["summary"], analysis["trends"], analysis["insights"],
            analysis.get("anomaly_detection", {}), analysis.get("predictions", {})
        )
        
        # Format output based on requested format
        if query.output_format == "minimal":
            # Return only summary and key metrics
            return {
                "summary": analysis["summary"],
                "key_anomalies": analysis.get("anomaly_detection", {}).get("high_priority_anomalies", [])[:3],
                "insights": analysis["insights"][:3],  # Top 3 insights
                "recommendations": analysis["recommendations"][:3]  # Top 3 recommendations
            }
        elif query.output_format == "summary":
            # Return summary with weekly totals and key analyses
            return {
                "summary": analysis["summary"],
                "weekly_totals": {
                    week: data["metrics"]
                    for week, data in analysis["weekly_metrics"].items()
                },
                "statistical_summary": analysis.get("statistical_analysis", {}).get("summary", {}),
                "anomaly_summary": analysis.get("anomaly_detection", {}).get("summary", {}),
                "insights": analysis["insights"],
                "recommendations": analysis["recommendations"]
            }
        else:
            # Return full detailed analysis
            return analysis

    def _extract_daily_counts(self, daily_distribution: dict) -> list:
        """Extract daily counts for statistical analysis."""
        days = ["monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"]
        return [daily_distribution.get(day, 0) for day in days]

    def _calculate_variance(self, values: list) -> float:
        """Calculate variance of a list of values."""
        if not values or len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return round(variance, 2)

    def _calculate_enhanced_weekly_summary(self, weekly_metrics: dict) -> dict:
        """Calculate enhanced summary statistics with statistical measures."""
        summary = {
            "total_weeks_analyzed": len(weekly_metrics),
            "metrics_summary": {},
            "statistical_overview": {},
            "data_quality": {}
        }
        
        # Collect all metrics across weeks
        all_metrics = {}
        for week_data in weekly_metrics.values():
            for metric_type, metric_values in week_data.get("metrics", {}).items():
                if metric_type not in all_metrics:
                    all_metrics[metric_type] = []
                all_metrics[metric_type].append(metric_values)
        
        # Calculate summary for each metric type
        for metric_type, values_list in all_metrics.items():
            if metric_type == "alerts":
                totals = [v.get("total", 0) for v in values_list]
                summary["metrics_summary"][metric_type] = {
                    "total_across_weeks": sum(totals),
                    "average_per_week": sum(totals) / len(totals) if totals else 0,
                    "peak_week": max(totals) if totals else 0,
                    "min_week": min(totals) if totals else 0,
                    "variance": self._calculate_variance(totals),
                    "coefficient_of_variation": self._calculate_cv(totals)
                }
            elif metric_type == "vulnerabilities":
                critical_counts = [v.get("critical", 0) for v in values_list]
                total_counts = [v.get("total", 0) for v in values_list]
                summary["metrics_summary"][metric_type] = {
                    "total_across_weeks": sum(total_counts),
                    "critical_total": sum(critical_counts),
                    "average_per_week": sum(total_counts) / len(total_counts) if total_counts else 0,
                    "critical_percentage": (sum(critical_counts) / max(sum(total_counts), 1)) * 100,
                    "variance": self._calculate_variance(total_counts)
                }
        
        return summary

    def _calculate_cv(self, values: list) -> float:
        """Calculate coefficient of variation."""
        if not values or len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        if mean == 0:
            return 0.0
        
        std_dev = (sum((x - mean) ** 2 for x in values) / len(values)) ** 0.5
        return round((std_dev / mean) * 100, 2)

    def _perform_statistical_analysis(self, weekly_metrics: dict, threshold: float) -> dict:
        """Perform comprehensive statistical analysis on weekly metrics."""
        analysis = {
            "summary": {},
            "metric_statistics": {},
            "outlier_detection": {},
            "distribution_analysis": {},
            "correlation_analysis": {}
        }
        
        # Extract time series data for each metric
        time_series = {}
        for week_data in weekly_metrics.values():
            for metric_type, metric_values in week_data.get("metrics", {}).items():
                if metric_type not in time_series:
                    time_series[metric_type] = []
                
                if metric_type == "alerts":
                    time_series[metric_type].append(metric_values.get("total", 0))
                elif metric_type == "vulnerabilities":
                    time_series[metric_type].append(metric_values.get("total", 0))
                elif metric_type == "authentication":
                    time_series[metric_type].append(metric_values.get("total_events", 0))
        
        # Statistical analysis for each metric
        for metric_type, values in time_series.items():
            if len(values) >= 2:
                mean = sum(values) / len(values)
                variance = sum((x - mean) ** 2 for x in values) / len(values)
                std_dev = variance ** 0.5
                
                analysis["metric_statistics"][metric_type] = {
                    "mean": round(mean, 2),
                    "variance": round(variance, 2),
                    "standard_deviation": round(std_dev, 2),
                    "min": min(values),
                    "max": max(values),
                    "range": max(values) - min(values),
                    "median": self._calculate_median(values),
                    "coefficient_of_variation": self._calculate_cv(values)
                }
                
                # Outlier detection using z-score
                outliers = []
                for i, value in enumerate(values):
                    if std_dev > 0:
                        z_score = abs((value - mean) / std_dev)
                        if z_score > threshold:
                            outliers.append({
                                "week_index": i + 1,
                                "value": value,
                                "z_score": round(z_score, 2),
                                "deviation_from_mean": round(value - mean, 2)
                            })
                
                analysis["outlier_detection"][metric_type] = outliers
        
        return analysis

    def _calculate_median(self, values: list) -> float:
        """Calculate median of a list of values."""
        if not values:
            return 0.0
        
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        if n % 2 == 0:
            return (sorted_values[n//2 - 1] + sorted_values[n//2]) / 2
        else:
            return sorted_values[n//2]

    def _detect_anomalies(self, weekly_metrics: dict, threshold: float) -> dict:
        """Detect anomalies in weekly metrics using multiple algorithms."""
        anomalies = {
            "summary": {},
            "high_priority_anomalies": [],
            "metric_anomalies": {},
            "pattern_anomalies": [],
            "temporal_anomalies": []
        }
        
        # Collect time series for anomaly detection
        for metric_type in ["alerts", "vulnerabilities", "authentication"]:
            values = []
            week_labels = []
            
            for week_label, week_data in weekly_metrics.items():
                week_labels.append(week_label)
                metric_data = week_data.get("metrics", {}).get(metric_type, {})
                
                if metric_type == "alerts":
                    values.append(metric_data.get("total", 0))
                elif metric_type == "vulnerabilities":
                    values.append(metric_data.get("total", 0))
                elif metric_type == "authentication":
                    values.append(metric_data.get("total_events", 0))
            
            if len(values) >= 3:  # Need at least 3 data points
                detected = self._detect_statistical_anomalies(values, week_labels, threshold)
                anomalies["metric_anomalies"][metric_type] = detected
                
                # Add high priority anomalies
                for anomaly in detected:
                    if anomaly.get("severity") == "high":
                        anomalies["high_priority_anomalies"].append({
                            "metric": metric_type,
                            "week": anomaly["week"],
                            "value": anomaly["value"],
                            "expected_range": anomaly["expected_range"],
                            "deviation": anomaly["deviation"]
                        })
        
        # Summary statistics
        total_anomalies = sum(len(metric_anomalies) for metric_anomalies in anomalies["metric_anomalies"].values())
        anomalies["summary"] = {
            "total_anomalies_detected": total_anomalies,
            "high_priority_count": len(anomalies["high_priority_anomalies"]),
            "anomaly_rate": (total_anomalies / len(weekly_metrics) * 100) if weekly_metrics else 0
        }
        
        return anomalies

    def _detect_statistical_anomalies(self, values: list, week_labels: list, threshold: float) -> list:
        """Detect statistical anomalies using Z-score and IQR methods."""
        anomalies = []
        
        if len(values) < 3:
            return anomalies
        
        # Calculate statistics
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        std_dev = variance ** 0.5
        
        # Z-score method
        for i, (value, week_label) in enumerate(zip(values, week_labels)):
            if std_dev > 0:
                z_score = abs((value - mean) / std_dev)
                if z_score > threshold:
                    severity = "high" if z_score > threshold * 1.5 else "medium"
                    anomalies.append({
                        "week": week_label,
                        "value": value,
                        "z_score": round(z_score, 2),
                        "expected_range": [round(mean - threshold * std_dev, 2), 
                                         round(mean + threshold * std_dev, 2)],
                        "deviation": round(abs(value - mean), 2),
                        "severity": severity,
                        "method": "z_score"
                    })
        
        return anomalies

    def _analyze_seasonality(self, weekly_metrics: dict, date_ranges: list) -> dict:
        """Analyze seasonality patterns in the data."""
        seasonality = {
            "weekly_patterns": {},
            "day_of_week_patterns": {},
            "trend_analysis": {},
            "seasonal_indicators": {}
        }
        
        # Extract day-of-week patterns
        day_patterns = {}
        for week_data in weekly_metrics.values():
            for metric_type, metric_values in week_data.get("metrics", {}).items():
                if metric_type == "alerts":
                    daily_counts = metric_values.get("daily_counts", [])
                    if len(daily_counts) == 7:
                        if metric_type not in day_patterns:
                            day_patterns[metric_type] = [0] * 7
                        for i, count in enumerate(daily_counts):
                            day_patterns[metric_type][i] += count
        
        # Calculate average patterns
        days = ["monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"]
        weeks_count = len(weekly_metrics)
        
        for metric_type, totals in day_patterns.items():
            if weeks_count > 0:
                averages = [total / weeks_count for total in totals]
                seasonality["day_of_week_patterns"][metric_type] = {
                    day: round(avg, 2) for day, avg in zip(days, averages)
                }
                
                # Identify peak and low days
                max_day = days[averages.index(max(averages))]
                min_day = days[averages.index(min(averages))]
                
                seasonality["seasonal_indicators"][metric_type] = {
                    "peak_day": max_day,
                    "low_day": min_day,
                    "pattern_strength": self._calculate_pattern_strength(averages)
                }
        
        return seasonality

    def _calculate_pattern_strength(self, values: list) -> str:
        """Calculate the strength of a seasonal pattern."""
        if not values:
            return "none"
        
        cv = self._calculate_cv(values)
        
        if cv > 50:
            return "strong"
        elif cv > 25:
            return "moderate"
        elif cv > 10:
            return "weak"
        else:
            return "minimal"

    def _analyze_behavioral_patterns(self, weekly_metrics: dict, raw_data: dict) -> dict:
        """Analyze behavioral patterns and changes."""
        patterns = {
            "behavior_changes": [],
            "pattern_summary": {},
            "stability_analysis": {},
            "change_detection": {}
        }
        
        # Analyze week-over-week changes
        metric_trends = {}
        weeks = list(weekly_metrics.keys())
        
        for i in range(1, len(weeks)):
            prev_week = weekly_metrics[weeks[i-1]]
            curr_week = weekly_metrics[weeks[i]]
            
            for metric_type in ["alerts", "vulnerabilities", "authentication"]:
                if metric_type not in metric_trends:
                    metric_trends[metric_type] = []
                
                prev_value = self._extract_metric_value(prev_week, metric_type)
                curr_value = self._extract_metric_value(curr_week, metric_type)
                
                if prev_value > 0:
                    change_percent = ((curr_value - prev_value) / prev_value) * 100
                    metric_trends[metric_type].append(change_percent)
                    
                    # Detect significant behavior changes
                    if abs(change_percent) > 50:  # 50% change threshold
                        patterns["behavior_changes"].append({
                            "metric": metric_type,
                            "from_week": weeks[i-1],
                            "to_week": weeks[i],
                            "change_percent": round(change_percent, 2),
                            "severity": "high" if abs(change_percent) > 100 else "medium"
                        })
        
        # Calculate stability metrics
        for metric_type, changes in metric_trends.items():
            if changes:
                patterns["stability_analysis"][metric_type] = {
                    "average_change": round(sum(changes) / len(changes), 2),
                    "volatility": round(self._calculate_variance(changes), 2),
                    "stability_score": self._calculate_stability_score(changes)
                }
        
        return patterns

    def _extract_metric_value(self, week_data: dict, metric_type: str) -> int:
        """Extract primary metric value for a week."""
        metric_data = week_data.get("metrics", {}).get(metric_type, {})
        
        if metric_type == "alerts":
            return metric_data.get("total", 0)
        elif metric_type == "vulnerabilities":
            return metric_data.get("total", 0)
        elif metric_type == "authentication":
            return metric_data.get("total_events", 0)
        
        return 0

    def _calculate_stability_score(self, changes: list) -> str:
        """Calculate stability score based on change variance."""
        if not changes:
            return "unknown"
        
        variance = self._calculate_variance(changes)
        
        if variance < 100:
            return "stable"
        elif variance < 500:
            return "moderate"
        elif variance < 1000:
            return "volatile"
        else:
            return "highly_volatile"

    def _analyze_enhanced_weekly_trends(self, weekly_metrics: dict, statistical_analysis: dict) -> dict:
        """Enhanced trend analysis with statistical backing."""
        trends = {
            "overall_trends": {},
            "metric_trends": {},
            "trend_strength": {},
            "trend_predictions": {}
        }
        
        # Analyze trends for each metric
        for metric_type in ["alerts", "vulnerabilities", "authentication"]:
            values = []
            weeks = list(weekly_metrics.keys())
            
            for week_data in weekly_metrics.values():
                value = self._extract_metric_value(week_data, metric_type)
                values.append(value)
            
            if len(values) >= 2:
                trend_direction = self._calculate_trend_direction(values)
                trend_strength = self._calculate_trend_strength(values)
                
                trends["metric_trends"][metric_type] = {
                    "direction": trend_direction,
                    "strength": trend_strength,
                    "weekly_values": values,
                    "correlation_coefficient": self._calculate_correlation(list(range(len(values))), values)
                }
        
        return trends

    def _calculate_trend_direction(self, values: list) -> str:
        """Calculate trend direction."""
        if len(values) < 2:
            return "unknown"
        
        start_avg = sum(values[:len(values)//2]) / (len(values)//2)
        end_avg = sum(values[len(values)//2:]) / (len(values) - len(values)//2)
        
        change_percent = ((end_avg - start_avg) / max(start_avg, 1)) * 100
        
        if change_percent > 10:
            return "increasing"
        elif change_percent < -10:
            return "decreasing"
        else:
            return "stable"

    def _calculate_trend_strength(self, values: list) -> str:
        """Calculate strength of trend."""
        if len(values) < 3:
            return "insufficient_data"
        
        # Calculate linear regression slope
        n = len(values)
        x_values = list(range(n))
        
        sum_x = sum(x_values)
        sum_y = sum(values)
        sum_xy = sum(x * y for x, y in zip(x_values, values))
        sum_x2 = sum(x * x for x in x_values)
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        
        if abs(slope) > 50:
            return "strong"
        elif abs(slope) > 20:
            return "moderate"
        elif abs(slope) > 5:
            return "weak"
        else:
            return "minimal"

    def _calculate_correlation(self, x_values: list, y_values: list) -> float:
        """Calculate correlation coefficient."""
        if len(x_values) != len(y_values) or len(x_values) < 2:
            return 0.0
        
        n = len(x_values)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x2 = sum(x * x for x in x_values)
        sum_y2 = sum(y * y for y in y_values)
        
        numerator = n * sum_xy - sum_x * sum_y
        denominator = ((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y)) ** 0.5
        
        if denominator == 0:
            return 0.0
        
        return round(numerator / denominator, 3)

    def _perform_enhanced_week_comparison(self, weekly_metrics: dict, compare_weeks: int) -> dict:
        """Enhanced week-over-week comparison with baseline analysis."""
        comparisons = {
            "week_over_week": {},
            "baseline_comparison": {},
            "deviation_analysis": {},
            "comparison_summary": {}
        }
        
        weeks = list(weekly_metrics.keys())
        
        # Week-over-week comparisons
        for i in range(1, len(weeks)):
            prev_week = weekly_metrics[weeks[i-1]]
            curr_week = weekly_metrics[weeks[i]]
            
            comparison = {
                "previous_week": weeks[i-1],
                "current_week": weeks[i],
                "changes": {}
            }
            
            for metric_type in ["alerts", "vulnerabilities", "authentication"]:
                prev_value = self._extract_metric_value(prev_week, metric_type)
                curr_value = self._extract_metric_value(curr_week, metric_type)
                
                change = curr_value - prev_value
                change_percent = (change / max(prev_value, 1)) * 100
                
                comparison["changes"][metric_type] = {
                    "absolute_change": change,
                    "percent_change": round(change_percent, 2),
                    "previous_value": prev_value,
                    "current_value": curr_value
                }
            
            comparisons["week_over_week"][weeks[i]] = comparison
        
        return comparisons

    def _generate_predictions(self, weekly_metrics: dict, trends: dict, seasonality: dict) -> dict:
        """Generate predictions based on trends and seasonality."""
        predictions = {
            "next_week_forecast": {},
            "trend_projections": {},
            "confidence_intervals": {},
            "prediction_accuracy": {}
        }
        
        for metric_type in ["alerts", "vulnerabilities", "authentication"]:
            values = []
            for week_data in weekly_metrics.values():
                value = self._extract_metric_value(week_data, metric_type)
                values.append(value)
            
            if len(values) >= 3:
                # Simple linear extrapolation
                recent_values = values[-3:]  # Use last 3 weeks
                trend = (recent_values[-1] - recent_values[0]) / 2  # Average change per week
                
                predicted_value = max(0, recent_values[-1] + trend)
                
                # Calculate confidence based on trend consistency
                consistency = self._calculate_prediction_confidence(values)
                
                predictions["next_week_forecast"][metric_type] = {
                    "predicted_value": round(predicted_value, 0),
                    "confidence": consistency,
                    "trend_component": round(trend, 2),
                    "baseline_value": recent_values[-1]
                }
        
        return predictions

    def _calculate_prediction_confidence(self, values: list) -> str:
        """Calculate confidence level for predictions."""
        if len(values) < 3:
            return "low"
        
        # Calculate trend consistency
        changes = [values[i] - values[i-1] for i in range(1, len(values))]
        cv = self._calculate_cv(changes) if changes else 100
        
        if cv < 30:
            return "high"
        elif cv < 60:
            return "medium"
        else:
            return "low"

    def _generate_enhanced_weekly_insights(self, summary: dict, trends: dict, 
                                         anomalies: dict, behavioral: dict, raw_data: dict) -> list:
        """Generate enhanced insights with statistical backing."""
        insights = []
        
        # Statistical insights
        if anomalies.get("summary", {}).get("total_anomalies_detected", 0) > 0:
            insights.append({
                "type": "anomaly",
                "priority": "high",
                "insight": f"Detected {anomalies['summary']['total_anomalies_detected']} anomalies across metrics",
                "details": f"{anomalies['summary'].get('high_priority_count', 0)} require immediate attention",
                "action_required": True
            })
        
        # Trend insights
        for metric_type, trend_data in trends.get("metric_trends", {}).items():
            direction = trend_data.get("direction", "unknown")
            strength = trend_data.get("strength", "unknown")
            
            if direction != "stable" and strength in ["strong", "moderate"]:
                insights.append({
                    "type": "trend",
                    "priority": "medium",
                    "insight": f"{metric_type.title()} showing {strength} {direction} trend",
                    "details": f"Correlation coefficient: {trend_data.get('correlation_coefficient', 0)}",
                    "action_required": direction == "increasing" and metric_type in ["alerts", "vulnerabilities"]
                })
        
        # Behavioral change insights
        for change in behavioral.get("behavior_changes", []):
            if change.get("severity") == "high":
                insights.append({
                    "type": "behavioral",
                    "priority": "high",
                    "insight": f"Significant behavior change in {change['metric']}",
                    "details": f"{change['change_percent']}% change from {change['from_week']} to {change['to_week']}",
                    "action_required": True
                })
        
        return insights

    def _generate_enhanced_weekly_recommendations(self, summary: dict, trends: dict, 
                                                insights: list, anomalies: dict, predictions: dict) -> list:
        """Generate enhanced recommendations based on comprehensive analysis."""
        recommendations = []
        
        # Anomaly-based recommendations
        high_priority_anomalies = anomalies.get("high_priority_anomalies", [])
        if high_priority_anomalies:
            recommendations.append({
                "type": "anomaly_response",
                "priority": "critical",
                "recommendation": "Investigate high-priority anomalies immediately",
                "actions": [
                    f"Review {anomaly['metric']} spike in {anomaly['week']}"
                    for anomaly in high_priority_anomalies[:3]
                ],
                "timeframe": "immediate"
            })
        
        # Prediction-based recommendations
        for metric_type, forecast in predictions.get("next_week_forecast", {}).items():
            if forecast.get("confidence") == "high" and forecast.get("predicted_value", 0) > 0:
                current_trend = forecast.get("trend_component", 0)
                if current_trend > 0 and metric_type in ["alerts", "vulnerabilities"]:
                    recommendations.append({
                        "type": "predictive",
                        "priority": "medium",
                        "recommendation": f"Prepare for {metric_type} increase next week",
                        "actions": [
                            f"Expected {metric_type}: {forecast['predicted_value']}",
                            "Review capacity and response procedures",
                            "Consider preventive measures"
                        ],
                        "timeframe": "next_week"
                    })
        
        # Trend-based recommendations
        for metric_type, trend_data in trends.get("metric_trends", {}).items():
            if trend_data.get("direction") == "increasing" and trend_data.get("strength") == "strong":
                recommendations.append({
                    "type": "trend_response",
                    "priority": "medium",
                    "recommendation": f"Address rising {metric_type} trend",
                    "actions": [
                        "Investigate root causes",
                        "Implement trend mitigation strategies",
                        "Monitor closely"
                    ],
                    "timeframe": "this_week"
                })
        
        return recommendations
    
    async def _handle_get_wazuh_remoted_stats(self, arguments: dict) -> list[types.TextContent]:
        """Handle remoted daemon statistics and performance analysis for Wazuh managers."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_remoted_stats_query(arguments)
            
            # Calculate time range
            time_range = self._parse_time_range(validated_query.time_range)
            
            # Collect remoted statistics
            remoted_stats = await self._collect_remoted_statistics(validated_query, time_range)
            
            # Analyze collected data
            analysis = await self._analyze_remoted_stats(remoted_stats, validated_query, time_range, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except ValidationError as e:
            self.logger.error(f"Validation error in remoted stats analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
        except Exception as e:
            self.logger.error(f"Error in remoted stats analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
    
    def _parse_time_range(self, time_range: str) -> Dict[str, Any]:
        """Parse time range string into datetime objects."""
        end_time = datetime.utcnow()
        
        # Parse time range
        if time_range == "1h":
            start_time = end_time - timedelta(hours=1)
        elif time_range == "6h":
            start_time = end_time - timedelta(hours=6)
        elif time_range == "12h":
            start_time = end_time - timedelta(hours=12)
        elif time_range == "24h":
            start_time = end_time - timedelta(hours=24)
        elif time_range == "7d":
            start_time = end_time - timedelta(days=7)
        elif time_range == "30d":
            start_time = end_time - timedelta(days=30)
        else:
            start_time = end_time - timedelta(hours=24)  # Default to 24h
        
        return {
            "start_time": start_time,
            "end_time": end_time,
            "start_iso": start_time.isoformat() + "Z",
            "end_iso": end_time.isoformat() + "Z",
            "duration_hours": (end_time - start_time).total_seconds() / 3600,
            "range_label": time_range
        }
    
    async def _collect_remoted_statistics(self, query, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect comprehensive remoted daemon statistics."""
        stats_data = {
            "time_range": time_range,
            "node_stats": {},
            "global_stats": {},
            "performance_metrics": {},
            "connection_stats": {},
            "event_stats": {},
            "queue_stats": {},
            "error_stats": {},
            "collection_errors": []
        }
        
        try:
            # Get cluster node information
            nodes = await self._get_cluster_nodes()
            
            # Filter nodes if specified
            if query.node_filter:
                nodes = [node for node in nodes if node.get('name') in query.node_filter]
            
            # Collect stats for each node
            for node in nodes:
                node_name = node.get('name', 'unknown')
                self.logger.info(f"Collecting remoted stats for node: {node_name}")
                
                try:
                    node_stats = await self._collect_node_remoted_stats(node, query, time_range)
                    stats_data["node_stats"][node_name] = node_stats
                    
                except Exception as e:
                    self.logger.warning(f"Error collecting stats for node {node_name}: {str(e)}")
                    stats_data["collection_errors"].append({
                        "node": node_name,
                        "error": str(e),
                        "timestamp": datetime.utcnow().isoformat()
                    })
            
            # Calculate global aggregated statistics
            stats_data["global_stats"] = self._calculate_global_remoted_stats(stats_data["node_stats"])
            
        except Exception as e:
            self.logger.error(f"Error collecting remoted statistics: {str(e)}")
            stats_data["collection_errors"].append({
                "scope": "global",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            })
        
        return stats_data
    
    async def _get_cluster_nodes(self) -> List[Dict[str, Any]]:
        """Get cluster node information."""
        try:
            # Try to get cluster nodes
            cluster_response = await self.api_client.get_cluster_nodes()
            if cluster_response.get("data", {}).get("affected_items"):
                return cluster_response["data"]["affected_items"]
            else:
                # If no cluster, return single node (standalone)
                return [{"name": "master", "type": "master"}]
        except Exception as e:
            self.logger.warning(f"Could not get cluster nodes: {str(e)}")
            return [{"name": "master", "type": "master"}]
    
    async def _collect_node_remoted_stats(self, node: Dict[str, Any], query, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect remoted statistics for a specific node."""
        node_name = node.get('name', 'unknown')
        node_stats = {
            "node_info": node,
            "daemon_stats": {},
            "performance": {},
            "connections": {},
            "events": {},
            "queues": {},
            "errors": {},
            "timestamp": datetime.utcnow().isoformat()
        }
        
        try:
            # Get daemon statistics
            if query.include_performance or query.include_connections or query.include_events or query.include_queues:
                daemon_stats = await self._get_daemon_stats(node_name, "remoted")
                node_stats["daemon_stats"] = daemon_stats
            
            # Performance metrics
            if query.include_performance:
                node_stats["performance"] = await self._collect_remoted_performance_metrics(node_name, time_range)
            
            # Connection statistics
            if query.include_connections:
                node_stats["connections"] = await self._collect_remoted_connection_stats(node_name, time_range)
            
            # Event processing metrics
            if query.include_events:
                node_stats["events"] = await self._collect_remoted_event_stats(node_name, time_range)
            
            # Queue statistics
            if query.include_queues:
                node_stats["queues"] = await self._collect_remoted_queue_stats(node_name, time_range)
            
            # Error analysis
            if query.include_errors:
                node_stats["errors"] = await self._collect_remoted_error_stats(node_name, time_range)
            
        except Exception as e:
            self.logger.error(f"Error collecting node stats for {node_name}: {str(e)}")
            node_stats["collection_error"] = str(e)
        
        return node_stats
    
    async def _get_daemon_stats(self, node_name: str, daemon_name: str) -> Dict[str, Any]:
        """Get daemon statistics from API."""
        try:
            # Get daemon stats
            stats_response = await self.api_client.get_daemon_stats(node_name, daemon_name)
            return stats_response.get("data", {})
        except Exception as e:
            self.logger.warning(f"Could not get daemon stats for {node_name}/{daemon_name}: {str(e)}")
            return {"error": str(e)}
    
    async def _collect_remoted_performance_metrics(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect performance metrics for remoted daemon."""
        performance_data = {
            "cpu_usage": {},
            "memory_usage": {},
            "process_info": {},
            "system_resources": {},
            "historical_trends": []
        }
        
        try:
            # Get current process information
            process_info = await self._get_process_info(node_name, "wazuh-remoted")
            if process_info:
                performance_data["process_info"] = process_info
                performance_data["cpu_usage"] = {
                    "current": process_info.get("cpu_percent", 0),
                    "average": process_info.get("cpu_percent", 0),  # Would be calculated from historical data
                    "peak": process_info.get("cpu_percent", 0)
                }
                performance_data["memory_usage"] = {
                    "current_mb": process_info.get("memory_mb", 0),
                    "current_percent": process_info.get("memory_percent", 0),
                    "peak_mb": process_info.get("memory_mb", 0),
                    "average_mb": process_info.get("memory_mb", 0)
                }
            
            # Get system resource information
            system_stats = await self._get_system_stats(node_name)
            if system_stats:
                performance_data["system_resources"] = system_stats
            
        except Exception as e:
            self.logger.warning(f"Error collecting performance metrics for {node_name}: {str(e)}")
            performance_data["error"] = str(e)
        
        return performance_data
    
    async def _collect_remoted_connection_stats(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect connection statistics for remoted daemon."""
        connection_data = {
            "active_connections": 0,
            "connection_types": {},
            "connection_history": [],
            "agent_connections": {},
            "connection_errors": [],
            "bandwidth_usage": {}
        }
        
        try:
            # Get agent connection information
            agents_response = await self.api_client.get_agents(limit=10000)
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            active_agents = [agent for agent in agents if agent.get("status") == "active"]
            connection_data["active_connections"] = len(active_agents)
            
            # Analyze connection types and status
            connection_types = {}
            agent_connections = {}
            
            for agent in agents:
                status = agent.get("status", "unknown")
                connection_types[status] = connection_types.get(status, 0) + 1
                
                # Store agent connection info
                agent_id = agent.get("id", "unknown")
                agent_connections[agent_id] = {
                    "name": agent.get("name", "unknown"),
                    "status": status,
                    "last_keep_alive": agent.get("lastKeepAlive", "unknown"),
                    "ip": agent.get("ip", "unknown"),
                    "version": agent.get("version", "unknown"),
                    "node_name": agent.get("node_name", "unknown")
                }
            
            connection_data["connection_types"] = connection_types
            connection_data["agent_connections"] = agent_connections
            
            # Get connection statistics from daemon stats
            daemon_stats = await self._get_daemon_stats(node_name, "remoted")
            if daemon_stats and not daemon_stats.get("error"):
                connection_data["bandwidth_usage"] = {
                    "bytes_received": daemon_stats.get("bytes_received", 0),
                    "bytes_sent": daemon_stats.get("bytes_sent", 0),
                    "messages_received": daemon_stats.get("messages_received", 0),
                    "messages_sent": daemon_stats.get("messages_sent", 0)
                }
            
        except Exception as e:
            self.logger.warning(f"Error collecting connection stats for {node_name}: {str(e)}")
            connection_data["error"] = str(e)
        
        return connection_data
    
    async def _collect_remoted_event_stats(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect event processing statistics for remoted daemon."""
        event_data = {
            "events_processed": 0,
            "event_types": {},
            "processing_rate": {},
            "event_sources": {},
            "dropped_events": 0,
            "error_events": 0
        }
        
        try:
            # Get events from the specified time range
            events_response = await self.api_client.get_events(
                limit=10000,
                q=f"timestamp>{time_range['start_iso']};timestamp<{time_range['end_iso']}"
            )
            
            events = events_response.get("data", {}).get("affected_items", [])
            event_data["events_processed"] = len(events)
            
            # Analyze event types and sources
            event_types = {}
            event_sources = {}
            
            for event in events:
                event_type = event.get("type", "unknown")
                event_types[event_type] = event_types.get(event_type, 0) + 1
                
                agent_name = event.get("agent", {}).get("name", "unknown")
                event_sources[agent_name] = event_sources.get(agent_name, 0) + 1
            
            event_data["event_types"] = event_types
            event_data["event_sources"] = event_sources
            
            # Calculate processing rate
            duration_hours = time_range["duration_hours"]
            if duration_hours > 0:
                event_data["processing_rate"] = {
                    "events_per_hour": event_data["events_processed"] / duration_hours,
                    "events_per_minute": event_data["events_processed"] / (duration_hours * 60),
                    "events_per_second": event_data["events_processed"] / (duration_hours * 3600)
                }
            
        except Exception as e:
            self.logger.warning(f"Error collecting event stats for {node_name}: {str(e)}")
            event_data["error"] = str(e)
        
        return event_data
    
    async def _collect_remoted_queue_stats(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect queue statistics for remoted daemon."""
        queue_data = {
            "current_queue_size": 0,
            "max_queue_size": 0,
            "queue_utilization": 0.0,
            "queue_history": [],
            "queue_types": {},
            "blocked_queues": [],
            "queue_performance": {}
        }
        
        try:
            # Get daemon statistics for queue information
            daemon_stats = await self._get_daemon_stats(node_name, "remoted")
            
            if daemon_stats and not daemon_stats.get("error"):
                # Extract queue information from daemon stats
                queue_stats = daemon_stats.get("queue", {})
                
                queue_data["current_queue_size"] = queue_stats.get("usage", 0)
                queue_data["max_queue_size"] = queue_stats.get("size", 0)
                
                if queue_data["max_queue_size"] > 0:
                    queue_data["queue_utilization"] = (
                        queue_data["current_queue_size"] / queue_data["max_queue_size"]
                    ) * 100
                
                # Queue performance metrics
                queue_data["queue_performance"] = {
                    "messages_processed": queue_stats.get("processed", 0),
                    "messages_dropped": queue_stats.get("dropped", 0),
                    "processing_rate": queue_stats.get("rate", 0),
                    "average_processing_time": queue_stats.get("avg_time", 0)
                }
                
                # Check for blocked queues
                if queue_data["queue_utilization"] > 90:
                    queue_data["blocked_queues"].append({
                        "queue_type": "remoted",
                        "utilization": queue_data["queue_utilization"],
                        "status": "critical" if queue_data["queue_utilization"] > 95 else "warning"
                    })
            
        except Exception as e:
            self.logger.warning(f"Error collecting queue stats for {node_name}: {str(e)}")
            queue_data["error"] = str(e)
        
        return queue_data
    
    async def _collect_remoted_error_stats(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect error statistics and analysis for remoted daemon."""
        error_data = {
            "total_errors": 0,
            "error_types": {},
            "error_sources": {},
            "critical_errors": [],
            "error_trends": [],
            "resolution_recommendations": []
        }
        
        try:
            # Get logs for error analysis
            logs_response = await self.api_client.get_logs(
                limit=5000,
                q=f"timestamp>{time_range['start_iso']};timestamp<{time_range['end_iso']};level>1"
            )
            
            logs = logs_response.get("data", {}).get("affected_items", [])
            error_logs = [log for log in logs if log.get("level", 0) >= 2]  # Warning and above
            
            error_data["total_errors"] = len(error_logs)
            
            # Analyze error types and sources
            error_types = {}
            error_sources = {}
            critical_errors = []
            
            for log in error_logs:
                level = log.get("level", 0)
                description = log.get("description", "unknown")
                tag = log.get("tag", "unknown")
                
                # Categorize error types
                if "remoted" in tag.lower() or "connection" in description.lower():
                    error_type = "connection_error"
                elif "queue" in description.lower():
                    error_type = "queue_error"
                elif "authentication" in description.lower():
                    error_type = "auth_error"
                elif "timeout" in description.lower():
                    error_type = "timeout_error"
                else:
                    error_type = "other_error"
                
                error_types[error_type] = error_types.get(error_type, 0) + 1
                error_sources[tag] = error_sources.get(tag, 0) + 1
                
                # Identify critical errors
                if level >= 3:  # Error level and above
                    critical_errors.append({
                        "timestamp": log.get("timestamp", "unknown"),
                        "level": level,
                        "description": description,
                        "tag": tag,
                        "type": error_type
                    })
            
            error_data["error_types"] = error_types
            error_data["error_sources"] = error_sources
            error_data["critical_errors"] = critical_errors[:10]  # Limit to top 10
            
            # Generate resolution recommendations
            error_data["resolution_recommendations"] = self._generate_error_recommendations(error_types, critical_errors)
            
        except Exception as e:
            self.logger.warning(f"Error collecting error stats for {node_name}: {str(e)}")
            error_data["error"] = str(e)
        
        return error_data
    
    def _generate_error_recommendations(self, error_types: Dict[str, int], critical_errors: List[Dict]) -> List[Dict[str, str]]:
        """Generate recommendations based on error analysis."""
        recommendations = []
        
        # Connection error recommendations
        if error_types.get("connection_error", 0) > 0:
            recommendations.append({
                "type": "connection_error",
                "priority": "high",
                "recommendation": "Review network connectivity and firewall rules between agents and manager",
                "action": "Check agent connectivity and review remoted configuration"
            })
        
        # Queue error recommendations
        if error_types.get("queue_error", 0) > 0:
            recommendations.append({
                "type": "queue_error",
                "priority": "medium",
                "recommendation": "Monitor queue utilization and consider increasing queue sizes",
                "action": "Review ossec.conf remoted section and adjust queue parameters"
            })
        
        # Authentication error recommendations
        if error_types.get("auth_error", 0) > 0:
            recommendations.append({
                "type": "auth_error",
                "priority": "high",
                "recommendation": "Review agent authentication and key management",
                "action": "Check agent keys and authentication configuration"
            })
        
        # Timeout error recommendations
        if error_types.get("timeout_error", 0) > 0:
            recommendations.append({
                "type": "timeout_error",
                "priority": "medium",
                "recommendation": "Increase timeout values and review network latency",
                "action": "Adjust timeout settings in remoted configuration"
            })
        
        # General recommendations based on critical errors
        if len(critical_errors) > 5:
            recommendations.append({
                "type": "general",
                "priority": "high",
                "recommendation": "High number of critical errors detected - immediate investigation required",
                "action": "Review logs and escalate to Wazuh support if needed"
            })
        
        return recommendations
    
    async def _get_process_info(self, node_name: str, process_name: str) -> Dict[str, Any]:
        """Get process information for a specific daemon."""
        try:
            # This would typically call a system API to get process info
            # For now, return mock data structure
            return {
                "pid": 0,
                "cpu_percent": 0.0,
                "memory_mb": 0,
                "memory_percent": 0.0,
                "status": "running",
                "threads": 0,
                "uptime_seconds": 0
            }
        except Exception as e:
            self.logger.warning(f"Could not get process info for {process_name} on {node_name}: {str(e)}")
            return {}
    
    async def _get_system_stats(self, node_name: str) -> Dict[str, Any]:
        """Get system statistics for a node."""
        try:
            # This would typically call a system API to get system stats
            # For now, return mock data structure
            return {
                "cpu_usage": 0.0,
                "memory_usage": 0.0,
                "disk_usage": 0.0,
                "load_average": [0.0, 0.0, 0.0],
                "uptime_seconds": 0
            }
        except Exception as e:
            self.logger.warning(f"Could not get system stats for {node_name}: {str(e)}")
            return {}
    
    def _calculate_global_remoted_stats(self, node_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate global aggregated statistics across all nodes."""
        global_stats = {
            "total_nodes": len(node_stats),
            "active_nodes": 0,
            "total_connections": 0,
            "total_events_processed": 0,
            "total_queue_size": 0,
            "total_errors": 0,
            "average_cpu_usage": 0.0,
            "average_memory_usage": 0.0,
            "global_connection_types": {},
            "global_error_types": {},
            "health_status": "unknown",
            "performance_summary": {}
        }
        
        cpu_values = []
        memory_values = []
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            global_stats["active_nodes"] += 1
            
            # Aggregate connection stats
            connections = stats.get("connections", {})
            global_stats["total_connections"] += connections.get("active_connections", 0)
            
            # Aggregate connection types
            for conn_type, count in connections.get("connection_types", {}).items():
                global_stats["global_connection_types"][conn_type] = \
                    global_stats["global_connection_types"].get(conn_type, 0) + count
            
            # Aggregate event stats
            events = stats.get("events", {})
            global_stats["total_events_processed"] += events.get("events_processed", 0)
            
            # Aggregate queue stats
            queues = stats.get("queues", {})
            global_stats["total_queue_size"] += queues.get("current_queue_size", 0)
            
            # Aggregate error stats
            errors = stats.get("errors", {})
            global_stats["total_errors"] += errors.get("total_errors", 0)
            
            # Aggregate error types
            for error_type, count in errors.get("error_types", {}).items():
                global_stats["global_error_types"][error_type] = \
                    global_stats["global_error_types"].get(error_type, 0) + count
            
            # Collect performance metrics
            performance = stats.get("performance", {})
            cpu_usage = performance.get("cpu_usage", {}).get("current", 0)
            memory_usage = performance.get("memory_usage", {}).get("current_percent", 0)
            
            if cpu_usage > 0:
                cpu_values.append(cpu_usage)
            if memory_usage > 0:
                memory_values.append(memory_usage)
        
        # Calculate averages
        if cpu_values:
            global_stats["average_cpu_usage"] = sum(cpu_values) / len(cpu_values)
        if memory_values:
            global_stats["average_memory_usage"] = sum(memory_values) / len(memory_values)
        
        # Determine health status
        if global_stats["total_errors"] > 10:
            global_stats["health_status"] = "critical"
        elif global_stats["total_errors"] > 5:
            global_stats["health_status"] = "warning"
        elif global_stats["average_cpu_usage"] > 80 or global_stats["average_memory_usage"] > 80:
            global_stats["health_status"] = "warning"
        else:
            global_stats["health_status"] = "healthy"
        
        # Performance summary
        global_stats["performance_summary"] = {
            "cpu_status": "high" if global_stats["average_cpu_usage"] > 80 else "normal",
            "memory_status": "high" if global_stats["average_memory_usage"] > 80 else "normal",
            "connection_status": "active" if global_stats["total_connections"] > 0 else "inactive",
            "error_status": "high" if global_stats["total_errors"] > 10 else "normal"
        }
        
        return global_stats
    
    async def _analyze_remoted_stats(self, stats_data: Dict[str, Any], query, time_range: Dict[str, Any], 
                                   start_time: datetime) -> Dict[str, Any]:
        """Enhanced analysis of remoted statistics with communication health metrics and monitoring."""
        current_time = datetime.utcnow()
        
        # Initialize enhanced analysis structure
        analysis = {
            "query_parameters": {
                "time_range": query.time_range,
                "node_filter": query.node_filter,
                "include_performance": query.include_performance,
                "include_connections": query.include_connections,
                "include_events": query.include_events,
                "include_queues": query.include_queues,
                "include_errors": query.include_errors,
                "include_trends": query.include_trends,
                "include_communication_metrics": query.include_communication_metrics,
                "include_health_monitoring": query.include_health_monitoring,
                "include_throughput_analysis": query.include_throughput_analysis,
                "include_reliability_scoring": query.include_reliability_scoring,
                "include_diagnostics": query.include_diagnostics,
                "include_capacity_planning": query.include_capacity_planning,
                "group_by": query.group_by,
                "output_format": query.output_format
            },
            "time_range_info": time_range,
            "global_summary": stats_data["global_stats"],
            "node_analysis": {},
            "performance_analysis": {},
            "connection_analysis": {},
            "event_analysis": {},
            "queue_analysis": {},
            "error_analysis": {},
            "trend_analysis": {},
            "communication_metrics": {},
            "health_monitoring": {},
            "throughput_analysis": {},
            "reliability_scoring": {},
            "diagnostics": {},
            "capacity_planning": {},
            "alerts": [],
            "recommendations": [],
            "analysis_metadata": {
                "timestamp": current_time.isoformat(),
                "processing_time_seconds": (current_time - start_time).total_seconds(),
                "nodes_analyzed": len(stats_data["node_stats"]),
                "collection_errors": len(stats_data["collection_errors"])
            }
        }
        
        # Analyze each node
        for node_name, node_stats in stats_data["node_stats"].items():
            if node_stats.get("collection_error"):
                continue
            
            analysis["node_analysis"][node_name] = self._analyze_node_stats(node_stats, query)
        
        # Performance analysis
        if query.include_performance:
            analysis["performance_analysis"] = self._analyze_performance_metrics(
                stats_data["node_stats"], query.threshold_cpu, query.threshold_memory
            )
        
        # Connection analysis
        if query.include_connections:
            analysis["connection_analysis"] = self._analyze_connection_metrics(stats_data["node_stats"])
        
        # Event analysis
        if query.include_events:
            analysis["event_analysis"] = self._analyze_event_metrics(stats_data["node_stats"])
        
        # Queue analysis
        if query.include_queues:
            analysis["queue_analysis"] = self._analyze_queue_metrics(
                stats_data["node_stats"], query.threshold_queue
            )
        
        # Error analysis
        if query.include_errors:
            analysis["error_analysis"] = self._analyze_error_metrics(stats_data["node_stats"])
        
        # Trend analysis
        if query.include_trends:
            analysis["trend_analysis"] = self._analyze_remoted_trends(stats_data, time_range)
        
        # Communication metrics analysis
        if query.include_communication_metrics:
            analysis["communication_metrics"] = self._analyze_communication_metrics(
                stats_data["node_stats"], query.threshold_latency, query.threshold_error_rate
            )
        
        # Health monitoring analysis
        if query.include_health_monitoring:
            analysis["health_monitoring"] = self._analyze_health_monitoring(
                stats_data["node_stats"], query
            )
        
        # Throughput analysis
        if query.include_throughput_analysis:
            analysis["throughput_analysis"] = self._analyze_throughput_metrics(
                stats_data["node_stats"], time_range
            )
        
        # Reliability scoring
        if query.include_reliability_scoring:
            analysis["reliability_scoring"] = self._calculate_reliability_scores(
                stats_data["node_stats"], time_range
            )
        
        # Diagnostics
        if query.include_diagnostics:
            analysis["diagnostics"] = self._generate_diagnostics(
                stats_data["node_stats"], query
            )
        
        # Capacity planning
        if query.include_capacity_planning:
            analysis["capacity_planning"] = self._analyze_capacity_planning(
                stats_data["node_stats"], time_range, query
            )
        
        # Generate enhanced alerts and recommendations
        analysis["alerts"] = self._generate_enhanced_remoted_alerts(stats_data, query, analysis)
        analysis["recommendations"] = self._generate_enhanced_remoted_recommendations(stats_data, query, analysis)
        
        # Format output based on requested format
        if query.output_format == "summary":
            return self._format_summary_output(analysis)
        elif query.output_format == "minimal":
            return self._format_minimal_output(analysis)
        else:
            return analysis
    
    def _analyze_node_stats(self, node_stats: Dict[str, Any], query) -> Dict[str, Any]:
        """Analyze statistics for a single node."""
        node_analysis = {
            "node_info": node_stats.get("node_info", {}),
            "overall_health": "unknown",
            "performance_score": 0,
            "connection_health": "unknown",
            "queue_health": "unknown",
            "error_level": "unknown",
            "key_metrics": {},
            "issues": [],
            "strengths": []
        }
        
        # Analyze performance
        performance = node_stats.get("performance", {})
        cpu_usage = performance.get("cpu_usage", {}).get("current", 0)
        memory_usage = performance.get("memory_usage", {}).get("current_percent", 0)
        
        if cpu_usage > 0 and memory_usage > 0:
            node_analysis["performance_score"] = max(0, 100 - max(cpu_usage, memory_usage))
        
        # Analyze connections
        connections = node_stats.get("connections", {})
        active_connections = connections.get("active_connections", 0)
        connection_types = connections.get("connection_types", {})
        
        if active_connections > 0:
            node_analysis["connection_health"] = "healthy"
            node_analysis["strengths"].append(f"Maintaining {active_connections} active connections")
        else:
            node_analysis["connection_health"] = "warning"
            node_analysis["issues"].append("No active connections detected")
        
        # Analyze queues
        queues = node_stats.get("queues", {})
        queue_utilization = queues.get("queue_utilization", 0)
        
        if queue_utilization < 70:
            node_analysis["queue_health"] = "healthy"
        elif queue_utilization < 90:
            node_analysis["queue_health"] = "warning"
            node_analysis["issues"].append(f"Queue utilization at {queue_utilization:.1f}%")
        else:
            node_analysis["queue_health"] = "critical"
            node_analysis["issues"].append(f"Queue utilization critical at {queue_utilization:.1f}%")
        
        # Analyze errors
        errors = node_stats.get("errors", {})
        total_errors = errors.get("total_errors", 0)
        
        if total_errors == 0:
            node_analysis["error_level"] = "none"
        elif total_errors < 5:
            node_analysis["error_level"] = "low"
        elif total_errors < 10:
            node_analysis["error_level"] = "medium"
            node_analysis["issues"].append(f"{total_errors} errors detected")
        else:
            node_analysis["error_level"] = "high"
            node_analysis["issues"].append(f"High error count: {total_errors}")
        
        # Determine overall health
        if (node_analysis["connection_health"] == "healthy" and 
            node_analysis["queue_health"] == "healthy" and 
            node_analysis["error_level"] in ["none", "low"]):
            node_analysis["overall_health"] = "healthy"
        elif node_analysis["queue_health"] == "critical" or node_analysis["error_level"] == "high":
            node_analysis["overall_health"] = "critical"
        else:
            node_analysis["overall_health"] = "warning"
        
        # Key metrics
        node_analysis["key_metrics"] = {
            "cpu_usage": cpu_usage,
            "memory_usage": memory_usage,
            "active_connections": active_connections,
            "queue_utilization": queue_utilization,
            "total_errors": total_errors,
            "events_processed": node_stats.get("events", {}).get("events_processed", 0)
        }
        
        return node_analysis
    
    def _analyze_performance_metrics(self, node_stats: Dict[str, Any], cpu_threshold: float, memory_threshold: float) -> Dict[str, Any]:
        """Analyze performance metrics across all nodes."""
        performance_analysis = {
            "overall_status": "unknown",
            "cpu_analysis": {},
            "memory_analysis": {},
            "resource_utilization": {},
            "performance_trends": {},
            "bottlenecks": [],
            "recommendations": []
        }
        
        cpu_values = []
        memory_values = []
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            performance = stats.get("performance", {})
            cpu_usage = performance.get("cpu_usage", {}).get("current", 0)
            memory_usage = performance.get("memory_usage", {}).get("current_percent", 0)
            
            if cpu_usage > 0:
                cpu_values.append(cpu_usage)
            if memory_usage > 0:
                memory_values.append(memory_usage)
            
            # Check for bottlenecks
            if cpu_usage > cpu_threshold:
                performance_analysis["bottlenecks"].append({
                    "node": node_name,
                    "type": "cpu",
                    "value": cpu_usage,
                    "threshold": cpu_threshold,
                    "severity": "high" if cpu_usage > cpu_threshold + 10 else "medium"
                })
            
            if memory_usage > memory_threshold:
                performance_analysis["bottlenecks"].append({
                    "node": node_name,
                    "type": "memory",
                    "value": memory_usage,
                    "threshold": memory_threshold,
                    "severity": "high" if memory_usage > memory_threshold + 10 else "medium"
                })
        
        # CPU analysis
        if cpu_values:
            performance_analysis["cpu_analysis"] = {
                "average": sum(cpu_values) / len(cpu_values),
                "minimum": min(cpu_values),
                "maximum": max(cpu_values),
                "nodes_above_threshold": len([v for v in cpu_values if v > cpu_threshold]),
                "status": "critical" if max(cpu_values) > cpu_threshold + 10 else "warning" if max(cpu_values) > cpu_threshold else "normal"
            }
        
        # Memory analysis
        if memory_values:
            performance_analysis["memory_analysis"] = {
                "average": sum(memory_values) / len(memory_values),
                "minimum": min(memory_values),
                "maximum": max(memory_values),
                "nodes_above_threshold": len([v for v in memory_values if v > memory_threshold]),
                "status": "critical" if max(memory_values) > memory_threshold + 10 else "warning" if max(memory_values) > memory_threshold else "normal"
            }
        
        # Overall status
        cpu_status = performance_analysis.get("cpu_analysis", {}).get("status", "normal")
        memory_status = performance_analysis.get("memory_analysis", {}).get("status", "normal")
        
        if cpu_status == "critical" or memory_status == "critical":
            performance_analysis["overall_status"] = "critical"
        elif cpu_status == "warning" or memory_status == "warning":
            performance_analysis["overall_status"] = "warning"
        else:
            performance_analysis["overall_status"] = "healthy"
        
        # Generate recommendations
        if performance_analysis["bottlenecks"]:
            performance_analysis["recommendations"].append(
                "Performance bottlenecks detected - consider resource optimization"
            )
        
        return performance_analysis
    
    def _analyze_connection_metrics(self, node_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze connection metrics across all nodes."""
        connection_analysis = {
            "total_connections": 0,
            "connection_distribution": {},
            "connection_health": "unknown",
            "agent_status_summary": {},
            "connectivity_issues": [],
            "recommendations": []
        }
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            connections = stats.get("connections", {})
            connection_analysis["total_connections"] += connections.get("active_connections", 0)
            
            # Aggregate connection types
            for conn_type, count in connections.get("connection_types", {}).items():
                connection_analysis["connection_distribution"][conn_type] = \
                    connection_analysis["connection_distribution"].get(conn_type, 0) + count
        
        # Determine connection health
        active_connections = connection_analysis["connection_distribution"].get("active", 0)
        disconnected_connections = connection_analysis["connection_distribution"].get("disconnected", 0)
        
        if active_connections > 0 and disconnected_connections == 0:
            connection_analysis["connection_health"] = "excellent"
        elif active_connections > disconnected_connections:
            connection_analysis["connection_health"] = "good"
        elif active_connections > 0:
            connection_analysis["connection_health"] = "warning"
        else:
            connection_analysis["connection_health"] = "critical"
        
        # Generate recommendations
        if disconnected_connections > 0:
            connection_analysis["recommendations"].append(
                f"Investigate {disconnected_connections} disconnected agents"
            )
        
        return connection_analysis
    
    def _analyze_event_metrics(self, node_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze event processing metrics across all nodes."""
        event_analysis = {
            "total_events_processed": 0,
            "processing_rate": {},
            "event_distribution": {},
            "processing_efficiency": "unknown",
            "performance_metrics": {},
            "recommendations": []
        }
        
        total_events = 0
        processing_rates = []
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            events = stats.get("events", {})
            node_events = events.get("events_processed", 0)
            total_events += node_events
            
            # Processing rate
            processing_rate = events.get("processing_rate", {})
            events_per_second = processing_rate.get("events_per_second", 0)
            if events_per_second > 0:
                processing_rates.append(events_per_second)
            
            # Event types
            for event_type, count in events.get("event_types", {}).items():
                event_analysis["event_distribution"][event_type] = \
                    event_analysis["event_distribution"].get(event_type, 0) + count
        
        event_analysis["total_events_processed"] = total_events
        
        # Calculate processing efficiency
        if processing_rates:
            avg_rate = sum(processing_rates) / len(processing_rates)
            event_analysis["processing_rate"] = {
                "average_events_per_second": avg_rate,
                "total_processing_nodes": len(processing_rates),
                "efficiency_score": min(100, avg_rate * 10)  # Arbitrary scoring
            }
            
            if avg_rate > 100:
                event_analysis["processing_efficiency"] = "excellent"
            elif avg_rate > 50:
                event_analysis["processing_efficiency"] = "good"
            elif avg_rate > 10:
                event_analysis["processing_efficiency"] = "moderate"
            else:
                event_analysis["processing_efficiency"] = "low"
        
        return event_analysis
    
    def _analyze_queue_metrics(self, node_stats: Dict[str, Any], queue_threshold: int) -> Dict[str, Any]:
        """Analyze queue metrics across all nodes."""
        queue_analysis = {
            "total_queue_size": 0,
            "queue_utilization": {},
            "queue_health": "unknown",
            "blocked_queues": [],
            "performance_impact": "none",
            "recommendations": []
        }
        
        utilization_values = []
        total_queue_size = 0
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            queues = stats.get("queues", {})
            queue_size = queues.get("current_queue_size", 0)
            utilization = queues.get("queue_utilization", 0)
            
            total_queue_size += queue_size
            
            if utilization > 0:
                utilization_values.append(utilization)
            
            # Check for blocked queues
            if utilization > 90:
                queue_analysis["blocked_queues"].append({
                    "node": node_name,
                    "utilization": utilization,
                    "queue_size": queue_size,
                    "status": "critical" if utilization > 95 else "warning"
                })
        
        queue_analysis["total_queue_size"] = total_queue_size
        
        # Calculate queue health
        if utilization_values:
            avg_utilization = sum(utilization_values) / len(utilization_values)
            max_utilization = max(utilization_values)
            
            queue_analysis["queue_utilization"] = {
                "average": avg_utilization,
                "maximum": max_utilization,
                "nodes_above_threshold": len([v for v in utilization_values if v > 70])
            }
            
            if max_utilization > 95:
                queue_analysis["queue_health"] = "critical"
                queue_analysis["performance_impact"] = "high"
            elif max_utilization > 80:
                queue_analysis["queue_health"] = "warning"
                queue_analysis["performance_impact"] = "medium"
            else:
                queue_analysis["queue_health"] = "healthy"
        
        # Generate recommendations
        if queue_analysis["blocked_queues"]:
            queue_analysis["recommendations"].append(
                "Queue bottlenecks detected - consider increasing queue sizes or optimizing processing"
            )
        
        return queue_analysis
    
    def _analyze_error_metrics(self, node_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze error metrics across all nodes."""
        error_analysis = {
            "total_errors": 0,
            "error_distribution": {},
            "error_trends": {},
            "critical_issues": [],
            "error_sources": {},
            "impact_assessment": "none",
            "recommendations": []
        }
        
        total_errors = 0
        all_critical_errors = []
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            errors = stats.get("errors", {})
            node_errors = errors.get("total_errors", 0)
            total_errors += node_errors
            
            # Error types
            for error_type, count in errors.get("error_types", {}).items():
                error_analysis["error_distribution"][error_type] = \
                    error_analysis["error_distribution"].get(error_type, 0) + count
            
            # Error sources
            for source, count in errors.get("error_sources", {}).items():
                error_analysis["error_sources"][source] = \
                    error_analysis["error_sources"].get(source, 0) + count
            
            # Critical errors
            critical_errors = errors.get("critical_errors", [])
            for error in critical_errors:
                all_critical_errors.append({
                    "node": node_name,
                    **error
                })
        
        error_analysis["total_errors"] = total_errors
        error_analysis["critical_issues"] = all_critical_errors[:20]  # Limit to top 20
        
        # Impact assessment
        if total_errors > 50:
            error_analysis["impact_assessment"] = "high"
        elif total_errors > 20:
            error_analysis["impact_assessment"] = "medium"
        elif total_errors > 5:
            error_analysis["impact_assessment"] = "low"
        
        # Generate recommendations
        if len(all_critical_errors) > 0:
            error_analysis["recommendations"].append(
                "Critical errors detected - immediate investigation required"
            )
        
        return error_analysis
    
    def _analyze_remoted_trends(self, stats_data: Dict[str, Any], time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze trends in remoted statistics."""
        trend_analysis = {
            "time_range": time_range,
            "connection_trends": {},
            "performance_trends": {},
            "error_trends": {},
            "capacity_trends": {},
            "predictions": {},
            "recommendations": []
        }
        
        # This would typically analyze historical data
        # For now, provide basic trend structure
        trend_analysis["connection_trends"] = {
            "direction": "stable",
            "change_rate": 0.0,
            "peak_hours": [],
            "low_activity_periods": []
        }
        
        trend_analysis["performance_trends"] = {
            "cpu_trend": "stable",
            "memory_trend": "stable",
            "processing_trend": "stable"
        }
        
        trend_analysis["error_trends"] = {
            "error_rate_trend": "stable",
            "error_severity_trend": "stable",
            "most_common_errors": []
        }
        
        # Generate trend-based recommendations
        trend_analysis["recommendations"].append(
            "Enable historical data collection for better trend analysis"
        )
        
        return trend_analysis
    
    def _generate_remoted_alerts(self, stats_data: Dict[str, Any], query) -> List[Dict[str, Any]]:
        """Generate alerts based on remoted statistics."""
        alerts = []
        
        global_stats = stats_data.get("global_stats", {})
        
        # CPU alerts
        if global_stats.get("average_cpu_usage", 0) > query.threshold_cpu:
            alerts.append({
                "type": "performance",
                "severity": "warning",
                "message": f"Average CPU usage ({global_stats['average_cpu_usage']:.1f}%) exceeds threshold ({query.threshold_cpu}%)",
                "category": "cpu_usage",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        # Memory alerts
        if global_stats.get("average_memory_usage", 0) > query.threshold_memory:
            alerts.append({
                "type": "performance",
                "severity": "warning",
                "message": f"Average memory usage ({global_stats['average_memory_usage']:.1f}%) exceeds threshold ({query.threshold_memory}%)",
                "category": "memory_usage",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        # Connection alerts
        if global_stats.get("total_connections", 0) == 0:
            alerts.append({
                "type": "connectivity",
                "severity": "critical",
                "message": "No active agent connections detected",
                "category": "no_connections",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        # Error alerts
        if global_stats.get("total_errors", 0) > 10:
            alerts.append({
                "type": "error",
                "severity": "warning",
                "message": f"High error count detected: {global_stats['total_errors']} errors",
                "category": "high_errors",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        # Health status alerts
        if global_stats.get("health_status") == "critical":
            alerts.append({
                "type": "health",
                "severity": "critical",
                "message": "Overall system health is critical",
                "category": "health_critical",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        return alerts
    
    def _generate_remoted_recommendations(self, stats_data: Dict[str, Any], query) -> List[Dict[str, Any]]:
        """Generate recommendations based on remoted statistics analysis."""
        recommendations = []
        
        global_stats = stats_data.get("global_stats", {})
        
        # Performance recommendations
        if global_stats.get("average_cpu_usage", 0) > 70:
            recommendations.append({
                "category": "performance",
                "priority": "medium",
                "title": "Optimize CPU Usage",
                "description": "High CPU usage detected across remoted daemons",
                "action": "Review remoted configuration and consider load balancing",
                "impact": "Improved system performance and responsiveness"
            })
        
        if global_stats.get("average_memory_usage", 0) > 70:
            recommendations.append({
                "category": "performance",
                "priority": "medium",
                "title": "Optimize Memory Usage",
                "description": "High memory usage detected across remoted daemons",
                "action": "Review memory settings and consider increasing available RAM",
                "impact": "Reduced memory pressure and improved stability"
            })
        
        # Connection recommendations
        disconnected_agents = global_stats.get("global_connection_types", {}).get("disconnected", 0)
        if disconnected_agents > 0:
            recommendations.append({
                "category": "connectivity",
                "priority": "high",
                "title": "Investigate Disconnected Agents",
                "description": f"{disconnected_agents} agents are currently disconnected",
                "action": "Review agent connectivity and network configuration",
                "impact": "Improved monitoring coverage and data collection"
            })
        
        # Error recommendations
        if global_stats.get("total_errors", 0) > 5:
            recommendations.append({
                "category": "error_management",
                "priority": "high",
                "title": "Address System Errors",
                "description": f"Multiple errors detected: {global_stats['total_errors']} total errors",
                "action": "Review error logs and implement corrective measures",
                "impact": "Improved system stability and reliability"
            })
        
        # General recommendations
        recommendations.append({
            "category": "monitoring",
            "priority": "low",
            "title": "Enable Continuous Monitoring",
            "description": "Set up automated monitoring and alerting for remoted statistics",
            "action": "Configure monitoring tools and establish alerting thresholds",
            "impact": "Proactive issue detection and resolution"
        })
        
        return recommendations
    
    def _format_summary_output(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Format analysis output in summary format."""
        return {
            "summary": {
                "total_nodes": analysis["global_summary"]["total_nodes"],
                "active_nodes": analysis["global_summary"]["active_nodes"],
                "health_status": analysis["global_summary"]["health_status"],
                "total_connections": analysis["global_summary"]["total_connections"],
                "total_errors": analysis["global_summary"]["total_errors"],
                "performance_status": analysis["global_summary"]["performance_summary"]
            },
            "alerts": analysis["alerts"],
            "top_recommendations": analysis["recommendations"][:3],
            "analysis_metadata": analysis["analysis_metadata"]
        }
    
    def _format_minimal_output(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Format analysis output in minimal format."""
        return {
            "status": analysis["global_summary"]["health_status"],
            "nodes": analysis["global_summary"]["total_nodes"],
            "connections": analysis["global_summary"]["total_connections"],
            "errors": analysis["global_summary"]["total_errors"],
            "alerts": len(analysis["alerts"]),
            "timestamp": analysis["analysis_metadata"]["timestamp"]
        }

    def _analyze_communication_metrics(self, node_stats: dict, threshold_latency: float, threshold_error_rate: float) -> dict:
        """Analyze communication health metrics across nodes."""
        communication_metrics = {
            "summary": {},
            "node_metrics": {},
            "latency_analysis": {},
            "error_rate_analysis": {},
            "connection_health": {},
            "throughput_summary": {}
        }
        
        total_latency = 0
        total_error_rate = 0
        healthy_nodes = 0
        total_nodes = 0
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
                
            total_nodes += 1
            
            # Extract communication metrics
            connections = stats.get("connections", {})
            performance = stats.get("performance", {})
            
            avg_latency = connections.get("average_latency", 0)
            error_rate = connections.get("error_rate", 0)
            connection_count = connections.get("active_connections", 0)
            
            node_metrics = {
                "average_latency": avg_latency,
                "error_rate": error_rate,
                "active_connections": connection_count,
                "connection_health": "healthy",
                "throughput": connections.get("messages_per_second", 0),
                "timeouts": connections.get("timeout_count", 0)
            }
            
            # Assess node health
            if avg_latency > threshold_latency:
                node_metrics["connection_health"] = "high_latency"
            elif error_rate > threshold_error_rate:
                node_metrics["connection_health"] = "high_errors"
            else:
                healthy_nodes += 1
            
            communication_metrics["node_metrics"][node_name] = node_metrics
            total_latency += avg_latency
            total_error_rate += error_rate
        
        # Calculate summary metrics
        if total_nodes > 0:
            communication_metrics["summary"] = {
                "average_latency": round(total_latency / total_nodes, 2),
                "average_error_rate": round(total_error_rate / total_nodes, 2),
                "healthy_nodes": healthy_nodes,
                "total_nodes": total_nodes,
                "health_percentage": round((healthy_nodes / total_nodes) * 100, 2)
            }
        
        return communication_metrics

    def _analyze_health_monitoring(self, node_stats: dict, query) -> dict:
        """Analyze health monitoring and diagnostics."""
        health_monitoring = {
            "overall_health": "unknown",
            "node_health": {},
            "critical_issues": [],
            "health_scores": {},
            "availability_metrics": {},
            "system_status": {}
        }
        
        health_scores = []
        critical_count = 0
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            # Calculate comprehensive health score
            performance = stats.get("performance", {})
            connections = stats.get("connections", {})
            queues = stats.get("queues", {})
            errors = stats.get("errors", {})
            
            cpu_score = max(0, 100 - performance.get("cpu_usage", {}).get("current", 0))
            memory_score = max(0, 100 - performance.get("memory_usage", {}).get("current_percent", 0))
            queue_score = max(0, 100 - queues.get("queue_utilization", 0))
            error_score = max(0, 100 - errors.get("error_rate", 0))
            
            overall_score = (cpu_score + memory_score + queue_score + error_score) / 4
            health_scores.append(overall_score)
            
            node_health = {
                "overall_score": round(overall_score, 2),
                "cpu_score": round(cpu_score, 2),
                "memory_score": round(memory_score, 2),
                "queue_score": round(queue_score, 2),
                "error_score": round(error_score, 2),
                "status": "healthy" if overall_score >= 80 else ("warning" if overall_score >= 60 else "critical"),
                "uptime": performance.get("uptime", "unknown"),
                "last_seen": stats.get("last_update", "unknown")
            }
            
            if overall_score < 60:
                critical_count += 1
                health_monitoring["critical_issues"].append({
                    "node": node_name,
                    "score": round(overall_score, 2),
                    "issues": self._identify_health_issues(stats, query)
                })
            
            health_monitoring["node_health"][node_name] = node_health
            health_monitoring["health_scores"][node_name] = overall_score
        
        # Calculate overall health
        if health_scores:
            avg_health = sum(health_scores) / len(health_scores)
            if avg_health >= 80:
                health_monitoring["overall_health"] = "healthy"
            elif avg_health >= 60:
                health_monitoring["overall_health"] = "degraded"
            else:
                health_monitoring["overall_health"] = "critical"
            
            health_monitoring["system_status"] = {
                "average_health_score": round(avg_health, 2),
                "critical_nodes": critical_count,
                "total_nodes": len(health_scores),
                "critical_percentage": round((critical_count / len(health_scores)) * 100, 2)
            }
        
        return health_monitoring

    def _identify_health_issues(self, stats: dict, query) -> list:
        """Identify specific health issues for a node."""
        issues = []
        
        performance = stats.get("performance", {})
        connections = stats.get("connections", {})
        queues = stats.get("queues", {})
        errors = stats.get("errors", {})
        
        cpu_usage = performance.get("cpu_usage", {}).get("current", 0)
        memory_usage = performance.get("memory_usage", {}).get("current_percent", 0)
        queue_util = queues.get("queue_utilization", 0)
        error_rate = errors.get("error_rate", 0)
        
        if cpu_usage > query.threshold_cpu:
            issues.append(f"High CPU usage: {cpu_usage}%")
        
        if memory_usage > query.threshold_memory:
            issues.append(f"High memory usage: {memory_usage}%")
        
        if queue_util > 80:
            issues.append(f"High queue utilization: {queue_util}%")
        
        if error_rate > query.threshold_error_rate:
            issues.append(f"High error rate: {error_rate}%")
        
        if connections.get("active_connections", 0) == 0:
            issues.append("No active connections")
        
        return issues

    def _analyze_throughput_metrics(self, node_stats: dict, time_range: dict) -> dict:
        """Analyze throughput and latency metrics."""
        throughput_analysis = {
            "summary": {},
            "node_throughput": {},
            "latency_distribution": {},
            "performance_trends": {},
            "bottleneck_analysis": {}
        }
        
        total_throughput = 0
        total_latency = 0
        node_count = 0
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            node_count += 1
            connections = stats.get("connections", {})
            events = stats.get("events", {})
            
            messages_per_sec = connections.get("messages_per_second", 0)
            events_per_sec = events.get("events_per_second", 0)
            avg_latency = connections.get("average_latency", 0)
            max_latency = connections.get("max_latency", 0)
            
            node_throughput = {
                "messages_per_second": messages_per_sec,
                "events_per_second": events_per_sec,
                "total_throughput": messages_per_sec + events_per_sec,
                "average_latency": avg_latency,
                "max_latency": max_latency,
                "efficiency_score": self._calculate_efficiency_score(messages_per_sec, avg_latency)
            }
            
            throughput_analysis["node_throughput"][node_name] = node_throughput
            total_throughput += messages_per_sec + events_per_sec
            total_latency += avg_latency
        
        if node_count > 0:
            throughput_analysis["summary"] = {
                "total_throughput": total_throughput,
                "average_throughput_per_node": round(total_throughput / node_count, 2),
                "average_latency": round(total_latency / node_count, 2),
                "analysis_period": time_range.get("duration_hours", 0)
            }
        
        return throughput_analysis

    def _calculate_efficiency_score(self, throughput: float, latency: float) -> float:
        """Calculate efficiency score based on throughput and latency."""
        if latency == 0:
            return 100.0
        
        # Higher throughput is better, lower latency is better
        efficiency = (throughput / max(latency, 0.001)) * 10
        return min(100.0, efficiency)

    def _calculate_reliability_scores(self, node_stats: dict, time_range: dict) -> dict:
        """Calculate reliability and availability scores."""
        reliability_scoring = {
            "overall_reliability": "unknown",
            "node_reliability": {},
            "availability_metrics": {},
            "mtbf_analysis": {},
            "sla_compliance": {}
        }
        
        reliability_scores = []
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            connections = stats.get("connections", {})
            errors = stats.get("errors", {})
            performance = stats.get("performance", {})
            
            # Calculate reliability factors
            uptime_hours = performance.get("uptime_hours", 0)
            total_hours = time_range.get("duration_hours", 24)
            availability = (uptime_hours / max(total_hours, 1)) * 100
            
            error_rate = errors.get("error_rate", 0)
            connection_success_rate = connections.get("success_rate", 100)
            
            # Overall reliability score
            reliability_score = (availability * 0.4 + 
                               (100 - error_rate) * 0.3 + 
                               connection_success_rate * 0.3)
            
            reliability_scores.append(reliability_score)
            
            node_reliability = {
                "reliability_score": round(reliability_score, 2),
                "availability_percentage": round(availability, 2),
                "error_rate": round(error_rate, 2),
                "connection_success_rate": round(connection_success_rate, 2),
                "uptime_hours": uptime_hours,
                "mtbf": self._calculate_mtbf(errors.get("failure_count", 0), uptime_hours),
                "sla_status": "compliant" if reliability_score >= 99.0 else "non_compliant"
            }
            
            reliability_scoring["node_reliability"][node_name] = node_reliability
        
        if reliability_scores:
            avg_reliability = sum(reliability_scores) / len(reliability_scores)
            reliability_scoring["overall_reliability"] = round(avg_reliability, 2)
            
            reliability_scoring["availability_metrics"] = {
                "average_reliability": round(avg_reliability, 2),
                "highest_reliability": round(max(reliability_scores), 2),
                "lowest_reliability": round(min(reliability_scores), 2),
                "nodes_above_99": sum(1 for score in reliability_scores if score >= 99.0),
                "total_nodes": len(reliability_scores)
            }
        
        return reliability_scoring

    def _calculate_mtbf(self, failure_count: int, uptime_hours: float) -> float:
        """Calculate Mean Time Between Failures."""
        if failure_count == 0:
            return uptime_hours
        return uptime_hours / failure_count

    def _generate_diagnostics(self, node_stats: dict, query) -> dict:
        """Generate troubleshooting diagnostics."""
        diagnostics = {
            "summary": {},
            "node_diagnostics": {},
            "common_issues": [],
            "optimization_suggestions": [],
            "troubleshooting_guide": []
        }
        
        issue_patterns = {}
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            node_diagnostics = {
                "issues_detected": [],
                "performance_recommendations": [],
                "configuration_suggestions": [],
                "monitoring_alerts": []
            }
            
            # Analyze performance issues
            performance = stats.get("performance", {})
            cpu_usage = performance.get("cpu_usage", {}).get("current", 0)
            memory_usage = performance.get("memory_usage", {}).get("current_percent", 0)
            
            if cpu_usage > query.threshold_cpu:
                issue = f"High CPU usage detected: {cpu_usage}%"
                node_diagnostics["issues_detected"].append(issue)
                node_diagnostics["performance_recommendations"].append("Consider scaling or optimizing CPU-intensive processes")
                issue_patterns["high_cpu"] = issue_patterns.get("high_cpu", 0) + 1
            
            if memory_usage > query.threshold_memory:
                issue = f"High memory usage detected: {memory_usage}%"
                node_diagnostics["issues_detected"].append(issue)
                node_diagnostics["performance_recommendations"].append("Review memory allocation and consider increasing memory")
                issue_patterns["high_memory"] = issue_patterns.get("high_memory", 0) + 1
            
            # Analyze connection issues
            connections = stats.get("connections", {})
            if connections.get("active_connections", 0) == 0:
                node_diagnostics["issues_detected"].append("No active connections")
                node_diagnostics["monitoring_alerts"].append("Check network connectivity and agent configurations")
                issue_patterns["no_connections"] = issue_patterns.get("no_connections", 0) + 1
            
            # Analyze queue issues
            queues = stats.get("queues", {})
            queue_util = queues.get("queue_utilization", 0)
            if queue_util > 80:
                node_diagnostics["issues_detected"].append(f"High queue utilization: {queue_util}%")
                node_diagnostics["configuration_suggestions"].append("Consider increasing queue size or processing capacity")
                issue_patterns["high_queue"] = issue_patterns.get("high_queue", 0) + 1
            
            diagnostics["node_diagnostics"][node_name] = node_diagnostics
        
        # Generate common issues summary
        diagnostics["common_issues"] = [
            {"issue": issue_type, "affected_nodes": count}
            for issue_type, count in issue_patterns.items()
            if count > 1
        ]
        
        return diagnostics

    def _analyze_capacity_planning(self, node_stats: dict, time_range: dict, query) -> dict:
        """Analyze capacity planning metrics and scaling recommendations."""
        capacity_planning = {
            "current_capacity": {},
            "utilization_trends": {},
            "scaling_recommendations": [],
            "resource_forecasting": {},
            "bottleneck_analysis": {}
        }
        
        total_cpu_used = 0
        total_memory_used = 0
        total_queue_used = 0
        node_count = 0
        
        for node_name, stats in node_stats.items():
            if stats.get("collection_error"):
                continue
            
            node_count += 1
            performance = stats.get("performance", {})
            queues = stats.get("queues", {})
            
            cpu_usage = performance.get("cpu_usage", {}).get("current", 0)
            memory_usage = performance.get("memory_usage", {}).get("current_percent", 0)
            queue_util = queues.get("queue_utilization", 0)
            
            total_cpu_used += cpu_usage
            total_memory_used += memory_usage
            total_queue_used += queue_util
            
            # Node-specific capacity analysis
            node_capacity = {
                "cpu_utilization": cpu_usage,
                "memory_utilization": memory_usage,
                "queue_utilization": queue_util,
                "headroom_cpu": 100 - cpu_usage,
                "headroom_memory": 100 - memory_usage,
                "capacity_status": self._determine_capacity_status(cpu_usage, memory_usage, queue_util)
            }
            
            capacity_planning["current_capacity"][node_name] = node_capacity
            
            # Generate scaling recommendations for individual nodes
            if cpu_usage > 80 or memory_usage > 80:
                capacity_planning["scaling_recommendations"].append({
                    "node": node_name,
                    "recommendation": "Consider vertical scaling (increase CPU/memory)",
                    "priority": "high" if max(cpu_usage, memory_usage) > 90 else "medium",
                    "reason": f"CPU: {cpu_usage}%, Memory: {memory_usage}%"
                })
            
            if queue_util > 80:
                capacity_planning["scaling_recommendations"].append({
                    "node": node_name,
                    "recommendation": "Consider horizontal scaling (add more nodes)",
                    "priority": "high",
                    "reason": f"Queue utilization: {queue_util}%"
                })
        
        # Overall capacity analysis
        if node_count > 0:
            avg_cpu = total_cpu_used / node_count
            avg_memory = total_memory_used / node_count
            avg_queue = total_queue_used / node_count
            
            capacity_planning["utilization_trends"] = {
                "average_cpu_utilization": round(avg_cpu, 2),
                "average_memory_utilization": round(avg_memory, 2),
                "average_queue_utilization": round(avg_queue, 2),
                "overall_capacity_health": self._determine_capacity_status(avg_cpu, avg_memory, avg_queue)
            }
            
            # Forecasting
            capacity_planning["resource_forecasting"] = {
                "predicted_cpu_24h": min(100, avg_cpu * 1.1),  # Simple 10% growth assumption
                "predicted_memory_24h": min(100, avg_memory * 1.05),  # 5% growth assumption
                "time_to_capacity_limit": self._calculate_time_to_limit(avg_cpu, avg_memory),
                "recommended_action": self._recommend_capacity_action(avg_cpu, avg_memory, avg_queue)
            }
        
        return capacity_planning

    def _determine_capacity_status(self, cpu: float, memory: float, queue: float) -> str:
        """Determine capacity status based on utilization."""
        max_util = max(cpu, memory, queue)
        if max_util >= 90:
            return "critical"
        elif max_util >= 80:
            return "high"
        elif max_util >= 60:
            return "moderate"
        else:
            return "healthy"

    def _calculate_time_to_limit(self, cpu: float, memory: float) -> str:
        """Calculate estimated time to reach capacity limit."""
        max_util = max(cpu, memory)
        if max_util >= 90:
            return "immediate"
        elif max_util >= 80:
            return "within_24h"
        elif max_util >= 70:
            return "within_week"
        else:
            return "normal"

    def _recommend_capacity_action(self, cpu: float, memory: float, queue: float) -> str:
        """Recommend capacity actions based on utilization."""
        if max(cpu, memory, queue) >= 90:
            return "immediate_scaling_required"
        elif max(cpu, memory, queue) >= 80:
            return "plan_scaling_within_24h"
        elif max(cpu, memory, queue) >= 70:
            return "monitor_closely"
        else:
            return "no_action_needed"

    def _generate_enhanced_remoted_alerts(self, stats_data: dict, query, analysis: dict) -> list:
        """Generate enhanced alerts with comprehensive analysis."""
        alerts = []
        
        # Health monitoring alerts
        health_data = analysis.get("health_monitoring", {})
        critical_issues = health_data.get("critical_issues", [])
        
        for issue in critical_issues:
            alerts.append({
                "type": "health_critical",
                "severity": "critical",
                "node": issue["node"],
                "message": f"Critical health issue on {issue['node']}: score {issue['score']}",
                "details": issue["issues"],
                "timestamp": datetime.utcnow().isoformat(),
                "action_required": True
            })
        
        # Communication alerts
        comm_data = analysis.get("communication_metrics", {})
        node_metrics = comm_data.get("node_metrics", {})
        
        for node_name, metrics in node_metrics.items():
            if metrics.get("connection_health") != "healthy":
                alerts.append({
                    "type": "communication_issue",
                    "severity": "high",
                    "node": node_name,
                    "message": f"Communication issue on {node_name}: {metrics['connection_health']}",
                    "details": {
                        "latency": metrics.get("average_latency", 0),
                        "error_rate": metrics.get("error_rate", 0)
                    },
                    "timestamp": datetime.utcnow().isoformat(),
                    "action_required": True
                })
        
        # Capacity alerts
        capacity_data = analysis.get("capacity_planning", {})
        recommendations = capacity_data.get("scaling_recommendations", [])
        
        for rec in recommendations:
            if rec.get("priority") == "high":
                alerts.append({
                    "type": "capacity_warning",
                    "severity": "high",
                    "node": rec["node"],
                    "message": f"Capacity limit approaching on {rec['node']}",
                    "details": rec["reason"],
                    "recommendation": rec["recommendation"],
                    "timestamp": datetime.utcnow().isoformat(),
                    "action_required": True
                })
        
        return alerts

    def _generate_enhanced_remoted_recommendations(self, stats_data: dict, query, analysis: dict) -> list:
        """Generate enhanced recommendations based on comprehensive analysis."""
        recommendations = []
        
        # Health-based recommendations
        health_data = analysis.get("health_monitoring", {})
        system_status = health_data.get("system_status", {})
        
        if system_status.get("critical_percentage", 0) > 20:
            recommendations.append({
                "type": "health_improvement",
                "priority": "critical",
                "recommendation": "Multiple nodes showing critical health issues",
                "actions": [
                    "Review system resources across all nodes",
                    "Check for common configuration issues",
                    "Consider infrastructure maintenance"
                ],
                "affected_nodes": system_status.get("critical_nodes", 0),
                "timeframe": "immediate"
            })
        
        # Communication-based recommendations
        comm_data = analysis.get("communication_metrics", {})
        summary = comm_data.get("summary", {})
        
        if summary.get("health_percentage", 100) < 80:
            recommendations.append({
                "type": "communication_optimization",
                "priority": "high",
                "recommendation": "Optimize communication performance",
                "actions": [
                    "Review network configuration",
                    "Check for bandwidth limitations",
                    "Optimize connection pooling"
                ],
                "current_health": f"{summary.get('health_percentage', 0)}%",
                "timeframe": "within_24h"
            })
        
        # Capacity-based recommendations
        capacity_data = analysis.get("capacity_planning", {})
        forecast = capacity_data.get("resource_forecasting", {})
        
        if forecast.get("recommended_action") == "immediate_scaling_required":
            recommendations.append({
                "type": "scaling_urgent",
                "priority": "critical",
                "recommendation": "Immediate scaling required",
                "actions": [
                    "Add additional nodes or increase resources",
                    "Redistribute load if possible",
                    "Monitor capacity continuously"
                ],
                "timeframe": "immediate"
            })
        
        # Reliability improvements
        reliability_data = analysis.get("reliability_scoring", {})
        availability = reliability_data.get("availability_metrics", {})
        
        if availability.get("average_reliability", 100) < 99.0:
            recommendations.append({
                "type": "reliability_improvement",
                "priority": "medium",
                "recommendation": "Improve system reliability",
                "actions": [
                    "Identify and address failure points",
                    "Implement redundancy where needed",
                    "Review and optimize configurations"
                ],
                "current_reliability": f"{availability.get('average_reliability', 0)}%",
                "timeframe": "within_week"
            })
        
        return recommendations
    
    async def _handle_get_wazuh_log_collector_stats(self, arguments: dict) -> list[types.TextContent]:
        """Handle log collector performance and file monitoring analysis."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_log_collector_stats_query(arguments)
            
            # Calculate time range
            time_range = self._parse_time_range(validated_query.time_range)
            
            # Collect log collector statistics
            log_stats = await self._collect_log_collector_statistics(validated_query, time_range)
            
            # Analyze collected data
            analysis = await self._analyze_log_collector_stats(log_stats, validated_query, time_range, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except ValidationError as e:
            self.logger.error(f"Validation error in log collector stats analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
        except Exception as e:
            self.logger.error(f"Error in log collector stats analysis: {str(e)}")
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e)
            )]
    
    async def _collect_log_collector_statistics(self, query, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect comprehensive log collector statistics."""
        stats_data = {
            "time_range": time_range,
            "node_stats": {},
            "agent_stats": {},
            "global_stats": {},
            "performance_metrics": {},
            "file_monitoring": {},
            "processing_stats": {},
            "error_stats": {},
            "efficiency_metrics": {},
            "collection_errors": []
        }
        
        try:
            # Get cluster nodes
            nodes = await self._get_cluster_nodes()
            
            # Filter nodes if specified
            if query.node_filter:
                nodes = [node for node in nodes if node.get('name') in query.node_filter]
            
            # Collect stats for each node
            for node in nodes:
                node_name = node.get('name', 'unknown')
                self.logger.info(f"Collecting log collector stats for node: {node_name}")
                
                try:
                    node_stats = await self._collect_node_log_collector_stats(node, query, time_range)
                    stats_data["node_stats"][node_name] = node_stats
                    
                except Exception as e:
                    self.logger.warning(f"Error collecting log collector stats for node {node_name}: {str(e)}")
                    stats_data["collection_errors"].append({
                        "node": node_name,
                        "error": str(e),
                        "timestamp": datetime.utcnow().isoformat()
                    })
            
            # Get agent statistics
            if query.include_performance or query.include_file_monitoring:
                stats_data["agent_stats"] = await self._collect_agent_log_stats(query, time_range)
            
            # Calculate global aggregated statistics
            stats_data["global_stats"] = self._calculate_global_log_collector_stats(
                stats_data["node_stats"], stats_data["agent_stats"]
            )
            
        except Exception as e:
            self.logger.error(f"Error collecting log collector statistics: {str(e)}")
            stats_data["collection_errors"].append({
                "scope": "global",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            })
        
        return stats_data
    
    async def _collect_node_log_collector_stats(self, node: Dict[str, Any], query, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect log collector statistics for a specific node."""
        node_name = node.get('name', 'unknown')
        node_stats = {
            "node_info": node,
            "daemon_stats": {},
            "performance": {},
            "file_monitoring": {},
            "processing": {},
            "errors": {},
            "efficiency": {},
            "timestamp": datetime.utcnow().isoformat()
        }
        
        try:
            # Get daemon statistics for logcollector
            if query.include_performance or query.include_processing_stats:
                daemon_stats = await self._get_daemon_stats(node_name, "logcollector")
                node_stats["daemon_stats"] = daemon_stats
            
            # Performance metrics
            if query.include_performance:
                node_stats["performance"] = await self._collect_log_collector_performance_metrics(node_name, time_range)
            
            # File monitoring statistics
            if query.include_file_monitoring:
                node_stats["file_monitoring"] = await self._collect_file_monitoring_stats(node_name, time_range, query)
            
            # Processing statistics
            if query.include_processing_stats:
                node_stats["processing"] = await self._collect_log_processing_stats(node_name, time_range, query)
            
            # Error analysis
            if query.include_error_analysis:
                node_stats["errors"] = await self._collect_log_collector_error_stats(node_name, time_range)
            
            # Efficiency analysis
            if query.include_efficiency:
                node_stats["efficiency"] = await self._collect_log_collector_efficiency_stats(node_name, time_range)
            
            # Coverage analysis
            if query.include_coverage_analysis:
                node_stats["coverage_analysis"] = await self._collect_coverage_analysis_stats(node_name, time_range, query)
            
            # Resource monitoring
            if query.include_resource_monitoring:
                node_stats["resource_monitoring"] = await self._collect_resource_monitoring_stats(node_name, time_range, query)
            
            # Bottleneck detection
            if query.include_bottleneck_detection:
                node_stats["bottleneck_detection"] = await self._collect_bottleneck_detection_stats(node_name, time_range, query)
            
            # Capacity planning
            if query.include_capacity_planning:
                node_stats["capacity_planning"] = await self._collect_capacity_planning_stats(node_name, time_range, query)
            
        except Exception as e:
            self.logger.error(f"Error collecting node log collector stats for {node_name}: {str(e)}")
            node_stats["collection_error"] = str(e)
        
        return node_stats
    
    async def _collect_log_collector_performance_metrics(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect performance metrics for log collector daemon."""
        performance_data = {
            "processing_rate": {},
            "throughput": {},
            "resource_usage": {},
            "latency_metrics": {},
            "bottlenecks": []
        }
        
        try:
            # Get system performance data
            system_stats = await self._get_system_stats(node_name)
            
            # Processing rate analysis
            performance_data["processing_rate"] = {
                "logs_per_second": system_stats.get("logs_processed", 0) / time_range["duration_hours"] / 3600,
                "events_per_minute": system_stats.get("events_generated", 0) / time_range["duration_hours"] / 60,
                "files_monitored": system_stats.get("files_monitored", 0),
                "active_watchers": system_stats.get("active_watchers", 0)
            }
            
            # Throughput metrics
            performance_data["throughput"] = {
                "bytes_processed": system_stats.get("bytes_processed", 0),
                "average_log_size": system_stats.get("average_log_size", 0),
                "compression_ratio": system_stats.get("compression_ratio", 1.0),
                "data_rate_mbps": (system_stats.get("bytes_processed", 0) / (1024 * 1024)) / time_range["duration_hours"]
            }
            
            # Resource usage
            performance_data["resource_usage"] = {
                "cpu_usage": system_stats.get("cpu_percent", 0),
                "memory_usage_mb": system_stats.get("memory_mb", 0),
                "disk_io_rate": system_stats.get("disk_io_rate", 0),
                "file_descriptors": system_stats.get("file_descriptors", 0)
            }
            
            # Latency metrics
            performance_data["latency_metrics"] = {
                "average_processing_time_ms": system_stats.get("avg_processing_time", 0),
                "file_read_latency_ms": system_stats.get("file_read_latency", 0),
                "queue_wait_time_ms": system_stats.get("queue_wait_time", 0),
                "end_to_end_latency_ms": system_stats.get("end_to_end_latency", 0)
            }
            
            # Identify bottlenecks
            if performance_data["resource_usage"]["cpu_usage"] > 80:
                performance_data["bottlenecks"].append({
                    "type": "cpu",
                    "severity": "high",
                    "value": performance_data["resource_usage"]["cpu_usage"],
                    "description": "High CPU usage in log collector"
                })
            
            if performance_data["latency_metrics"]["average_processing_time_ms"] > 1000:
                performance_data["bottlenecks"].append({
                    "type": "processing_latency",
                    "severity": "medium",
                    "value": performance_data["latency_metrics"]["average_processing_time_ms"],
                    "description": "High log processing latency"
                })
            
        except Exception as e:
            self.logger.warning(f"Error collecting performance metrics for {node_name}: {str(e)}")
            performance_data["error"] = str(e)
        
        return performance_data
    
    async def _collect_file_monitoring_stats(self, node_name: str, time_range: Dict[str, Any], query) -> Dict[str, Any]:
        """Collect file monitoring statistics."""
        monitoring_data = {
            "monitored_files": {},
            "file_types": {},
            "monitoring_efficiency": {},
            "lag_analysis": {},
            "rotation_handling": {},
            "issues": []
        }
        
        try:
            # Get file monitoring configuration from agents
            agents_response = await self.api_client.get_agents()
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            # Filter agents if specified
            if query.agent_filter:
                agents = [agent for agent in agents if 
                         agent.get("id") in query.agent_filter or 
                         agent.get("name") in query.agent_filter]
            
            total_files = 0
            monitored_types = {}
            lag_issues = []
            rotation_issues = []
            
            for agent in agents:
                agent_id = agent.get("id")
                
                try:
                    # Get agent configuration for logcollector
                    config_response = await self.api_client.get_agent_config(agent_id, "logcollector")
                    config_data = config_response.get("data", {})
                    
                    # Analyze configured log files
                    localfiles = config_data.get("logcollector", {}).get("localfile", [])
                    if not isinstance(localfiles, list):
                        localfiles = [localfiles] if localfiles else []
                    
                    for localfile in localfiles:
                        if isinstance(localfile, dict):
                            log_format = localfile.get("log_format", "unknown")
                            monitored_types[log_format] = monitored_types.get(log_format, 0) + 1
                            total_files += 1
                            
                            # Check for potential lag issues
                            target = localfile.get("target", "")
                            if any(keyword in target.lower() for keyword in ["rotate", "large", "archive"]):
                                lag_issues.append({
                                    "agent_id": agent_id,
                                    "file": target,
                                    "issue": "potential_lag",
                                    "reason": "Large or rotated file detected"
                                })
                
                except Exception as e:
                    self.logger.debug(f"Could not get logcollector config for agent {agent_id}: {str(e)}")
                    continue
            
            # Compile monitoring statistics
            monitoring_data["monitored_files"] = {
                "total_files": total_files,
                "active_monitors": len([a for a in agents if a.get("status") == "active"]),
                "agents_with_logs": len(agents),
                "average_files_per_agent": total_files / max(len(agents), 1)
            }
            
            monitoring_data["file_types"] = monitored_types
            
            monitoring_data["monitoring_efficiency"] = {
                "coverage_percentage": min(100.0, (len(agents) / max(total_files, 1)) * 100),
                "active_monitoring_rate": len([a for a in agents if a.get("status") == "active"]) / max(len(agents), 1) * 100,
                "configuration_completeness": len(monitored_types) / max(total_files, 1) * 100 if total_files > 0 else 0
            }
            
            monitoring_data["lag_analysis"] = {
                "potential_lag_issues": len(lag_issues),
                "agents_with_issues": len(set(issue["agent_id"] for issue in lag_issues)),
                "lag_risk_score": min(100, len(lag_issues) * 10),
                "issues_details": lag_issues[:10]  # Limit to top 10
            }
            
            monitoring_data["rotation_handling"] = {
                "rotation_capable_files": len([f for f in monitored_types.keys() if "syslog" in f or "log" in f]),
                "rotation_efficiency": 85.0,  # Mock data - would analyze actual rotation handling
                "missed_rotations": len(rotation_issues)
            }
            
        except Exception as e:
            self.logger.warning(f"Error collecting file monitoring stats for {node_name}: {str(e)}")
            monitoring_data["error"] = str(e)
        
        return monitoring_data
    
    async def _collect_log_processing_stats(self, node_name: str, time_range: Dict[str, Any], query) -> Dict[str, Any]:
        """Collect log processing statistics."""
        processing_data = {
            "parsing_efficiency": {},
            "rule_matching": {},
            "filtering_stats": {},
            "format_distribution": {},
            "processing_errors": [],
            "performance_metrics": {}
        }
        
        try:
            # Get alerts to analyze processing efficiency
            alerts_response = await self.api_client.get_alerts(
                limit=5000,
                q=f"timestamp>{time_range['start_iso']};timestamp<{time_range['end_iso']}"
            )
            alerts = alerts_response.get("data", {}).get("affected_items", [])
            
            # Filter by log type if specified
            if query.log_type_filter:
                filtered_alerts = []
                for alert in alerts:
                    rule_groups = alert.get("rule", {}).get("groups", [])
                    if any(log_type.lower() in " ".join(rule_groups).lower() for log_type in query.log_type_filter):
                        filtered_alerts.append(alert)
                alerts = filtered_alerts
            
            total_alerts = len(alerts)
            
            # Analyze parsing efficiency
            parsed_successfully = len([a for a in alerts if a.get("rule", {}).get("level", 0) > 0])
            processing_data["parsing_efficiency"] = {
                "total_logs_processed": total_alerts,
                "successfully_parsed": parsed_successfully,
                "parsing_success_rate": (parsed_successfully / max(total_alerts, 1)) * 100,
                "average_processing_rate": total_alerts / max(time_range["duration_hours"], 1),
                "failed_parsing_count": total_alerts - parsed_successfully
            }
            
            # Rule matching analysis
            rule_distribution = {}
            for alert in alerts:
                rule_id = alert.get("rule", {}).get("id", "unknown")
                rule_distribution[rule_id] = rule_distribution.get(rule_id, 0) + 1
            
            processing_data["rule_matching"] = {
                "unique_rules_triggered": len(rule_distribution),
                "most_frequent_rules": sorted(rule_distribution.items(), key=lambda x: x[1], reverse=True)[:10],
                "rule_coverage": len(rule_distribution) / max(total_alerts, 1) * 100,
                "average_matches_per_rule": sum(rule_distribution.values()) / max(len(rule_distribution), 1)
            }
            
            # Format distribution
            format_distribution = {}
            for alert in alerts:
                decoder = alert.get("decoder", {}).get("name", "unknown")
                format_distribution[decoder] = format_distribution.get(decoder, 0) + 1
            
            processing_data["format_distribution"] = format_distribution
            
            # Filtering statistics
            high_level_alerts = len([a for a in alerts if a.get("rule", {}).get("level", 0) >= 10])
            processing_data["filtering_stats"] = {
                "total_events": total_alerts,
                "high_priority_events": high_level_alerts,
                "filtering_ratio": (high_level_alerts / max(total_alerts, 1)) * 100,
                "noise_reduction": ((total_alerts - high_level_alerts) / max(total_alerts, 1)) * 100
            }
            
            # Performance metrics
            processing_data["performance_metrics"] = {
                "events_per_second": total_alerts / max(time_range["duration_hours"] * 3600, 1),
                "peak_processing_rate": max(rule_distribution.values()) if rule_distribution else 0,
                "processing_efficiency_score": min(100, (parsed_successfully / max(total_alerts, 1)) * 100),
                "latency_estimate_ms": 50 + (total_alerts / 1000)  # Mock latency calculation
            }
            
        except Exception as e:
            self.logger.warning(f"Error collecting processing stats for {node_name}: {str(e)}")
            processing_data["error"] = str(e)
        
        return processing_data
    
    async def _collect_log_collector_error_stats(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect error statistics for log collector."""
        error_data = {
            "total_errors": 0,
            "error_categories": {},
            "critical_errors": [],
            "error_trends": [],
            "impact_analysis": {},
            "resolution_suggestions": []
        }
        
        try:
            # Get logs for error analysis
            logs_response = await self.api_client.get_logs(
                limit=5000,
                q=f"timestamp>{time_range['start_iso']};timestamp<{time_range['end_iso']};level>1"
            )
            
            logs = logs_response.get("data", {}).get("affected_items", [])
            # Filter for logcollector related errors
            logcollector_logs = [log for log in logs if 
                               "logcollector" in log.get("tag", "").lower() or
                               "file" in log.get("description", "").lower() or
                               "log" in log.get("description", "").lower()]
            
            error_data["total_errors"] = len(logcollector_logs)
            
            # Categorize errors
            error_categories = {}
            critical_errors = []
            
            for log in logcollector_logs:
                level = log.get("level", 0)
                description = log.get("description", "unknown")
                
                # Categorize error types
                if "permission" in description.lower():
                    error_type = "permission_error"
                elif "file not found" in description.lower() or "no such file" in description.lower():
                    error_type = "file_not_found"
                elif "parse" in description.lower() or "format" in description.lower():
                    error_type = "parsing_error"
                elif "disk" in description.lower() or "space" in description.lower():
                    error_type = "disk_error"
                elif "timeout" in description.lower():
                    error_type = "timeout_error"
                elif "rotation" in description.lower():
                    error_type = "rotation_error"
                else:
                    error_type = "other_error"
                
                error_categories[error_type] = error_categories.get(error_type, 0) + 1
                
                # Identify critical errors
                if level >= 3:
                    critical_errors.append({
                        "timestamp": log.get("timestamp", "unknown"),
                        "level": level,
                        "description": description,
                        "category": error_type,
                        "impact": "high" if level >= 4 else "medium"
                    })
            
            error_data["error_categories"] = error_categories
            error_data["critical_errors"] = critical_errors[:10]  # Limit to top 10
            
            # Impact analysis
            error_data["impact_analysis"] = {
                "error_rate_percentage": (len(logcollector_logs) / max(len(logs), 1)) * 100,
                "critical_error_rate": (len(critical_errors) / max(len(logcollector_logs), 1)) * 100,
                "most_impactful_category": max(error_categories.keys(), key=lambda k: error_categories[k]) if error_categories else "none",
                "service_availability_impact": min(100, len(critical_errors) * 5)  # Mock calculation
            }
            
            # Resolution suggestions
            error_data["resolution_suggestions"] = self._generate_log_collector_error_resolutions(error_categories, critical_errors)
            
        except Exception as e:
            self.logger.warning(f"Error collecting log collector error stats for {node_name}: {str(e)}")
            error_data["error"] = str(e)
        
        return error_data
    
    async def _collect_log_collector_efficiency_stats(self, node_name: str, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect efficiency statistics for log collector."""
        efficiency_data = {
            "collection_efficiency": {},
            "resource_optimization": {},
            "coverage_analysis": {},
            "performance_score": 0,
            "optimization_opportunities": []
        }
        
        try:
            # Mock efficiency calculations (would typically analyze real performance data)
            efficiency_data["collection_efficiency"] = {
                "log_collection_rate": 95.2,  # Percentage of logs successfully collected
                "real_time_processing": 88.7,  # Percentage processed in real-time
                "queue_efficiency": 92.1,     # Queue utilization efficiency
                "parser_efficiency": 89.5     # Parsing success rate
            }
            
            efficiency_data["resource_optimization"] = {
                "memory_efficiency": 85.3,    # Memory usage optimization
                "cpu_efficiency": 78.9,       # CPU usage efficiency
                "io_efficiency": 82.4,        # Disk I/O efficiency
                "network_efficiency": 91.2    # Network usage efficiency
            }
            
            efficiency_data["coverage_analysis"] = {
                "file_coverage": 87.6,        # Percentage of target files monitored
                "agent_coverage": 94.1,       # Percentage of agents with log collection
                "log_type_coverage": 76.8,    # Coverage of different log types
                "critical_system_coverage": 98.2  # Coverage of critical system logs
            }
            
            # Calculate overall performance score
            scores = list(efficiency_data["collection_efficiency"].values()) + \
                    list(efficiency_data["resource_optimization"].values()) + \
                    list(efficiency_data["coverage_analysis"].values())
            efficiency_data["performance_score"] = sum(scores) / len(scores)
            
            # Identify optimization opportunities
            if efficiency_data["resource_optimization"]["cpu_efficiency"] < 80:
                efficiency_data["optimization_opportunities"].append({
                    "category": "cpu_optimization",
                    "priority": "medium",
                    "description": "CPU usage can be optimized for log collection",
                    "potential_improvement": "15-20%"
                })
            
            if efficiency_data["coverage_analysis"]["log_type_coverage"] < 80:
                efficiency_data["optimization_opportunities"].append({
                    "category": "coverage_expansion",
                    "priority": "high",
                    "description": "Expand log type coverage for better visibility",
                    "potential_improvement": "20-25%"
                })
            
        except Exception as e:
            self.logger.warning(f"Error collecting efficiency stats for {node_name}: {str(e)}")
            efficiency_data["error"] = str(e)
        
        return efficiency_data
    
    async def _collect_agent_log_stats(self, query, time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Collect agent-specific log collection statistics."""
        agent_stats = {
            "total_agents": 0,
            "agents_with_logs": 0,
            "agent_performance": {},
            "log_distribution": {},
            "connectivity_issues": []
        }
        
        try:
            # Get all agents
            agents_response = await self.api_client.get_agents()
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            # Filter agents if specified
            if query.agent_filter:
                agents = [agent for agent in agents if 
                         agent.get("id") in query.agent_filter or 
                         agent.get("name") in query.agent_filter]
            
            agent_stats["total_agents"] = len(agents)
            
            agents_with_logs = 0
            performance_data = {}
            log_distribution = {}
            
            for agent in agents:
                agent_id = agent.get("id")
                agent_name = agent.get("name", agent_id)
                
                try:
                    # Get alerts from this agent to assess log activity
                    agent_alerts = await self.api_client.get_alerts(
                        limit=1000,
                        q=f"timestamp>{time_range['start_iso']};timestamp<{time_range['end_iso']};agent.id={agent_id}"
                    )
                    
                    alerts = agent_alerts.get("data", {}).get("affected_items", [])
                    if alerts:
                        agents_with_logs += 1
                        
                        # Analyze agent performance
                        performance_data[agent_id] = {
                            "name": agent_name,
                            "log_count": len(alerts),
                            "logs_per_hour": len(alerts) / max(time_range["duration_hours"], 1),
                            "status": agent.get("status", "unknown"),
                            "last_keep_alive": agent.get("last_keep_alive", "unknown")
                        }
                        
                        # Log type distribution for this agent
                        agent_log_types = {}
                        for alert in alerts:
                            decoder = alert.get("decoder", {}).get("name", "unknown")
                            agent_log_types[decoder] = agent_log_types.get(decoder, 0) + 1
                        
                        log_distribution[agent_id] = agent_log_types
                    
                    # Check for connectivity issues
                    if agent.get("status") != "active":
                        agent_stats["connectivity_issues"].append({
                            "agent_id": agent_id,
                            "agent_name": agent_name,
                            "status": agent.get("status", "unknown"),
                            "last_seen": agent.get("last_keep_alive", "unknown")
                        })
                
                except Exception as e:
                    self.logger.debug(f"Could not get stats for agent {agent_id}: {str(e)}")
                    continue
            
            agent_stats["agents_with_logs"] = agents_with_logs
            agent_stats["agent_performance"] = performance_data
            agent_stats["log_distribution"] = log_distribution
            
        except Exception as e:
            self.logger.warning(f"Error collecting agent log stats: {str(e)}")
            agent_stats["error"] = str(e)
        
        return agent_stats
    
    def _calculate_global_log_collector_stats(self, node_stats: Dict[str, Any], agent_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate global aggregated log collector statistics."""
        global_stats = {
            "aggregated_metrics": {},
            "performance_summary": {},
            "coverage_summary": {},
            "health_overview": {}
        }
        
        try:
            # Aggregate metrics across nodes
            total_files_monitored = 0
            total_processing_rate = 0
            total_errors = 0
            node_count = 0
            
            for node_name, stats in node_stats.items():
                if "collection_error" not in stats:
                    node_count += 1
                    
                    # File monitoring
                    file_stats = stats.get("file_monitoring", {}).get("monitored_files", {})
                    total_files_monitored += file_stats.get("total_files", 0)
                    
                    # Performance
                    perf_stats = stats.get("performance", {}).get("processing_rate", {})
                    total_processing_rate += perf_stats.get("logs_per_second", 0)
                    
                    # Errors
                    error_stats = stats.get("errors", {})
                    total_errors += error_stats.get("total_errors", 0)
            
            global_stats["aggregated_metrics"] = {
                "total_nodes": len(node_stats),
                "active_nodes": node_count,
                "total_files_monitored": total_files_monitored,
                "total_agents": agent_stats.get("total_agents", 0),
                "agents_with_logs": agent_stats.get("agents_with_logs", 0),
                "total_processing_rate": total_processing_rate,
                "total_errors": total_errors
            }
            
            # Performance summary
            global_stats["performance_summary"] = {
                "average_processing_rate": total_processing_rate / max(node_count, 1),
                "files_per_node": total_files_monitored / max(node_count, 1),
                "agent_coverage_percentage": (agent_stats.get("agents_with_logs", 0) / 
                                            max(agent_stats.get("total_agents", 1), 1)) * 100,
                "error_rate_percentage": (total_errors / max(total_files_monitored, 1)) * 100
            }
            
            # Coverage summary
            global_stats["coverage_summary"] = {
                "monitored_infrastructure": total_files_monitored > 0,
                "multi_node_deployment": len(node_stats) > 1,
                "agent_participation_rate": global_stats["performance_summary"]["agent_coverage_percentage"],
                "monitoring_completeness": min(100, (total_files_monitored / max(agent_stats.get("total_agents", 1), 1)) * 10)
            }
            
            # Health overview
            health_score = 100
            if global_stats["performance_summary"]["error_rate_percentage"] > 5:
                health_score -= 20
            if global_stats["performance_summary"]["agent_coverage_percentage"] < 80:
                health_score -= 15
            if total_processing_rate < 100:
                health_score -= 10
            
            global_stats["health_overview"] = {
                "overall_health_score": max(0, health_score),
                "health_status": "excellent" if health_score >= 90 else 
                               "good" if health_score >= 70 else 
                               "fair" if health_score >= 50 else "poor",
                "critical_issues": total_errors > 10,
                "performance_issues": total_processing_rate < 50
            }
            
        except Exception as e:
            self.logger.error(f"Error calculating global log collector stats: {str(e)}")
            global_stats["error"] = str(e)
        
        return global_stats
    
    async def _analyze_log_collector_stats(self, stats_data: Dict[str, Any], query, 
                                         time_range: Dict[str, Any], start_time: datetime) -> Dict[str, Any]:
        """Analyze collected log collector statistics and generate insights."""
        analysis = {
            "query_parameters": {
                "time_range": query.time_range,
                "node_filter": query.node_filter,
                "agent_filter": query.agent_filter,
                "log_type_filter": query.log_type_filter,
                "include_performance": query.include_performance,
                "include_file_monitoring": query.include_file_monitoring,
                "include_processing_stats": query.include_processing_stats,
                "include_error_analysis": query.include_error_analysis,
                "include_efficiency": query.include_efficiency,
                "include_coverage_analysis": query.include_coverage_analysis,
                "include_resource_monitoring": query.include_resource_monitoring,
                "include_bottleneck_detection": query.include_bottleneck_detection,
                "include_capacity_planning": query.include_capacity_planning,
                "compliance_frameworks": query.compliance_frameworks,
                "group_by": query.group_by,
                "output_format": query.output_format
            },
            "summary": {},
            "node_analysis": {},
            "agent_analysis": {},
            "global_stats": stats_data.get("global_stats", {}),
            "performance_insights": [],
            "optimization_recommendations": [],
            "coverage_analysis": {},
            "resource_monitoring": {},
            "bottleneck_detection": {},
            "capacity_planning": {},
            "compliance_mapping": {},
            "alerts": [],
            "analysis_metadata": {}
        }
        
        try:
            # Generate summary
            analysis["summary"] = self._generate_log_collector_summary(stats_data, query)
            
            # Analyze each node
            for node_name, node_stats in stats_data.get("node_stats", {}).items():
                analysis["node_analysis"][node_name] = self._analyze_node_log_collector_stats(
                    node_stats, query, time_range
                )
            
            # Analyze agent performance
            analysis["agent_analysis"] = self._analyze_agent_log_performance(
                stats_data.get("agent_stats", {}), query
            )
            
            # Generate performance insights
            analysis["performance_insights"] = self._generate_log_collector_insights(
                stats_data, analysis["node_analysis"], analysis["agent_analysis"]
            )
            
            # Generate optimization recommendations
            analysis["optimization_recommendations"] = self._generate_log_collector_optimization_recommendations(
                stats_data, analysis["summary"], query
            )
            
            # Generate alerts based on thresholds
            analysis["alerts"] = self._generate_log_collector_alerts(
                stats_data, analysis["node_analysis"], query
            )
            
            # Enhanced analysis features
            if query.include_coverage_analysis:
                analysis["coverage_analysis"] = self._aggregate_coverage_analysis(stats_data, query)
            
            if query.include_resource_monitoring:
                analysis["resource_monitoring"] = self._aggregate_resource_monitoring(stats_data, query)
            
            if query.include_bottleneck_detection:
                analysis["bottleneck_detection"] = self._aggregate_bottleneck_detection(stats_data, query)
            
            if query.include_capacity_planning:
                analysis["capacity_planning"] = self._aggregate_capacity_planning(stats_data, query)
            
            # Compliance mapping
            if query.compliance_frameworks:
                analysis["compliance_mapping"] = self._aggregate_compliance_mapping(stats_data, query)
            
            # Include trends if requested
            if query.include_trends:
                analysis["trends"] = self._analyze_log_collector_trends(stats_data, time_range)
            
            # Format output based on requested format
            if query.output_format == "summary":
                analysis = self._format_log_collector_summary_output(analysis)
            elif query.output_format == "minimal":
                analysis = self._format_log_collector_minimal_output(analysis)
            
            # Analysis metadata
            analysis["analysis_metadata"] = {
                "analysis_duration_seconds": (datetime.utcnow() - start_time).total_seconds(),
                "timestamp": datetime.utcnow().isoformat(),
                "nodes_analyzed": len(stats_data.get("node_stats", {})),
                "agents_analyzed": stats_data.get("agent_stats", {}).get("total_agents", 0),
                "collection_errors": len(stats_data.get("collection_errors", [])),
                "time_range": time_range
            }
            
        except Exception as e:
            self.logger.error(f"Error analyzing log collector stats: {str(e)}")
            analysis["analysis_error"] = str(e)
        
        return analysis
    
    def _generate_log_collector_summary(self, stats_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Generate summary of log collector statistics."""
        global_stats = stats_data.get("global_stats", {})
        aggregated = global_stats.get("aggregated_metrics", {})
        performance = global_stats.get("performance_summary", {})
        health = global_stats.get("health_overview", {})
        
        return {
            "total_nodes": aggregated.get("total_nodes", 0),
            "active_nodes": aggregated.get("active_nodes", 0),
            "total_agents": aggregated.get("total_agents", 0),
            "agents_with_logs": aggregated.get("agents_with_logs", 0),
            "files_monitored": aggregated.get("total_files_monitored", 0),
            "processing_rate": aggregated.get("total_processing_rate", 0),
            "error_count": aggregated.get("total_errors", 0),
            "agent_coverage_percentage": performance.get("agent_coverage_percentage", 0),
            "error_rate_percentage": performance.get("error_rate_percentage", 0),
            "overall_health_score": health.get("overall_health_score", 0),
            "health_status": health.get("health_status", "unknown"),
            "collection_errors": len(stats_data.get("collection_errors", [])),
            "message": self._get_log_collector_status_message(health.get("overall_health_score", 0))
        }
    
    def _get_log_collector_status_message(self, health_score: float) -> str:
        """Get status message based on health score."""
        if health_score >= 90:
            return "Log collection is operating excellently with high efficiency"
        elif health_score >= 70:
            return "Log collection is performing well with minor optimization opportunities"
        elif health_score >= 50:
            return "Log collection has moderate issues that should be addressed"
        else:
            return "Log collection has significant issues requiring immediate attention"
    
    def _generate_log_collector_error_resolutions(self, error_categories: Dict[str, int], 
                                                critical_errors: List[Dict]) -> List[Dict[str, str]]:
        """Generate resolution suggestions for log collector errors."""
        resolutions = []
        
        # Permission errors
        if error_categories.get("permission_error", 0) > 0:
            resolutions.append({
                "error_type": "permission_error",
                "priority": "high",
                "resolution": "Check file permissions and Wazuh agent user access rights",
                "command": "chmod 644 /path/to/logfile && chown wazuh:wazuh /path/to/logfile"
            })
        
        # File not found errors
        if error_categories.get("file_not_found", 0) > 0:
            resolutions.append({
                "error_type": "file_not_found",
                "priority": "medium",
                "resolution": "Verify log file paths in ossec.conf and ensure files exist",
                "command": f"ls -la /path/to/logfile && {get_wazuh_paths()['bin']}/ossec-control restart"
            })
        
        # Parsing errors
        if error_categories.get("parsing_error", 0) > 0:
            resolutions.append({
                "error_type": "parsing_error",
                "priority": "medium",
                "resolution": "Review log formats and decoder configurations",
                "command": f"{get_wazuh_paths()['bin']}/ossec-logtest < /path/to/logfile"
            })
        
        # Disk errors
        if error_categories.get("disk_error", 0) > 0:
            resolutions.append({
                "error_type": "disk_error",
                "priority": "high",
                "resolution": "Check disk space and I/O performance",
                "command": "df -h && iostat -x 1 5"
            })
        
        return resolutions
    
    def _analyze_node_log_collector_stats(self, node_stats: Dict[str, Any], query, 
                                        time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze log collector statistics for a specific node."""
        return {
            "node_info": node_stats.get("node_info", {}),
            "performance_analysis": self._analyze_log_collector_performance(node_stats.get("performance", {}), query),
            "file_monitoring_analysis": self._analyze_file_monitoring(node_stats.get("file_monitoring", {}), query),
            "processing_analysis": self._analyze_log_processing(node_stats.get("processing", {}), query),
            "error_analysis": self._analyze_log_collector_errors(node_stats.get("errors", {})),
            "efficiency_analysis": node_stats.get("efficiency", {}),
            "health_score": self._calculate_node_log_collector_health(node_stats),
            "recommendations": self._generate_node_log_collector_recommendations(node_stats, query)
        }
    
    def _analyze_log_collector_performance(self, performance_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Analyze log collector performance metrics."""
        analysis = {
            "processing_efficiency": "good",
            "throughput_analysis": {},
            "resource_utilization": {},
            "bottleneck_detection": [],
            "performance_score": 85
        }
        
        try:
            processing_rate = performance_data.get("processing_rate", {})
            throughput = performance_data.get("throughput", {})
            resource_usage = performance_data.get("resource_usage", {})
            
            # Analyze processing rate
            logs_per_second = processing_rate.get("logs_per_second", 0)
            if logs_per_second >= query.threshold_processing_rate:
                analysis["processing_efficiency"] = "excellent"
            elif logs_per_second >= query.threshold_processing_rate * 0.7:
                analysis["processing_efficiency"] = "good"
            elif logs_per_second >= query.threshold_processing_rate * 0.4:
                analysis["processing_efficiency"] = "fair"
            else:
                analysis["processing_efficiency"] = "poor"
            
            # Throughput analysis
            analysis["throughput_analysis"] = {
                "data_rate_mbps": throughput.get("data_rate_mbps", 0),
                "compression_efficiency": throughput.get("compression_ratio", 1.0),
                "average_log_size": throughput.get("average_log_size", 0),
                "throughput_score": min(100, logs_per_second / 10)
            }
            
            # Resource utilization
            cpu_usage = resource_usage.get("cpu_usage", 0)
            memory_usage = resource_usage.get("memory_usage_mb", 0)
            
            analysis["resource_utilization"] = {
                "cpu_efficiency": 100 - cpu_usage,
                "memory_efficiency": max(0, 100 - (memory_usage / 1024)),  # Assuming 1GB is baseline
                "io_performance": 100 - min(50, resource_usage.get("disk_io_rate", 0) / 100),
                "resource_score": (100 - cpu_usage + max(0, 100 - memory_usage / 10)) / 2
            }
            
            # Bottleneck detection
            bottlenecks = performance_data.get("bottlenecks", [])
            analysis["bottleneck_detection"] = bottlenecks
            
            # Calculate performance score
            scores = [
                analysis["throughput_analysis"]["throughput_score"],
                analysis["resource_utilization"]["resource_score"],
                100 - len(bottlenecks) * 10  # Deduct for bottlenecks
            ]
            analysis["performance_score"] = sum(scores) / len(scores)
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def _analyze_file_monitoring(self, monitoring_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Analyze file monitoring statistics."""
        analysis = {
            "monitoring_coverage": {},
            "efficiency_metrics": {},
            "lag_assessment": {},
            "configuration_health": {},
            "monitoring_score": 85
        }
        
        try:
            monitored_files = monitoring_data.get("monitored_files", {})
            efficiency = monitoring_data.get("monitoring_efficiency", {})
            lag_analysis = monitoring_data.get("lag_analysis", {})
            
            # Coverage analysis
            analysis["monitoring_coverage"] = {
                "total_files": monitored_files.get("total_files", 0),
                "active_monitors": monitored_files.get("active_monitors", 0),
                "coverage_ratio": monitored_files.get("active_monitors", 0) / max(monitored_files.get("total_files", 1), 1),
                "files_per_agent": monitored_files.get("average_files_per_agent", 0)
            }
            
            # Efficiency metrics
            analysis["efficiency_metrics"] = {
                "coverage_percentage": efficiency.get("coverage_percentage", 0),
                "active_monitoring_rate": efficiency.get("active_monitoring_rate", 0),
                "configuration_completeness": efficiency.get("configuration_completeness", 0)
            }
            
            # Lag assessment
            lag_issues = lag_analysis.get("potential_lag_issues", 0)
            lag_threshold = query.threshold_file_lag
            
            analysis["lag_assessment"] = {
                "lag_issues_count": lag_issues,
                "lag_risk_level": "high" if lag_issues > 5 else "medium" if lag_issues > 2 else "low",
                "agents_affected": lag_analysis.get("agents_with_issues", 0),
                "lag_score": max(0, 100 - lag_analysis.get("lag_risk_score", 0))
            }
            
            # Configuration health
            analysis["configuration_health"] = {
                "file_types_diversity": len(monitoring_data.get("file_types", {})),
                "rotation_readiness": monitoring_data.get("rotation_handling", {}).get("rotation_efficiency", 0),
                "configuration_errors": len(monitoring_data.get("issues", []))
            }
            
            # Calculate monitoring score
            scores = [
                analysis["efficiency_metrics"]["coverage_percentage"],
                analysis["efficiency_metrics"]["active_monitoring_rate"],
                analysis["lag_assessment"]["lag_score"],
                analysis["configuration_health"]["rotation_readiness"]
            ]
            analysis["monitoring_score"] = sum(scores) / len(scores)
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def _analyze_log_processing(self, processing_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Analyze log processing statistics."""
        analysis = {
            "parsing_performance": {},
            "rule_effectiveness": {},
            "filtering_efficiency": {},
            "format_analysis": {},
            "processing_score": 85
        }
        
        try:
            parsing = processing_data.get("parsing_efficiency", {})
            rule_matching = processing_data.get("rule_matching", {})
            filtering = processing_data.get("filtering_stats", {})
            
            # Parsing performance
            parsing_rate = parsing.get("parsing_success_rate", 0)
            analysis["parsing_performance"] = {
                "success_rate": parsing_rate,
                "performance_level": "excellent" if parsing_rate >= 95 else
                                   "good" if parsing_rate >= 85 else
                                   "fair" if parsing_rate >= 70 else "poor",
                "failed_parsing_rate": 100 - parsing_rate,
                "processing_rate": parsing.get("average_processing_rate", 0)
            }
            
            # Rule effectiveness
            analysis["rule_effectiveness"] = {
                "rule_coverage": rule_matching.get("rule_coverage", 0),
                "unique_rules_triggered": rule_matching.get("unique_rules_triggered", 0),
                "rule_efficiency": rule_matching.get("average_matches_per_rule", 0),
                "top_rules": rule_matching.get("most_frequent_rules", [])[:5]
            }
            
            # Filtering efficiency
            filtering_ratio = filtering.get("filtering_ratio", 0)
            analysis["filtering_efficiency"] = {
                "noise_reduction": filtering.get("noise_reduction", 0),
                "signal_quality": filtering_ratio,
                "filtering_effectiveness": "excellent" if filtering_ratio >= 20 else
                                        "good" if filtering_ratio >= 10 else
                                        "fair" if filtering_ratio >= 5 else "poor"
            }
            
            # Format analysis
            format_dist = processing_data.get("format_distribution", {})
            analysis["format_analysis"] = {
                "format_diversity": len(format_dist),
                "most_common_format": max(format_dist.keys(), key=lambda k: format_dist[k]) if format_dist else "unknown",
                "format_distribution": format_dist
            }
            
            # Calculate processing score
            scores = [
                parsing_rate,
                rule_matching.get("rule_coverage", 0),
                min(100, filtering_ratio * 5),  # Scale filtering ratio
                min(100, len(format_dist) * 10)  # Reward format diversity
            ]
            analysis["processing_score"] = sum(scores) / len(scores)
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def _analyze_log_collector_errors(self, error_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze log collector error statistics."""
        analysis = {
            "error_summary": {},
            "error_impact": {},
            "critical_issues": [],
            "resolution_priority": [],
            "error_score": 85
        }
        
        try:
            total_errors = error_data.get("total_errors", 0)
            error_categories = error_data.get("error_categories", {})
            critical_errors = error_data.get("critical_errors", [])
            impact = error_data.get("impact_analysis", {})
            
            # Error summary
            analysis["error_summary"] = {
                "total_errors": total_errors,
                "error_rate": impact.get("error_rate_percentage", 0),
                "critical_error_count": len(critical_errors),
                "most_common_error": max(error_categories.keys(), key=lambda k: error_categories[k]) if error_categories else "none"
            }
            
            # Error impact
            analysis["error_impact"] = {
                "service_impact": impact.get("service_availability_impact", 0),
                "critical_error_rate": impact.get("critical_error_rate", 0),
                "operational_impact": "high" if total_errors > 20 else
                                    "medium" if total_errors > 10 else
                                    "low" if total_errors > 0 else "none"
            }
            
            # Critical issues
            analysis["critical_issues"] = critical_errors[:5]  # Top 5 critical issues
            
            # Resolution priority
            for error_type, count in sorted(error_categories.items(), key=lambda x: x[1], reverse=True):
                priority = "high" if count > 10 else "medium" if count > 5 else "low"
                analysis["resolution_priority"].append({
                    "error_type": error_type,
                    "count": count,
                    "priority": priority
                })
            
            # Calculate error score (lower errors = higher score)
            error_penalty = min(50, total_errors * 2)
            critical_penalty = min(30, len(critical_errors) * 5)
            analysis["error_score"] = max(0, 100 - error_penalty - critical_penalty)
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def _calculate_node_log_collector_health(self, node_stats: Dict[str, Any]) -> float:
        """Calculate overall health score for a node's log collector."""
        if "collection_error" in node_stats:
            return 0.0
        
        try:
            # Get individual component scores
            performance_score = node_stats.get("performance", {}).get("performance_metrics", {}).get("processing_efficiency_score", 85)
            monitoring_score = 85  # Would be calculated from monitoring efficiency
            processing_score = 85  # Would be calculated from processing efficiency
            error_score = 100 - min(50, node_stats.get("errors", {}).get("total_errors", 0) * 2)
            efficiency_score = node_stats.get("efficiency", {}).get("performance_score", 85)
            
            # Weight the scores
            weights = {
                "performance": 0.25,
                "monitoring": 0.20,
                "processing": 0.25,
                "errors": 0.15,
                "efficiency": 0.15
            }
            
            weighted_score = (
                performance_score * weights["performance"] +
                monitoring_score * weights["monitoring"] +
                processing_score * weights["processing"] +
                error_score * weights["errors"] +
                efficiency_score * weights["efficiency"]
            )
            
            return min(100.0, max(0.0, weighted_score))
            
        except Exception as e:
            self.logger.warning(f"Error calculating node log collector health: {str(e)}")
            return 50.0  # Default moderate score
    
    def _generate_node_log_collector_recommendations(self, node_stats: Dict[str, Any], query) -> List[Dict[str, str]]:
        """Generate recommendations for node log collector optimization."""
        recommendations = []
        
        try:
            # Performance recommendations
            performance = node_stats.get("performance", {})
            bottlenecks = performance.get("bottlenecks", [])
            
            if bottlenecks:
                for bottleneck in bottlenecks:
                    if bottleneck.get("type") == "cpu":
                        recommendations.append({
                            "category": "performance",
                            "priority": "high",
                            "title": "High CPU Usage Detected",
                            "description": f"CPU usage is {bottleneck.get('value', 0)}% which may impact log processing",
                            "action": "Consider optimizing log collection configuration or upgrading hardware",
                            "impact": "Improved log processing throughput"
                        })
            
            # File monitoring recommendations
            monitoring = node_stats.get("file_monitoring", {})
            lag_issues = monitoring.get("lag_analysis", {}).get("potential_lag_issues", 0)
            
            if lag_issues > 0:
                recommendations.append({
                    "category": "monitoring",
                    "priority": "medium",
                    "title": "File Monitoring Lag Detected",
                    "description": f"Found {lag_issues} potential file monitoring lag issues",
                    "action": "Review file rotation policies and monitoring configuration",
                    "impact": "Reduced log collection delays"
                })
            
            # Error recommendations
            errors = node_stats.get("errors", {})
            error_categories = errors.get("error_categories", {})
            
            if error_categories.get("permission_error", 0) > 0:
                recommendations.append({
                    "category": "configuration",
                    "priority": "high",
                    "title": "Permission Errors Detected",
                    "description": "Log files have permission issues preventing collection",
                    "action": "Fix file permissions: chmod 644 logfiles && chown wazuh:wazuh logfiles",
                    "impact": "Restored log collection from affected files"
                })
            
            # Efficiency recommendations
            efficiency = node_stats.get("efficiency", {})
            if efficiency.get("performance_score", 85) < 80:
                recommendations.append({
                    "category": "optimization",
                    "priority": "medium",
                    "title": "Log Collection Efficiency Below Optimal",
                    "description": "Log collection efficiency can be improved",
                    "action": "Review configuration for unused rules and optimize log parsing",
                    "impact": "Enhanced overall log collection performance"
                })
            
        except Exception as e:
            self.logger.warning(f"Error generating node recommendations: {str(e)}")
        
        return recommendations
    
    def _analyze_agent_log_performance(self, agent_stats: Dict[str, Any], query) -> Dict[str, Any]:
        """Analyze agent log collection performance."""
        analysis = {
            "coverage_analysis": {},
            "performance_distribution": {},
            "connectivity_health": {},
            "top_performers": [],
            "agents_needing_attention": []
        }
        
        try:
            total_agents = agent_stats.get("total_agents", 0)
            agents_with_logs = agent_stats.get("agents_with_logs", 0)
            agent_performance = agent_stats.get("agent_performance", {})
            connectivity_issues = agent_stats.get("connectivity_issues", [])
            
            # Coverage analysis
            analysis["coverage_analysis"] = {
                "total_agents": total_agents,
                "agents_with_logs": agents_with_logs,
                "coverage_percentage": (agents_with_logs / max(total_agents, 1)) * 100,
                "inactive_agents": len(connectivity_issues),
                "coverage_health": "excellent" if agents_with_logs / max(total_agents, 1) >= 0.9 else
                                "good" if agents_with_logs / max(total_agents, 1) >= 0.7 else
                                "fair" if agents_with_logs / max(total_agents, 1) >= 0.5 else "poor"
            }
            
            # Performance distribution
            if agent_performance:
                log_rates = [perf.get("logs_per_hour", 0) for perf in agent_performance.values()]
                analysis["performance_distribution"] = {
                    "average_logs_per_hour": sum(log_rates) / len(log_rates),
                    "max_logs_per_hour": max(log_rates),
                    "min_logs_per_hour": min(log_rates),
                    "performance_variance": max(log_rates) - min(log_rates)
                }
                
                # Top performers
                sorted_agents = sorted(agent_performance.items(), 
                                     key=lambda x: x[1].get("logs_per_hour", 0), reverse=True)
                analysis["top_performers"] = [
                    {
                        "agent_id": agent_id,
                        "agent_name": perf.get("name", agent_id),
                        "logs_per_hour": perf.get("logs_per_hour", 0),
                        "status": perf.get("status", "unknown")
                    }
                    for agent_id, perf in sorted_agents[:5]
                ]
                
                # Agents needing attention (low performance or issues)
                for agent_id, perf in agent_performance.items():
                    if perf.get("logs_per_hour", 0) < 10 or perf.get("status") != "active":
                        analysis["agents_needing_attention"].append({
                            "agent_id": agent_id,
                            "agent_name": perf.get("name", agent_id),
                            "logs_per_hour": perf.get("logs_per_hour", 0),
                            "status": perf.get("status", "unknown"),
                            "issue": "low_activity" if perf.get("logs_per_hour", 0) < 10 else "connectivity"
                        })
            
            # Connectivity health
            analysis["connectivity_health"] = {
                "total_connectivity_issues": len(connectivity_issues),
                "connectivity_rate": ((total_agents - len(connectivity_issues)) / max(total_agents, 1)) * 100,
                "agents_offline": [issue for issue in connectivity_issues if issue.get("status") == "disconnected"],
                "agents_never_connected": [issue for issue in connectivity_issues if issue.get("status") == "never_connected"]
            }
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def _generate_log_collector_insights(self, stats_data: Dict[str, Any], 
                                       node_analysis: Dict[str, Any], 
                                       agent_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate insights from log collector analysis."""
        insights = []
        
        try:
            global_stats = stats_data.get("global_stats", {})
            performance_summary = global_stats.get("performance_summary", {})
            
            # Performance insights
            avg_processing_rate = performance_summary.get("average_processing_rate", 0)
            if avg_processing_rate > 1000:
                insights.append({
                    "category": "performance",
                    "priority": "info",
                    "title": "High Log Processing Performance",
                    "description": f"Average processing rate of {avg_processing_rate:.1f} logs/second indicates excellent performance",
                    "impact": "positive"
                })
            elif avg_processing_rate < 100:
                insights.append({
                    "category": "performance",
                    "priority": "warning",
                    "title": "Low Log Processing Rate",
                    "description": f"Average processing rate of {avg_processing_rate:.1f} logs/second is below optimal",
                    "impact": "negative"
                })
            
            # Coverage insights
            agent_coverage = performance_summary.get("agent_coverage_percentage", 0)
            if agent_coverage > 95:
                insights.append({
                    "category": "coverage",
                    "priority": "info",
                    "title": "Excellent Agent Coverage",
                    "description": f"{agent_coverage:.1f}% of agents are actively sending logs",
                    "impact": "positive"
                })
            elif agent_coverage < 70:
                insights.append({
                    "category": "coverage",
                    "priority": "warning",
                    "title": "Low Agent Coverage",
                    "description": f"Only {agent_coverage:.1f}% of agents are sending logs",
                    "impact": "negative"
                })
            
            # Error insights
            error_rate = performance_summary.get("error_rate_percentage", 0)
            if error_rate > 10:
                insights.append({
                    "category": "errors",
                    "priority": "critical",
                    "title": "High Error Rate Detected",
                    "description": f"Error rate of {error_rate:.1f}% indicates significant collection issues",
                    "impact": "negative"
                })
            elif error_rate < 1:
                insights.append({
                    "category": "errors",
                    "priority": "info",
                    "title": "Low Error Rate",
                    "description": f"Error rate of {error_rate:.1f}% indicates stable log collection",
                    "impact": "positive"
                })
            
            # Node-specific insights
            for node_name, analysis in node_analysis.items():
                health_score = analysis.get("health_score", 0)
                if health_score < 70:
                    insights.append({
                        "category": "node_health",
                        "priority": "warning",
                        "title": f"Node {node_name} Needs Attention",
                        "description": f"Health score of {health_score:.1f} indicates performance issues",
                        "impact": "negative"
                    })
            
            # Agent insights
            connectivity_health = agent_analysis.get("connectivity_health", {})
            connectivity_rate = connectivity_health.get("connectivity_rate", 0)
            if connectivity_rate < 80:
                insights.append({
                    "category": "connectivity",
                    "priority": "warning",
                    "title": "Agent Connectivity Issues",
                    "description": f"Only {connectivity_rate:.1f}% of agents are properly connected",
                    "impact": "negative"
                })
            
        except Exception as e:
            self.logger.warning(f"Error generating insights: {str(e)}")
        
        return insights
    
    def _generate_log_collector_optimization_recommendations(self, stats_data: Dict[str, Any], 
                                                           summary: Dict[str, Any], 
                                                           query) -> List[Dict[str, str]]:
        """Generate optimization recommendations for log collector."""
        recommendations = []
        
        try:
            # Performance optimization
            processing_rate = summary.get("processing_rate", 0)
            if processing_rate < query.threshold_processing_rate:
                recommendations.append({
                    "category": "performance",
                    "priority": "high",
                    "title": "Optimize Log Processing Rate",
                    "description": f"Current rate {processing_rate:.1f} is below threshold {query.threshold_processing_rate}",
                    "action": "Review log collection rules, increase buffer sizes, or upgrade hardware",
                    "impact": "Improved log processing throughput and reduced latency"
                })
            
            # Coverage optimization
            agent_coverage = summary.get("agent_coverage_percentage", 0)
            if agent_coverage < 85:
                recommendations.append({
                    "category": "coverage",
                    "priority": "medium",
                    "title": "Improve Agent Coverage",
                    "description": f"Only {agent_coverage:.1f}% of agents are actively sending logs",
                    "action": "Review agent configurations and connectivity issues",
                    "impact": "Enhanced visibility across infrastructure"
                })
            
            # Error rate optimization
            error_rate = summary.get("error_rate_percentage", 0)
            if error_rate > query.threshold_error_rate:
                recommendations.append({
                    "category": "reliability",
                    "priority": "high",
                    "title": "Reduce Error Rate",
                    "description": f"Error rate {error_rate:.1f}% exceeds threshold {query.threshold_error_rate}%",
                    "action": "Address permission issues, file paths, and configuration errors",
                    "impact": "More reliable log collection and reduced data loss"
                })
            
            # Resource optimization
            global_stats = stats_data.get("global_stats", {})
            files_monitored = global_stats.get("aggregated_metrics", {}).get("total_files_monitored", 0)
            total_agents = global_stats.get("aggregated_metrics", {}).get("total_agents", 1)
            
            if files_monitored / total_agents > 20:  # High file-to-agent ratio
                recommendations.append({
                    "category": "efficiency",
                    "priority": "medium",
                    "title": "Optimize File Monitoring",
                    "description": f"High file-to-agent ratio ({files_monitored/total_agents:.1f}) may impact performance",
                    "action": "Consolidate log files or implement log rotation policies",
                    "impact": "Reduced resource usage and improved monitoring efficiency"
                })
            
            # Configuration optimization
            if summary.get("collection_errors", 0) > 0:
                recommendations.append({
                    "category": "configuration",
                    "priority": "medium",
                    "title": "Fix Configuration Issues",
                    "description": f"Found {summary.get('collection_errors', 0)} configuration-related errors",
                    "action": "Review and correct agent configurations and file paths",
                    "impact": "Eliminated collection errors and improved data quality"
                })
            
        except Exception as e:
            self.logger.warning(f"Error generating optimization recommendations: {str(e)}")
        
        return recommendations
    
    def _generate_log_collector_alerts(self, stats_data: Dict[str, Any], 
                                     node_analysis: Dict[str, Any], 
                                     query) -> List[Dict[str, str]]:
        """Generate alerts based on log collector thresholds."""
        alerts = []
        
        try:
            # Performance alerts
            global_stats = stats_data.get("global_stats", {})
            performance_summary = global_stats.get("performance_summary", {})
            
            avg_processing_rate = performance_summary.get("average_processing_rate", 0)
            if avg_processing_rate < query.threshold_processing_rate:
                alerts.append({
                    "type": "performance",
                    "severity": "warning",
                    "message": f"Log processing rate {avg_processing_rate:.1f} below threshold {query.threshold_processing_rate}",
                    "node": "global",
                    "metric": "processing_rate",
                    "value": avg_processing_rate,
                    "threshold": query.threshold_processing_rate
                })
            
            # Error rate alerts
            error_rate = performance_summary.get("error_rate_percentage", 0)
            if error_rate > query.threshold_error_rate:
                alerts.append({
                    "type": "error_rate",
                    "severity": "critical" if error_rate > query.threshold_error_rate * 2 else "warning",
                    "message": f"Error rate {error_rate:.1f}% exceeds threshold {query.threshold_error_rate}%",
                    "node": "global",
                    "metric": "error_rate",
                    "value": error_rate,
                    "threshold": query.threshold_error_rate
                })
            
            # Node-specific alerts
            for node_name, analysis in node_analysis.items():
                # Health score alerts
                health_score = analysis.get("health_score", 100)
                if health_score < 70:
                    alerts.append({
                        "type": "node_health",
                        "severity": "critical" if health_score < 50 else "warning",
                        "message": f"Node {node_name} health score {health_score:.1f} is low",
                        "node": node_name,
                        "metric": "health_score",
                        "value": health_score,
                        "threshold": 70
                    })
                
                # File lag alerts
                file_analysis = analysis.get("file_monitoring_analysis", {})
                lag_issues = file_analysis.get("lag_assessment", {}).get("lag_issues_count", 0)
                if lag_issues > 0:
                    alerts.append({
                        "type": "file_lag",
                        "severity": "warning",
                        "message": f"Node {node_name} has {lag_issues} file monitoring lag issues",
                        "node": node_name,
                        "metric": "lag_issues",
                        "value": lag_issues,
                        "threshold": 0
                    })
            
        except Exception as e:
            self.logger.warning(f"Error generating alerts: {str(e)}")
        
        return alerts
    
    def _analyze_log_collector_trends(self, stats_data: Dict[str, Any], time_range: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze trends in log collector performance."""
        trends = {
            "processing_trend": "stable",
            "error_trend": "stable",
            "coverage_trend": "stable",
            "performance_direction": "stable",
            "trend_analysis": "Limited historical data for trend analysis"
        }
        
        # Note: This is a basic implementation
        # In a real scenario, this would analyze historical data over time
        try:
            global_stats = stats_data.get("global_stats", {})
            health_overview = global_stats.get("health_overview", {})
            
            # Mock trend analysis based on current health
            health_score = health_overview.get("overall_health_score", 85)
            
            if health_score > 90:
                trends["performance_direction"] = "improving"
            elif health_score < 70:
                trends["performance_direction"] = "declining"
            else:
                trends["performance_direction"] = "stable"
            
            trends["trend_analysis"] = f"Current health score of {health_score:.1f} indicates {trends['performance_direction']} performance"
            
        except Exception as e:
            trends["error"] = str(e)
        
        return trends
    
    def _format_log_collector_summary_output(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Format analysis for summary output."""
        return {
            "summary": analysis["summary"],
            "key_metrics": {
                "processing_rate": analysis["summary"]["processing_rate"],
                "agent_coverage": analysis["summary"]["agent_coverage_percentage"],
                "error_rate": analysis["summary"]["error_rate_percentage"],
                "health_score": analysis["summary"]["overall_health_score"]
            },
            "top_insights": analysis["performance_insights"][:3],
            "priority_recommendations": [r for r in analysis["optimization_recommendations"] if r.get("priority") == "high"][:3],
            "critical_alerts": [a for a in analysis["alerts"] if a.get("severity") == "critical"],
            "analysis_metadata": analysis["analysis_metadata"]
        }
    
    def _format_log_collector_minimal_output(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Format analysis for minimal output."""
        return {
            "status": analysis["summary"]["health_status"],
            "health_score": analysis["summary"]["overall_health_score"],
            "agents_monitored": analysis["summary"]["agents_with_logs"],
            "processing_rate": analysis["summary"]["processing_rate"],
            "errors": analysis["summary"]["error_count"],
            "alerts": len([a for a in analysis["alerts"] if a.get("severity") in ["critical", "warning"]]),
            "timestamp": analysis["analysis_metadata"]["timestamp"]
        }
    
    async def _collect_coverage_analysis_stats(self, node_name: str, time_range: Dict[str, Any], query) -> Dict[str, Any]:
        """Collect coverage analysis statistics with compliance mapping."""
        coverage_data = {
            "log_coverage": {},
            "compliance_mapping": {},
            "coverage_gaps": [],
            "recommendations": [],
            "coverage_score": 0
        }
        
        try:
            # Get agents and their configurations
            agents_response = await self.api_client.get_agents()
            agents = agents_response.get("data", {}).get("affected_items", [])
            
            if query.agent_filter:
                agents = [agent for agent in agents if 
                         agent.get("id") in query.agent_filter or 
                         agent.get("name") in query.agent_filter]
            
            total_coverage_items = 0
            covered_items = 0
            compliance_gaps = {}
            
            # Analyze log coverage per agent
            for agent in agents:
                agent_id = agent.get("id")
                try:
                    config_response = await self.api_client.get_agent_config(agent_id, "logcollector")
                    config_data = config_response.get("data", {})
                    
                    localfiles = config_data.get("logcollector", {}).get("localfile", [])
                    if not isinstance(localfiles, list):
                        localfiles = [localfiles] if localfiles else []
                    
                    total_coverage_items += 10  # Expected coverage items per agent
                    covered_items += min(len(localfiles), 10)
                    
                except Exception:
                    continue
            
            coverage_data["log_coverage"] = {
                "total_agents": len(agents),
                "agents_with_coverage": sum(1 for a in agents if a.get("status") == "active"),
                "coverage_percentage": (covered_items / max(total_coverage_items, 1)) * 100,
                "missing_coverage_items": total_coverage_items - covered_items
            }
            
            # Compliance framework mapping
            if query.compliance_frameworks:
                for framework in query.compliance_frameworks:
                    framework_lower = framework.lower()
                    
                    if framework_lower in ["pci", "pci-dss"]:
                        coverage_data["compliance_mapping"][framework] = {
                            "required_logs": ["auth", "access", "firewall", "database"],
                            "coverage_status": "partial",
                            "missing_logs": ["database"],
                            "compliance_score": 75.0
                        }
                    elif framework_lower in ["sox", "sarbanes-oxley"]:
                        coverage_data["compliance_mapping"][framework] = {
                            "required_logs": ["audit", "access", "financial_app", "database"],
                            "coverage_status": "good",
                            "missing_logs": [],
                            "compliance_score": 90.0
                        }
                    elif framework_lower == "hipaa":
                        coverage_data["compliance_mapping"][framework] = {
                            "required_logs": ["access", "audit", "medical_app", "database"],
                            "coverage_status": "partial",
                            "missing_logs": ["medical_app"],
                            "compliance_score": 70.0
                        }
            
            # Identify coverage gaps
            if coverage_data["log_coverage"]["coverage_percentage"] < query.coverage_threshold:
                coverage_data["coverage_gaps"].append({
                    "type": "insufficient_coverage",
                    "severity": "high",
                    "description": f"Coverage {coverage_data['log_coverage']['coverage_percentage']:.1f}% below threshold {query.coverage_threshold}%",
                    "agents_affected": coverage_data["log_coverage"]["total_agents"]
                })
            
            # Generate recommendations
            if coverage_data["log_coverage"]["coverage_percentage"] < 90:
                coverage_data["recommendations"].append({
                    "priority": "high",
                    "action": "expand_log_collection",
                    "description": "Add log collection for critical system files",
                    "expected_improvement": "15-20% coverage increase"
                })
            
            coverage_data["coverage_score"] = coverage_data["log_coverage"]["coverage_percentage"]
            
        except Exception as e:
            self.logger.warning(f"Error collecting coverage analysis for {node_name}: {str(e)}")
            coverage_data["error"] = str(e)
        
        return coverage_data
    
    async def _collect_resource_monitoring_stats(self, node_name: str, time_range: Dict[str, Any], query) -> Dict[str, Any]:
        """Collect resource usage tracking and monitoring statistics."""
        resource_data = {
            "cpu_monitoring": {},
            "memory_monitoring": {},
            "disk_monitoring": {},
            "network_monitoring": {},
            "resource_alerts": [],
            "optimization_suggestions": []
        }
        
        try:
            # Get system resource statistics
            system_stats = await self._get_system_stats(node_name)
            
            # CPU monitoring
            cpu_usage = system_stats.get("cpu_percent", 0)
            resource_data["cpu_monitoring"] = {
                "current_usage": cpu_usage,
                "threshold": query.threshold_resource_usage,
                "status": "normal" if cpu_usage < query.threshold_resource_usage else "warning",
                "trend": "stable",  # Would be calculated from historical data
                "peak_usage": cpu_usage * 1.2  # Mock peak calculation
            }
            
            # Memory monitoring
            memory_usage = system_stats.get("memory_percent", 0)
            resource_data["memory_monitoring"] = {
                "current_usage": memory_usage,
                "threshold": query.threshold_resource_usage,
                "status": "normal" if memory_usage < query.threshold_resource_usage else "warning",
                "available_mb": system_stats.get("memory_available_mb", 0),
                "used_mb": system_stats.get("memory_used_mb", 0)
            }
            
            # Disk monitoring
            disk_usage = system_stats.get("disk_percent", 0)
            resource_data["disk_monitoring"] = {
                "current_usage": disk_usage,
                "threshold": 90.0,  # Higher threshold for disk
                "status": "normal" if disk_usage < 90 else "warning",
                "free_gb": system_stats.get("disk_free_gb", 0),
                "io_rate": system_stats.get("disk_io_rate", 0)
            }
            
            # Network monitoring
            resource_data["network_monitoring"] = {
                "bytes_sent": system_stats.get("network_bytes_sent", 0),
                "bytes_received": system_stats.get("network_bytes_received", 0),
                "packets_sent": system_stats.get("network_packets_sent", 0),
                "packets_received": system_stats.get("network_packets_received", 0),
                "connection_count": system_stats.get("connection_count", 0)
            }
            
            # Generate resource alerts
            if cpu_usage > query.threshold_resource_usage:
                resource_data["resource_alerts"].append({
                    "type": "high_cpu",
                    "severity": "warning",
                    "message": f"CPU usage {cpu_usage:.1f}% exceeds threshold {query.threshold_resource_usage}%",
                    "node": node_name
                })
            
            if memory_usage > query.threshold_resource_usage:
                resource_data["resource_alerts"].append({
                    "type": "high_memory",
                    "severity": "warning", 
                    "message": f"Memory usage {memory_usage:.1f}% exceeds threshold {query.threshold_resource_usage}%",
                    "node": node_name
                })
            
            # Generate optimization suggestions
            if cpu_usage > 70:
                resource_data["optimization_suggestions"].append({
                    "category": "cpu_optimization",
                    "suggestion": "Consider optimizing log parsing rules or increasing processing threads",
                    "potential_impact": "10-15% CPU reduction"
                })
            
            if memory_usage > 80:
                resource_data["optimization_suggestions"].append({
                    "category": "memory_optimization",
                    "suggestion": "Implement log rotation or increase memory buffer sizes",
                    "potential_impact": "20-25% memory reduction"
                })
            
        except Exception as e:
            self.logger.warning(f"Error collecting resource monitoring for {node_name}: {str(e)}")
            resource_data["error"] = str(e)
        
        return resource_data
    
    async def _collect_bottleneck_detection_stats(self, node_name: str, time_range: Dict[str, Any], query) -> Dict[str, Any]:
        """Collect bottleneck detection and optimization statistics."""
        bottleneck_data = {
            "identified_bottlenecks": [],
            "performance_analysis": {},
            "optimization_hints": [],
            "bottleneck_score": 0
        }
        
        try:
            # Get performance metrics
            system_stats = await self._get_system_stats(node_name)
            
            # Analyze potential bottlenecks
            bottlenecks = []
            
            # CPU bottleneck analysis
            cpu_usage = system_stats.get("cpu_percent", 0)
            if cpu_usage > 85:
                bottlenecks.append({
                    "type": "cpu_bottleneck",
                    "severity": "high",
                    "metric": "cpu_usage",
                    "current_value": cpu_usage,
                    "threshold": 85,
                    "impact": "Log processing delays",
                    "recommendation": "Optimize parsing rules or add CPU cores"
                })
            
            # Memory bottleneck analysis
            memory_usage = system_stats.get("memory_percent", 0)
            if memory_usage > 90:
                bottlenecks.append({
                    "type": "memory_bottleneck",
                    "severity": "critical",
                    "metric": "memory_usage",
                    "current_value": memory_usage,
                    "threshold": 90,
                    "impact": "Risk of log loss or system instability",
                    "recommendation": "Increase memory or implement aggressive log rotation"
                })
            
            # Disk I/O bottleneck analysis
            disk_io = system_stats.get("disk_io_rate", 0)
            if disk_io > 80:  # Mock threshold
                bottlenecks.append({
                    "type": "disk_io_bottleneck",
                    "severity": "medium",
                    "metric": "disk_io_rate",
                    "current_value": disk_io,
                    "threshold": 80,
                    "impact": "Slow log file reading",
                    "recommendation": "Use faster storage or implement read caching"
                })
            
            bottleneck_data["identified_bottlenecks"] = bottlenecks
            
            # Performance analysis
            bottleneck_data["performance_analysis"] = {
                "processing_efficiency": max(0, 100 - cpu_usage),
                "memory_efficiency": max(0, 100 - memory_usage),
                "io_efficiency": max(0, 100 - disk_io),
                "overall_efficiency": max(0, (300 - cpu_usage - memory_usage - disk_io) / 3)
            }
            
            # Generate optimization hints
            if cpu_usage > 70:
                bottleneck_data["optimization_hints"].append({
                    "category": "processing",
                    "hint": "Consider implementing parallel log processing",
                    "complexity": "medium",
                    "estimated_improvement": "20-30%"
                })
            
            if len(bottlenecks) > 0:
                bottleneck_data["optimization_hints"].append({
                    "category": "infrastructure",
                    "hint": "Review hardware specifications and consider scaling",
                    "complexity": "high",
                    "estimated_improvement": "40-50%"
                })
            
            # Calculate bottleneck score (lower is better)
            bottleneck_data["bottleneck_score"] = len(bottlenecks) * 25 + max(0, cpu_usage - 50) + max(0, memory_usage - 50)
            
        except Exception as e:
            self.logger.warning(f"Error collecting bottleneck detection for {node_name}: {str(e)}")
            bottleneck_data["error"] = str(e)
        
        return bottleneck_data
    
    async def _collect_capacity_planning_stats(self, node_name: str, time_range: Dict[str, Any], query) -> Dict[str, Any]:
        """Collect capacity planning metrics and scaling recommendations."""
        capacity_data = {
            "current_capacity": {},
            "utilization_trends": {},
            "scaling_recommendations": [],
            "growth_projections": {},
            "capacity_alerts": []
        }
        
        try:
            # Get current system capacity metrics
            system_stats = await self._get_system_stats(node_name)
            
            # Current capacity assessment
            capacity_data["current_capacity"] = {
                "cpu_cores": system_stats.get("cpu_cores", 1),
                "memory_total_gb": system_stats.get("memory_total_gb", 1),
                "disk_total_gb": system_stats.get("disk_total_gb", 100),
                "max_concurrent_logs": system_stats.get("max_concurrent_logs", 1000),
                "current_log_rate": system_stats.get("current_log_rate", 100)
            }
            
            # Utilization trends analysis
            cpu_usage = system_stats.get("cpu_percent", 0)
            memory_usage = system_stats.get("memory_percent", 0)
            disk_usage = system_stats.get("disk_percent", 0)
            
            capacity_data["utilization_trends"] = {
                "cpu_trend": "increasing" if cpu_usage > 70 else "stable",
                "memory_trend": "increasing" if memory_usage > 80 else "stable",
                "disk_trend": "increasing" if disk_usage > 85 else "stable",
                "log_volume_trend": "growing",  # Would be calculated from historical data
                "peak_utilization_period": "business_hours"
            }
            
            # Growth projections (mock calculations)
            current_log_rate = capacity_data["current_capacity"]["current_log_rate"]
            capacity_data["growth_projections"] = {
                "30_day_projection": {
                    "expected_log_rate": current_log_rate * 1.1,
                    "cpu_requirement": cpu_usage * 1.1,
                    "memory_requirement": memory_usage * 1.05,
                    "disk_requirement": disk_usage * 1.15
                },
                "90_day_projection": {
                    "expected_log_rate": current_log_rate * 1.3,
                    "cpu_requirement": cpu_usage * 1.3,
                    "memory_requirement": memory_usage * 1.15,
                    "disk_requirement": disk_usage * 1.45
                }
            }
            
            # Scaling recommendations
            if cpu_usage > 75:
                capacity_data["scaling_recommendations"].append({
                    "resource": "cpu",
                    "action": "scale_up",
                    "priority": "high",
                    "description": "Add CPU cores or upgrade to faster processors",
                    "timeline": "immediate",
                    "estimated_cost": "medium"
                })
            
            if memory_usage > 85:
                capacity_data["scaling_recommendations"].append({
                    "resource": "memory",
                    "action": "scale_up",
                    "priority": "critical",
                    "description": "Increase memory to prevent log loss",
                    "timeline": "immediate",
                    "estimated_cost": "low"
                })
            
            if disk_usage > 90:
                capacity_data["scaling_recommendations"].append({
                    "resource": "storage",
                    "action": "scale_up",
                    "priority": "critical",
                    "description": "Add storage capacity and implement log archiving",
                    "timeline": "immediate",
                    "estimated_cost": "medium"
                })
            
            # Capacity alerts
            if any(capacity_data["growth_projections"]["30_day_projection"][key] > 90 
                   for key in ["cpu_requirement", "memory_requirement", "disk_requirement"]):
                capacity_data["capacity_alerts"].append({
                    "type": "capacity_warning",
                    "severity": "warning",
                    "message": "Projected resource utilization will exceed safe limits within 30 days",
                    "affected_resources": ["cpu", "memory", "disk"]
                })
            
        except Exception as e:
            self.logger.warning(f"Error collecting capacity planning for {node_name}: {str(e)}")
            capacity_data["error"] = str(e)
        
        return capacity_data
    
    def _aggregate_coverage_analysis(self, stats_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Aggregate coverage analysis data from all nodes."""
        aggregated = {
            "overall_coverage": {},
            "compliance_summary": {},
            "critical_gaps": [],
            "recommendations": []
        }
        
        total_agents = 0
        covered_agents = 0
        compliance_scores = {}
        all_gaps = []
        
        # Aggregate from node stats
        for node_name, node_stats in stats_data.get("node_stats", {}).items():
            coverage_data = node_stats.get("coverage_analysis", {})
            if coverage_data:
                log_coverage = coverage_data.get("log_coverage", {})
                total_agents += log_coverage.get("total_agents", 0)
                covered_agents += log_coverage.get("agents_with_coverage", 0)
                
                # Aggregate compliance scores
                compliance_mapping = coverage_data.get("compliance_mapping", {})
                for framework, data in compliance_mapping.items():
                    if framework not in compliance_scores:
                        compliance_scores[framework] = []
                    compliance_scores[framework].append(data.get("compliance_score", 0))
                
                # Collect gaps
                all_gaps.extend(coverage_data.get("coverage_gaps", []))
        
        aggregated["overall_coverage"] = {
            "total_agents": total_agents,
            "covered_agents": covered_agents,
            "coverage_percentage": (covered_agents / max(total_agents, 1)) * 100,
            "uncovered_agents": total_agents - covered_agents
        }
        
        # Calculate average compliance scores
        for framework, scores in compliance_scores.items():
            aggregated["compliance_summary"][framework] = {
                "average_score": sum(scores) / len(scores) if scores else 0,
                "nodes_assessed": len(scores),
                "status": "compliant" if (sum(scores) / len(scores) if scores else 0) >= 80 else "non_compliant"
            }
        
        aggregated["critical_gaps"] = all_gaps[:10]  # Top 10 critical gaps
        
        return aggregated
    
    def _aggregate_resource_monitoring(self, stats_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Aggregate resource monitoring data from all nodes."""
        aggregated = {
            "cluster_resources": {},
            "resource_alerts": [],
            "optimization_summary": {},
            "health_status": "good"
        }
        
        cpu_usages = []
        memory_usages = []
        disk_usages = []
        all_alerts = []
        
        # Aggregate from node stats
        for node_name, node_stats in stats_data.get("node_stats", {}).items():
            resource_data = node_stats.get("resource_monitoring", {})
            if resource_data:
                cpu_monitoring = resource_data.get("cpu_monitoring", {})
                memory_monitoring = resource_data.get("memory_monitoring", {})
                disk_monitoring = resource_data.get("disk_monitoring", {})
                
                cpu_usages.append(cpu_monitoring.get("current_usage", 0))
                memory_usages.append(memory_monitoring.get("current_usage", 0))
                disk_usages.append(disk_monitoring.get("current_usage", 0))
                
                all_alerts.extend(resource_data.get("resource_alerts", []))
        
        aggregated["cluster_resources"] = {
            "average_cpu_usage": sum(cpu_usages) / len(cpu_usages) if cpu_usages else 0,
            "average_memory_usage": sum(memory_usages) / len(memory_usages) if memory_usages else 0,
            "average_disk_usage": sum(disk_usages) / len(disk_usages) if disk_usages else 0,
            "max_cpu_usage": max(cpu_usages) if cpu_usages else 0,
            "max_memory_usage": max(memory_usages) if memory_usages else 0,
            "max_disk_usage": max(disk_usages) if disk_usages else 0,
            "nodes_monitored": len([n for n in stats_data.get("node_stats", {}).values() if n.get("resource_monitoring")])
        }
        
        aggregated["resource_alerts"] = all_alerts
        
        # Determine health status
        avg_cpu = aggregated["cluster_resources"]["average_cpu_usage"]
        avg_memory = aggregated["cluster_resources"]["average_memory_usage"]
        
        if avg_cpu > 90 or avg_memory > 90:
            aggregated["health_status"] = "critical"
        elif avg_cpu > 80 or avg_memory > 80:
            aggregated["health_status"] = "warning"
        else:
            aggregated["health_status"] = "good"
        
        return aggregated
    
    def _aggregate_bottleneck_detection(self, stats_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Aggregate bottleneck detection data from all nodes."""
        aggregated = {
            "cluster_bottlenecks": [],
            "performance_summary": {},
            "optimization_priorities": [],
            "overall_efficiency": 0
        }
        
        all_bottlenecks = []
        efficiency_scores = []
        
        # Aggregate from node stats
        for node_name, node_stats in stats_data.get("node_stats", {}).items():
            bottleneck_data = node_stats.get("bottleneck_detection", {})
            if bottleneck_data:
                bottlenecks = bottleneck_data.get("identified_bottlenecks", [])
                for bottleneck in bottlenecks:
                    bottleneck["node"] = node_name
                    all_bottlenecks.append(bottleneck)
                
                performance = bottleneck_data.get("performance_analysis", {})
                overall_eff = performance.get("overall_efficiency", 0)
                if overall_eff > 0:
                    efficiency_scores.append(overall_eff)
        
        aggregated["cluster_bottlenecks"] = sorted(all_bottlenecks, key=lambda x: x.get("severity", ""), reverse=True)[:10]
        
        aggregated["performance_summary"] = {
            "nodes_with_bottlenecks": len(set(b["node"] for b in all_bottlenecks)),
            "total_bottlenecks": len(all_bottlenecks),
            "critical_bottlenecks": len([b for b in all_bottlenecks if b.get("severity") == "critical"]),
            "high_bottlenecks": len([b for b in all_bottlenecks if b.get("severity") == "high"])
        }
        
        aggregated["overall_efficiency"] = sum(efficiency_scores) / len(efficiency_scores) if efficiency_scores else 100
        
        return aggregated
    
    def _aggregate_capacity_planning(self, stats_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Aggregate capacity planning data from all nodes."""
        aggregated = {
            "cluster_capacity": {},
            "scaling_recommendations": [],
            "growth_projections": {},
            "capacity_alerts": []
        }
        
        all_recommendations = []
        all_alerts = []
        capacity_metrics = []
        
        # Aggregate from node stats
        for node_name, node_stats in stats_data.get("node_stats", {}).items():
            capacity_data = node_stats.get("capacity_planning", {})
            if capacity_data:
                current_capacity = capacity_data.get("current_capacity", {})
                capacity_metrics.append(current_capacity)
                
                recommendations = capacity_data.get("scaling_recommendations", [])
                for rec in recommendations:
                    rec["node"] = node_name
                    all_recommendations.append(rec)
                
                all_alerts.extend(capacity_data.get("capacity_alerts", []))
        
        # Calculate cluster capacity
        if capacity_metrics:
            aggregated["cluster_capacity"] = {
                "total_cpu_cores": sum(m.get("cpu_cores", 0) for m in capacity_metrics),
                "total_memory_gb": sum(m.get("memory_total_gb", 0) for m in capacity_metrics),
                "total_disk_gb": sum(m.get("disk_total_gb", 0) for m in capacity_metrics),
                "total_log_rate": sum(m.get("current_log_rate", 0) for m in capacity_metrics),
                "nodes_assessed": len(capacity_metrics)
            }
        
        # Prioritize scaling recommendations
        priority_order = {"critical": 3, "high": 2, "medium": 1, "low": 0}
        aggregated["scaling_recommendations"] = sorted(
            all_recommendations, 
            key=lambda x: priority_order.get(x.get("priority", "low"), 0), 
            reverse=True
        )[:5]  # Top 5 recommendations
        
        aggregated["capacity_alerts"] = all_alerts
        
        return aggregated
    
    def _aggregate_compliance_mapping(self, stats_data: Dict[str, Any], query) -> Dict[str, Any]:
        """Aggregate compliance mapping data from all nodes."""
        aggregated = {
            "framework_compliance": {},
            "overall_compliance_score": 0,
            "compliance_gaps": [],
            "remediation_plan": []
        }
        
        framework_data = {}
        
        # Aggregate from node stats
        for node_name, node_stats in stats_data.get("node_stats", {}).items():
            coverage_data = node_stats.get("coverage_analysis", {})
            compliance_mapping = coverage_data.get("compliance_mapping", {})
            
            for framework, data in compliance_mapping.items():
                if framework not in framework_data:
                    framework_data[framework] = {
                        "scores": [],
                        "missing_logs": set(),
                        "nodes_assessed": 0
                    }
                
                framework_data[framework]["scores"].append(data.get("compliance_score", 0))
                framework_data[framework]["missing_logs"].update(data.get("missing_logs", []))
                framework_data[framework]["nodes_assessed"] += 1
        
        # Calculate framework compliance
        total_score = 0
        framework_count = 0
        
        for framework, data in framework_data.items():
            avg_score = sum(data["scores"]) / len(data["scores"]) if data["scores"] else 0
            aggregated["framework_compliance"][framework] = {
                "average_score": avg_score,
                "status": "compliant" if avg_score >= 80 else "non_compliant",
                "missing_logs": list(data["missing_logs"]),
                "nodes_assessed": data["nodes_assessed"]
            }
            total_score += avg_score
            framework_count += 1
        
        aggregated["overall_compliance_score"] = total_score / framework_count if framework_count > 0 else 0
        
        return aggregated
    
    def _find_peak_hour(self, hourly_distribution: List[int]) -> int:
        """Find the hour with most activity."""
        if not hourly_distribution:
            return 0
        return hourly_distribution.index(max(hourly_distribution))
    
    def _find_most_active_day(self, daily_distribution: Dict[str, int]) -> str:
        """Find the day with most activity."""
        if not daily_distribution:
            return "N/A"
        return max(daily_distribution, key=daily_distribution.get)
    
    def _calculate_weekly_summary(self, weekly_metrics: Dict) -> Dict[str, Any]:
        """Calculate summary statistics across all weeks."""
        summary = {
            "total_weeks_analyzed": len(weekly_metrics),
            "overall_metrics": {},
            "averages": {},
            "totals": {}
        }
        
        # Initialize counters
        metric_totals = {}
        metric_counts = {}
        
        # Aggregate metrics
        for week_label, week_data in weekly_metrics.items():
            for metric_type, metric_values in week_data.get("metrics", {}).items():
                if metric_type not in metric_totals:
                    metric_totals[metric_type] = {}
                    metric_counts[metric_type] = 0
                
                metric_counts[metric_type] += 1
                
                # Sum up totals
                for key, value in metric_values.items():
                    if isinstance(value, (int, float)) and "percentage" not in key and "rate" not in key:
                        if key not in metric_totals[metric_type]:
                            metric_totals[metric_type][key] = 0
                        metric_totals[metric_type][key] += value
        
        # Calculate averages and totals
        for metric_type, totals in metric_totals.items():
            summary["totals"][metric_type] = totals
            summary["averages"][metric_type] = {
                key: value / metric_counts[metric_type]
                for key, value in totals.items()
            }
        
        return summary
    
    def _analyze_weekly_trends(self, weekly_metrics: Dict) -> Dict[str, Any]:
        """Analyze trends across weeks."""
        trends = {
            "alert_trend": [],
            "vulnerability_trend": [],
            "agent_trend": [],
            "overall_direction": "stable"
        }
        
        # Extract time series data
        weeks = sorted(weekly_metrics.keys())
        
        for week in weeks:
            week_data = weekly_metrics[week]
            
            # Alert trends
            if "alerts" in week_data.get("metrics", {}):
                trends["alert_trend"].append({
                    "week": week,
                    "total": week_data["metrics"]["alerts"].get("total", 0),
                    "critical": week_data["metrics"]["alerts"].get("severity_breakdown", {}).get("critical", 0)
                })
            
            # Vulnerability trends
            if "vulnerabilities" in week_data.get("metrics", {}):
                trends["vulnerability_trend"].append({
                    "week": week,
                    "total": week_data["metrics"]["vulnerabilities"].get("total", 0),
                    "critical": week_data["metrics"]["vulnerabilities"].get("critical", 0)
                })
            
            # Agent trends
            if "agents" in week_data.get("metrics", {}):
                trends["agent_trend"].append({
                    "week": week,
                    "total": week_data["metrics"]["agents"].get("total", 0),
                    "active": week_data["metrics"]["agents"].get("active", 0)
                })
        
        # Determine overall direction
        if len(trends["alert_trend"]) >= 2:
            first_week_alerts = trends["alert_trend"][0]["total"]
            last_week_alerts = trends["alert_trend"][-1]["total"]
            
            if last_week_alerts > first_week_alerts * 1.2:
                trends["overall_direction"] = "increasing"
            elif last_week_alerts < first_week_alerts * 0.8:
                trends["overall_direction"] = "decreasing"
        
        return trends
    
    def _compare_weeks(self, weekly_metrics: Dict) -> Dict[str, Any]:
        """Compare metrics between weeks."""
        comparisons = {
            "week_over_week_changes": [],
            "best_week": {},
            "worst_week": {}
        }
        
        weeks = sorted(weekly_metrics.keys())
        
        # Week-over-week comparisons
        for i in range(1, len(weeks)):
            prev_week = weekly_metrics[weeks[i-1]]
            curr_week = weekly_metrics[weeks[i]]
            
            comparison = {
                "weeks": f"{weeks[i-1]} vs {weeks[i]}",
                "changes": {}
            }
            
            # Compare alerts
            if "alerts" in prev_week.get("metrics", {}) and "alerts" in curr_week.get("metrics", {}):
                prev_total = prev_week["metrics"]["alerts"].get("total", 0)
                curr_total = curr_week["metrics"]["alerts"].get("total", 0)
                
                if prev_total > 0:
                    change_pct = ((curr_total - prev_total) / prev_total) * 100
                    comparison["changes"]["alerts"] = {
                        "previous": prev_total,
                        "current": curr_total,
                        "change_percent": round(change_pct, 2)
                    }
            
            comparisons["week_over_week_changes"].append(comparison)
        
        return comparisons
    
    def _forecast_next_week(self, weekly_metrics: Dict) -> Dict[str, Any]:
        """Basic forecasting for next week based on trends."""
        forecast = {
            "next_week_estimates": {},
            "confidence": "low",
            "method": "simple_moving_average"
        }
        
        # Simple moving average forecast
        weeks = sorted(weekly_metrics.keys())
        if len(weeks) >= 3:
            # Use last 3 weeks for forecast
            recent_weeks = weeks[-3:]
            
            # Calculate averages for each metric
            metric_sums = {}
            metric_counts = {}
            
            for week in recent_weeks:
                week_data = weekly_metrics[week]
                for metric_type, metrics in week_data.get("metrics", {}).items():
                    if metric_type not in metric_sums:
                        metric_sums[metric_type] = {}
                        metric_counts[metric_type] = 0
                    
                    metric_counts[metric_type] += 1
                    
                    for key, value in metrics.items():
                        if isinstance(value, (int, float)) and key == "total":
                            if key not in metric_sums[metric_type]:
                                metric_sums[metric_type][key] = 0
                            metric_sums[metric_type][key] += value
            
            # Calculate forecasts
            for metric_type, sums in metric_sums.items():
                forecast["next_week_estimates"][metric_type] = {
                    key: round(value / metric_counts[metric_type])
                    for key, value in sums.items()
                }
            
            forecast["confidence"] = "medium" if len(weeks) >= 4 else "low"
        
        return forecast
    
    def _generate_weekly_insights(self, summary: Dict, trends: Dict, raw_data: Dict) -> List[Dict]:
        """Generate insights from weekly statistics."""
        insights = []
        
        # Alert insights
        if "alerts" in summary.get("totals", {}):
            total_alerts = summary["totals"]["alerts"].get("total", 0)
            avg_alerts = summary["averages"]["alerts"].get("total", 0)
            
            if total_alerts > 0:
                insights.append({
                    "category": "alerts",
                    "insight": f"Processed {total_alerts} total alerts across {summary['total_weeks_analyzed']} weeks",
                    "detail": f"Average of {round(avg_alerts)} alerts per week",
                    "importance": "high"
                })
        
        # Trend insights
        if trends and "overall_direction" in trends:
            if trends["overall_direction"] == "increasing":
                insights.append({
                    "category": "trends",
                    "insight": "Alert volume is trending upward",
                    "detail": "Consider investigating the cause of increased activity",
                    "importance": "high"
                })
            elif trends["overall_direction"] == "decreasing":
                insights.append({
                    "category": "trends",
                    "insight": "Alert volume is trending downward",
                    "detail": "Security posture may be improving",
                    "importance": "medium"
                })
        
        # Vulnerability insights
        if "vulnerabilities" in summary.get("totals", {}):
            critical_vulns = summary["totals"]["vulnerabilities"].get("critical", 0)
            if critical_vulns > 0:
                insights.append({
                    "category": "vulnerabilities",
                    "insight": f"Detected {critical_vulns} critical vulnerabilities",
                    "detail": "Immediate patching required for critical systems",
                    "importance": "critical"
                })
        
        # Authentication insights
        for week_label, week_data in raw_data.items():
            if "authentication" in week_data:
                auth_data = week_data["authentication"]
                if "suspicious_activity" in auth_data and len(auth_data["suspicious_activity"]) > 0:
                    insights.append({
                        "category": "security",
                        "insight": f"Suspicious authentication activity detected in {week_label}",
                        "detail": f"{len(auth_data['suspicious_activity'])} sources with multiple failed attempts",
                        "importance": "high"
                    })
                    break  # Only report once
        
        # Sort by importance
        importance_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        insights.sort(key=lambda x: importance_order.get(x["importance"], 4))
        
        return insights
    
    def _generate_weekly_recommendations(self, summary: Dict, trends: Dict, insights: List[Dict]) -> List[Dict]:
        """Generate recommendations based on weekly analysis."""
        recommendations = []
        
        # Alert-based recommendations
        if "alerts" in summary.get("averages", {}):
            avg_alerts = summary["averages"]["alerts"].get("total", 0)
            if avg_alerts > 1000:  # High alert volume
                recommendations.append({
                    "priority": "HIGH",
                    "category": "optimization",
                    "title": "Optimize Alert Rules",
                    "description": f"High alert volume detected (avg {round(avg_alerts)}/week)",
                    "action": "Review and tune alert rules to reduce noise",
                    "impact": "Improved signal-to-noise ratio"
                })
        
        # Trend-based recommendations
        if trends and trends.get("overall_direction") == "increasing":
            recommendations.append({
                "priority": "MEDIUM",
                "category": "investigation",
                "title": "Investigate Increasing Alert Trend",
                "description": "Alert volume is trending upward over analyzed period",
                "action": "Analyze root causes of increased security events",
                "impact": "Proactive threat mitigation"
            })
        
        # Vulnerability recommendations
        if "vulnerabilities" in summary.get("totals", {}):
            critical_vulns = summary["totals"]["vulnerabilities"].get("critical", 0)
            if critical_vulns > 0:
                recommendations.append({
                    "priority": "CRITICAL",
                    "category": "patching",
                    "title": "Address Critical Vulnerabilities",
                    "description": f"{critical_vulns} critical vulnerabilities detected",
                    "action": "Prioritize patching of critical vulnerabilities",
                    "impact": "Reduced attack surface"
                })
        
        # Agent health recommendations
        if "agents" in summary.get("averages", {}):
            health_pct = summary["averages"]["agents"].get("health_percentage", 100)
            if health_pct < 90:
                recommendations.append({
                    "priority": "MEDIUM",
                    "category": "infrastructure",
                    "title": "Improve Agent Health",
                    "description": f"Average agent health is {round(health_pct)}%",
                    "action": "Investigate and reconnect disconnected agents",
                    "impact": "Complete security monitoring coverage"
                })
        
        return recommendations
    
    async def run(self):
        """Run the MCP server with robust error handling and logging."""
        api_client_entered = False
        try:
            self.logger.info("Starting Wazuh MCP Server session...")
            await self.api_client.__aenter__()
            api_client_entered = True
            
            self.logger.info(f"Wazuh MCP Server v{__version__} starting...")
            self.logger.info(f"Connecting to Wazuh at {self.config.base_url}")
            
            # Test connection with timeout and better error handling
            try:
                health_data = await asyncio.wait_for(
                    self.api_client.health_check(), 
                    timeout=self.config.request_timeout_seconds
                )
                if health_data.get("status") != "healthy":
                    self.logger.warning(f"Wazuh API health check returned: {health_data}")
                    # Continue anyway - some Wazuh versions may not have health endpoint
                else:
                    self.logger.info("Wazuh API connection verified successfully")
            except asyncio.TimeoutError:
                self.logger.warning("Wazuh API health check timed out, continuing anyway")
            except Exception as e:
                self.logger.warning(f"Wazuh API health check failed: {str(e)}, continuing anyway")
            
            init_options = InitializationOptions(
                server_name="wazuh-mcp",
                server_version=__version__,
                capabilities=self.server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={}
                )
            )
            
            # Use stdio transport for MCP compatibility
            async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
                self.logger.info("MCP server started successfully on stdio transport")
                await self.server.run(
                    read_stream,
                    write_stream,
                    init_options
                )
                
        except Exception as e:
            self.logger.error(f"Server runtime error: {str(e)}")
            raise
        finally:
            self.logger.info("Shutting down Wazuh MCP Server...")
            try:
                if api_client_entered:
                    await self.api_client.__aexit__(None, None, None)
            except Exception as e:
                self.logger.error(f"Error during cleanup: {str(e)}")
            self.logger.info("Server shutdown completed")

    async def _handle_get_wazuh_cluster_health(self, arguments: dict) -> list[types.TextContent]:
        """Handle comprehensive cluster health analysis."""
        start_time = datetime.utcnow()
        
        try:
            # Validate input parameters
            validated_query = validate_cluster_health_query(arguments)
            
            # Collect cluster health data
            health_data = await self._collect_cluster_health_data(validated_query)
            
            # Analyze cluster health
            analysis = await self._analyze_cluster_health(health_data, validated_query, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except Exception as e:
            self.logger.error(f"Error in cluster health analysis: {str(e)}")
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e, execution_time=execution_time)
            )]

    async def _collect_cluster_health_data(self, query: 'ClusterHealthQuery') -> dict:
        """Collect comprehensive cluster health data."""
        health_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "cluster_nodes": [],
            "cluster_status": {},
            "performance_metrics": {},
            "connectivity_status": {},
            "resource_utilization": {},
            "service_status": {},
            "disk_usage": {},
            "network_statistics": {},
            "collection_errors": []
        }
        
        try:
            # Get cluster nodes information
            cluster_response = await self.api_client.get_cluster_nodes()
            health_data["cluster_nodes"] = cluster_response.get("data", [])
            
            # Get cluster status
            try:
                status_response = await self.api_client.get_cluster_status()
                health_data["cluster_status"] = status_response.get("data", {})
            except Exception as e:
                health_data["collection_errors"].append({
                    "component": "cluster_status",
                    "error": str(e)
                })
            
            # Collect performance metrics for each node
            if query.include_performance:
                health_data["performance_metrics"] = await self._collect_node_performance_metrics(
                    health_data["cluster_nodes"]
                )
            
            # Test connectivity between nodes
            if query.include_connectivity:
                health_data["connectivity_status"] = await self._test_cluster_connectivity(
                    health_data["cluster_nodes"], query.connectivity_timeout
                )
            
            # Collect resource utilization
            if query.include_resource_usage:
                health_data["resource_utilization"] = await self._collect_resource_utilization(
                    health_data["cluster_nodes"]
                )
            
            # Check service status
            if query.include_service_status:
                health_data["service_status"] = await self._check_service_status(
                    health_data["cluster_nodes"]
                )
            
            # Analyze disk usage
            if query.include_disk_usage:
                health_data["disk_usage"] = await self._analyze_disk_usage(
                    health_data["cluster_nodes"]
                )
            
            # Collect network statistics
            if query.include_network_stats:
                health_data["network_statistics"] = await self._collect_network_statistics(
                    health_data["cluster_nodes"]
                )
            
        except Exception as e:
            self.logger.error(f"Error collecting cluster health data: {str(e)}")
            health_data["collection_errors"].append({
                "component": "general",
                "error": str(e)
            })
        
        return health_data

    async def _collect_node_performance_metrics(self, nodes: list) -> dict:
        """Collect performance metrics for all nodes."""
        performance_data = {}
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            try:
                # Get node statistics
                node_stats = await self.api_client.get_node_stats(node_name)
                
                # Extract performance metrics
                performance_data[node_name] = {
                    "cpu_usage": self._extract_cpu_usage(node_stats),
                    "memory_usage": self._extract_memory_usage(node_stats),
                    "load_average": self._extract_load_average(node_stats),
                    "uptime": self._extract_uptime(node_stats),
                    "process_count": self._extract_process_count(node_stats),
                    "file_descriptors": self._extract_file_descriptors(node_stats),
                    "network_connections": self._extract_network_connections(node_stats)
                }
                
            except Exception as e:
                performance_data[node_name] = {
                    "error": str(e),
                    "status": "collection_failed"
                }
        
        return performance_data

    async def _test_cluster_connectivity(self, nodes: list, timeout: int) -> dict:
        """Test connectivity between cluster nodes."""
        connectivity_data = {
            "overall_status": "unknown",
            "node_connectivity": {},
            "connection_matrix": {},
            "failed_connections": [],
            "latency_analysis": {}
        }
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            node_ip = node.get("ip", "unknown")
            
            try:
                # Test node responsiveness
                response_time = await self._test_node_connectivity(node_ip, timeout)
                
                connectivity_data["node_connectivity"][node_name] = {
                    "status": "connected" if response_time is not None else "disconnected",
                    "response_time_ms": response_time,
                    "ip_address": node_ip,
                    "node_type": node.get("type", "unknown")
                }
                
                if response_time is None:
                    connectivity_data["failed_connections"].append({
                        "node": node_name,
                        "ip": node_ip,
                        "reason": "Connection timeout"
                    })
                
            except Exception as e:
                connectivity_data["node_connectivity"][node_name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # Determine overall connectivity status
        connected_nodes = sum(1 for conn in connectivity_data["node_connectivity"].values() 
                            if conn.get("status") == "connected")
        total_nodes = len(nodes)
        
        if connected_nodes == total_nodes:
            connectivity_data["overall_status"] = "healthy"
        elif connected_nodes >= total_nodes * 0.5:
            connectivity_data["overall_status"] = "degraded"
        else:
            connectivity_data["overall_status"] = "critical"
        
        return connectivity_data

    async def _collect_resource_utilization(self, nodes: list) -> dict:
        """Collect resource utilization data for all nodes."""
        resource_data = {}
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            try:
                # Get detailed resource metrics
                resource_stats = await self.api_client.get_node_resources(node_name)
                
                resource_data[node_name] = {
                    "cpu": {
                        "usage_percent": self._extract_cpu_usage(resource_stats),
                        "core_count": self._extract_cpu_cores(resource_stats),
                        "load_1m": self._extract_load_1m(resource_stats),
                        "load_5m": self._extract_load_5m(resource_stats),
                        "load_15m": self._extract_load_15m(resource_stats)
                    },
                    "memory": {
                        "usage_percent": self._extract_memory_usage(resource_stats),
                        "total_gb": self._extract_memory_total(resource_stats),
                        "available_gb": self._extract_memory_available(resource_stats),
                        "cached_gb": self._extract_memory_cached(resource_stats),
                        "swap_usage_percent": self._extract_swap_usage(resource_stats)
                    },
                    "disk": {
                        "usage_percent": self._extract_disk_usage(resource_stats),
                        "total_gb": self._extract_disk_total(resource_stats),
                        "available_gb": self._extract_disk_available(resource_stats),
                        "io_read_ops": self._extract_disk_read_ops(resource_stats),
                        "io_write_ops": self._extract_disk_write_ops(resource_stats)
                    },
                    "network": {
                        "bytes_sent": self._extract_network_bytes_sent(resource_stats),
                        "bytes_received": self._extract_network_bytes_received(resource_stats),
                        "packets_sent": self._extract_network_packets_sent(resource_stats),
                        "packets_received": self._extract_network_packets_received(resource_stats),
                        "errors": self._extract_network_errors(resource_stats)
                    }
                }
                
            except Exception as e:
                resource_data[node_name] = {
                    "error": str(e),
                    "status": "collection_failed"
                }
        
        return resource_data

    async def _check_service_status(self, nodes: list) -> dict:
        """Check service status for all cluster nodes."""
        service_data = {}
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            try:
                # Get service status information
                service_stats = await self.api_client.get_node_services(node_name)
                
                service_data[node_name] = {
                    "wazuh_manager": self._extract_service_status(service_stats, "wazuh-manager"),
                    "wazuh_indexer": self._extract_service_status(service_stats, "wazuh-indexer"),
                    "wazuh_dashboard": self._extract_service_status(service_stats, "wazuh-dashboard"),
                    "filebeat": self._extract_service_status(service_stats, "filebeat"),
                    "system_services": self._extract_system_services(service_stats),
                    "daemon_status": self._extract_daemon_status(service_stats)
                }
                
            except Exception as e:
                service_data[node_name] = {
                    "error": str(e),
                    "status": "collection_failed"
                }
        
        return service_data

    async def _analyze_disk_usage(self, nodes: list) -> dict:
        """Analyze disk usage across all nodes."""
        disk_data = {}
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            try:
                # Get disk usage statistics
                disk_stats = await self.api_client.get_node_disk_usage(node_name)
                
                disk_data[node_name] = {
                    "filesystems": self._extract_filesystem_usage(disk_stats),
                    "log_directory": self._extract_log_dir_usage(disk_stats),
                    "data_directory": self._extract_data_dir_usage(disk_stats),
                    "backup_directory": self._extract_backup_dir_usage(disk_stats),
                    "temp_directory": self._extract_temp_dir_usage(disk_stats),
                    "disk_health": self._analyze_disk_health(disk_stats),
                    "growth_trend": self._analyze_disk_growth(disk_stats)
                }
                
            except Exception as e:
                disk_data[node_name] = {
                    "error": str(e),
                    "status": "collection_failed"
                }
        
        return disk_data

    async def _collect_network_statistics(self, nodes: list) -> dict:
        """Collect network statistics for all nodes."""
        network_data = {}
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            try:
                # Get network statistics
                network_stats = await self.api_client.get_node_network_stats(node_name)
                
                network_data[node_name] = {
                    "interfaces": self._extract_network_interfaces(network_stats),
                    "bandwidth_usage": self._extract_bandwidth_usage(network_stats),
                    "connection_counts": self._extract_connection_counts(network_stats),
                    "packet_statistics": self._extract_packet_statistics(network_stats),
                    "error_statistics": self._extract_network_error_stats(network_stats),
                    "latency_metrics": self._extract_latency_metrics(network_stats)
                }
                
            except Exception as e:
                network_data[node_name] = {
                    "error": str(e),
                    "status": "collection_failed"
                }
        
        return network_data

    async def _analyze_cluster_health(self, health_data: dict, query: 'ClusterHealthQuery', start_time: datetime) -> dict:
        """Analyze cluster health data and generate comprehensive report."""
        analysis = {
            "query_parameters": {
                "include_node_details": query.include_node_details,
                "include_performance": query.include_performance,
                "include_connectivity": query.include_connectivity,
                "include_resource_usage": query.include_resource_usage,
                "include_service_status": query.include_service_status,
                "include_disk_usage": query.include_disk_usage,
                "include_network_stats": query.include_network_stats,
                "health_thresholds": {
                    "cpu": query.health_threshold_cpu,
                    "memory": query.health_threshold_memory,
                    "disk": query.health_threshold_disk
                },
                "output_format": query.output_format
            },
            "cluster_overview": self._generate_cluster_overview(health_data),
            "health_summary": self._generate_health_summary(health_data, query),
            "node_analysis": self._generate_node_analysis(health_data, query),
            "performance_analysis": self._generate_performance_analysis(health_data, query),
            "connectivity_analysis": self._generate_connectivity_analysis(health_data),
            "resource_analysis": self._generate_resource_analysis(health_data, query),
            "service_analysis": self._generate_service_analysis(health_data),
            "disk_analysis": self._generate_disk_analysis(health_data, query),
            "network_analysis": self._generate_network_analysis(health_data),
            "health_score": self._calculate_overall_health_score(health_data, query),
            "recommendations": [],
            "analysis_metadata": {
                "timestamp": start_time.isoformat(),
                "processing_time_seconds": (datetime.utcnow() - start_time).total_seconds(),
                "nodes_analyzed": len(health_data.get("cluster_nodes", [])),
                "collection_errors": len(health_data.get("collection_errors", [])),
                "data_completeness": self._calculate_data_completeness(health_data)
            }
        }
        
        # Generate recommendations based on analysis
        if query.include_recommendations:
            analysis["recommendations"] = self._generate_cluster_health_recommendations(
                analysis, health_data, query
            )
        
        # Apply output format filtering
        if query.output_format == "summary":
            analysis = self._apply_summary_format(analysis)
        elif query.output_format == "minimal":
            analysis = self._apply_minimal_format(analysis)
        
        return analysis

    def _generate_cluster_overview(self, health_data: dict) -> dict:
        """Generate cluster overview information."""
        nodes = health_data.get("cluster_nodes", [])
        cluster_status = health_data.get("cluster_status", {})
        
        return {
            "cluster_name": cluster_status.get("cluster", "unknown"),
            "total_nodes": len(nodes),
            "node_types": self._count_node_types(nodes),
            "cluster_mode": cluster_status.get("mode", "unknown"),
            "master_node": self._identify_master_node(nodes),
            "cluster_size": cluster_status.get("nodes", 0),
            "cluster_version": cluster_status.get("version", "unknown"),
            "cluster_uptime": cluster_status.get("uptime", "unknown")
        }

    def _generate_health_summary(self, health_data: dict, query: 'ClusterHealthQuery') -> dict:
        """Generate comprehensive health summary."""
        nodes = health_data.get("cluster_nodes", [])
        connectivity = health_data.get("connectivity_status", {})
        
        connected_nodes = sum(1 for conn in connectivity.get("node_connectivity", {}).values() 
                            if conn.get("status") == "connected")
        
        return {
            "overall_status": self._determine_overall_health_status(health_data, query),
            "node_connectivity": {
                "connected": connected_nodes,
                "total": len(nodes),
                "percentage": round((connected_nodes / len(nodes) * 100) if nodes else 0, 2)
            },
            "critical_issues": self._identify_critical_issues(health_data, query),
            "warning_issues": self._identify_warning_issues(health_data, query),
            "healthy_components": self._count_healthy_components(health_data, query),
            "degraded_components": self._count_degraded_components(health_data, query),
            "failed_components": self._count_failed_components(health_data, query)
        }

    def _calculate_overall_health_score(self, health_data: dict, query: 'ClusterHealthQuery') -> dict:
        """Calculate overall cluster health score."""
        score_components = {
            "connectivity": 0,
            "performance": 0,
            "resources": 0,
            "services": 0,
            "disk": 0,
            "network": 0
        }
        
        weights = {
            "connectivity": 0.25,
            "performance": 0.20,
            "resources": 0.20,
            "services": 0.15,
            "disk": 0.10,
            "network": 0.10
        }
        
        # Calculate connectivity score
        connectivity = health_data.get("connectivity_status", {})
        if connectivity:
            connected_ratio = len([conn for conn in connectivity.get("node_connectivity", {}).values() 
                                 if conn.get("status") == "connected"]) / max(len(connectivity.get("node_connectivity", {})), 1)
            score_components["connectivity"] = connected_ratio * 100
        
        # Calculate performance score
        performance = health_data.get("performance_metrics", {})
        if performance:
            perf_scores = []
            for node_perf in performance.values():
                if isinstance(node_perf, dict) and "cpu_usage" in node_perf:
                    cpu_score = max(0, 100 - node_perf.get("cpu_usage", 0))
                    memory_score = max(0, 100 - node_perf.get("memory_usage", 0))
                    perf_scores.append((cpu_score + memory_score) / 2)
            score_components["performance"] = sum(perf_scores) / max(len(perf_scores), 1)
        
        # Calculate resource utilization score
        resources = health_data.get("resource_utilization", {})
        if resources:
            resource_scores = []
            for node_res in resources.values():
                if isinstance(node_res, dict) and "cpu" in node_res:
                    cpu_score = max(0, 100 - node_res.get("cpu", {}).get("usage_percent", 0))
                    mem_score = max(0, 100 - node_res.get("memory", {}).get("usage_percent", 0))
                    disk_score = max(0, 100 - node_res.get("disk", {}).get("usage_percent", 0))
                    resource_scores.append((cpu_score + mem_score + disk_score) / 3)
            score_components["resources"] = sum(resource_scores) / max(len(resource_scores), 1)
        
        # Calculate service health score
        services = health_data.get("service_status", {})
        if services:
            service_scores = []
            for node_services in services.values():
                if isinstance(node_services, dict):
                    healthy_services = sum(1 for service in node_services.values() 
                                         if isinstance(service, dict) and service.get("status") == "active")
                    total_services = len([s for s in node_services.values() if isinstance(s, dict)])
                    service_scores.append((healthy_services / max(total_services, 1)) * 100)
            score_components["services"] = sum(service_scores) / max(len(service_scores), 1)
        
        # Calculate disk health score
        disk = health_data.get("disk_usage", {})
        if disk:
            disk_scores = []
            for node_disk in disk.values():
                if isinstance(node_disk, dict) and "filesystems" in node_disk:
                    fs_scores = []
                    for fs in node_disk["filesystems"]:
                        if isinstance(fs, dict) and "usage_percent" in fs:
                            fs_scores.append(max(0, 100 - fs["usage_percent"]))
                    disk_scores.append(sum(fs_scores) / max(len(fs_scores), 1))
            score_components["disk"] = sum(disk_scores) / max(len(disk_scores), 1)
        
        # Calculate network health score
        network = health_data.get("network_statistics", {})
        if network:
            network_scores = []
            for node_net in network.values():
                if isinstance(node_net, dict) and "error_statistics" in node_net:
                    # Simple network health based on error rates
                    error_stats = node_net["error_statistics"]
                    if isinstance(error_stats, dict):
                        error_rate = error_stats.get("error_rate", 0)
                        network_scores.append(max(0, 100 - error_rate))
                    else:
                        network_scores.append(100)  # Assume healthy if no error data
            score_components["network"] = sum(network_scores) / max(len(network_scores), 1)
        
        # Calculate weighted overall score
        overall_score = sum(score_components[component] * weights[component] 
                          for component in score_components)
        
        return {
            "overall_score": round(overall_score, 2),
            "score_components": score_components,
            "weights": weights,
            "health_rating": self._get_health_rating(overall_score),
            "score_interpretation": self._interpret_health_score(overall_score)
        }

    def _get_health_rating(self, score: float) -> str:
        """Get health rating based on score."""
        if score >= 90:
            return "Excellent"
        elif score >= 75:
            return "Good"
        elif score >= 60:
            return "Fair"
        elif score >= 40:
            return "Poor"
        else:
            return "Critical"

    def _interpret_health_score(self, score: float) -> str:
        """Interpret health score with actionable insights."""
        if score >= 90:
            return "Cluster is operating at optimal levels with minimal issues."
        elif score >= 75:
            return "Cluster is healthy but may have minor performance or resource concerns."
        elif score >= 60:
            return "Cluster is functional but has noticeable issues that should be addressed."
        elif score >= 40:
            return "Cluster has significant problems that require immediate attention."
        else:
            return "Cluster is in critical condition and requires urgent intervention."

    def _generate_cluster_health_recommendations(self, analysis: dict, health_data: dict, query: 'ClusterHealthQuery') -> list:
        """Generate cluster health recommendations."""
        recommendations = []
        
        health_score = analysis.get("health_score", {}).get("overall_score", 0)
        connectivity = health_data.get("connectivity_status", {})
        
        # Connectivity recommendations
        failed_connections = connectivity.get("failed_connections", [])
        if failed_connections:
            recommendations.append({
                "priority": "HIGH",
                "category": "connectivity",
                "title": "Cluster Connectivity Issues",
                "description": f"Found {len(failed_connections)} failed node connections",
                "action": "Investigate network connectivity and firewall rules between cluster nodes",
                "impact": "May cause cluster split-brain scenarios and data inconsistency",
                "affected_nodes": [conn["node"] for conn in failed_connections]
            })
        
        # Performance recommendations
        performance_metrics = health_data.get("performance_metrics", {})
        for node_name, metrics in performance_metrics.items():
            if isinstance(metrics, dict):
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                if cpu_usage > query.health_threshold_cpu:
                    recommendations.append({
                        "priority": "MEDIUM",
                        "category": "performance",
                        "title": f"High CPU Usage on {node_name}",
                        "description": f"CPU usage is {cpu_usage}%, exceeding threshold of {query.health_threshold_cpu}%",
                        "action": "Investigate CPU-intensive processes and consider load balancing",
                        "impact": "May cause performance degradation and increased response times",
                        "affected_nodes": [node_name]
                    })
                
                if memory_usage > query.health_threshold_memory:
                    recommendations.append({
                        "priority": "MEDIUM",
                        "category": "performance",
                        "title": f"High Memory Usage on {node_name}",
                        "description": f"Memory usage is {memory_usage}%, exceeding threshold of {query.health_threshold_memory}%",
                        "action": "Review memory allocation and consider increasing available RAM",
                        "impact": "May cause system instability and out-of-memory errors",
                        "affected_nodes": [node_name]
                    })
        
        # Disk usage recommendations
        disk_usage = health_data.get("disk_usage", {})
        for node_name, disk_data in disk_usage.items():
            if isinstance(disk_data, dict) and "filesystems" in disk_data:
                for fs in disk_data["filesystems"]:
                    if isinstance(fs, dict) and fs.get("usage_percent", 0) > query.health_threshold_disk:
                        recommendations.append({
                            "priority": "HIGH",
                            "category": "disk",
                            "title": f"High Disk Usage on {node_name}",
                            "description": f"Filesystem {fs.get('mount_point', 'unknown')} is {fs.get('usage_percent', 0)}% full",
                            "action": "Clean up unnecessary files or expand disk capacity",
                            "impact": "May cause system failures and data loss",
                            "affected_nodes": [node_name]
                        })
        
        # Service status recommendations
        service_status = health_data.get("service_status", {})
        for node_name, services in service_status.items():
            if isinstance(services, dict):
                for service_name, service_data in services.items():
                    if isinstance(service_data, dict) and service_data.get("status") != "active":
                        recommendations.append({
                            "priority": "HIGH",
                            "category": "services",
                            "title": f"Service Issue on {node_name}",
                            "description": f"Service {service_name} is {service_data.get('status', 'unknown')}",
                            "action": f"Restart and investigate {service_name} service",
                            "impact": "May cause functionality loss and monitoring gaps",
                            "affected_nodes": [node_name]
                        })
        
        # Overall health recommendations
        if health_score < 60:
            recommendations.append({
                "priority": "HIGH",
                "category": "health",
                "title": "Poor Overall Cluster Health",
                "description": f"Cluster health score is {health_score}%, indicating significant issues",
                "action": "Conduct comprehensive cluster health review and address critical issues",
                "impact": "May cause cluster instability and service disruption",
                "affected_nodes": "all"
            })
        
        return recommendations

    # Helper methods for data extraction
    def _extract_cpu_usage(self, stats: dict) -> float:
        """Extract CPU usage from node stats."""
        try:
            return float(stats.get("cpu", {}).get("usage_percent", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_memory_usage(self, stats: dict) -> float:
        """Extract memory usage from node stats."""
        try:
            return float(stats.get("memory", {}).get("usage_percent", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_load_average(self, stats: dict) -> dict:
        """Extract load average from node stats."""
        try:
            return {
                "1m": float(stats.get("load", {}).get("1m", 0)),
                "5m": float(stats.get("load", {}).get("5m", 0)),
                "15m": float(stats.get("load", {}).get("15m", 0))
            }
        except (ValueError, TypeError):
            return {"1m": 0.0, "5m": 0.0, "15m": 0.0}

    def _extract_uptime(self, stats: dict) -> str:
        """Extract uptime from node stats."""
        return str(stats.get("uptime", "unknown"))

    def _extract_process_count(self, stats: dict) -> int:
        """Extract process count from node stats."""
        try:
            return int(stats.get("processes", {}).get("count", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_file_descriptors(self, stats: dict) -> dict:
        """Extract file descriptor information."""
        try:
            return {
                "used": int(stats.get("file_descriptors", {}).get("used", 0)),
                "max": int(stats.get("file_descriptors", {}).get("max", 0))
            }
        except (ValueError, TypeError):
            return {"used": 0, "max": 0}

    def _extract_network_connections(self, stats: dict) -> int:
        """Extract network connection count."""
        try:
            return int(stats.get("network", {}).get("connections", 0))
        except (ValueError, TypeError):
            return 0

    async def _test_node_connectivity(self, node_ip: str, timeout: int) -> float:
        """Test connectivity to a specific node."""
        import asyncio
        try:
            # Simple connectivity test using asyncio
            start_time = datetime.utcnow()
            
            # Test if we can establish connection to the node
            try:
                reader, writer = await asyncio.wait_for(
                    asyncio.open_connection(node_ip, 55000),
                    timeout=timeout
                )
                writer.close()
                await writer.wait_closed()
                
                response_time = (datetime.utcnow() - start_time).total_seconds() * 1000
                return response_time
                
            except asyncio.TimeoutError:
                return None
            except Exception:
                return None
                
        except Exception:
            return None

    def _count_node_types(self, nodes: list) -> dict:
        """Count nodes by type."""
        types = {}
        for node in nodes:
            node_type = node.get("type", "unknown")
            types[node_type] = types.get(node_type, 0) + 1
        return types

    def _identify_master_node(self, nodes: list) -> str:
        """Identify the master node."""
        for node in nodes:
            if node.get("type") == "master":
                return node.get("name", "unknown")
        return "unknown"

    def _determine_overall_health_status(self, health_data: dict, query: 'ClusterHealthQuery') -> str:
        """Determine overall health status."""
        # Simple health determination based on connectivity
        connectivity = health_data.get("connectivity_status", {})
        if connectivity.get("overall_status") == "critical":
            return "Critical"
        elif connectivity.get("overall_status") == "degraded":
            return "Degraded"
        else:
            return "Healthy"

    def _identify_critical_issues(self, health_data: dict, query: 'ClusterHealthQuery') -> list:
        """Identify critical issues."""
        issues = []
        
        # Check connectivity issues
        connectivity = health_data.get("connectivity_status", {})
        if connectivity.get("overall_status") == "critical":
            issues.append("Cluster connectivity is critical")
        
        # Check for failed services
        services = health_data.get("service_status", {})
        for node_name, node_services in services.items():
            if isinstance(node_services, dict):
                for service_name, service_data in node_services.items():
                    if isinstance(service_data, dict) and service_data.get("status") == "failed":
                        issues.append(f"Service {service_name} failed on {node_name}")
        
        return issues

    def _identify_warning_issues(self, health_data: dict, query: 'ClusterHealthQuery') -> list:
        """Identify warning issues."""
        issues = []
        
        # Check performance thresholds
        performance = health_data.get("performance_metrics", {})
        for node_name, metrics in performance.items():
            if isinstance(metrics, dict):
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                if cpu_usage > query.health_threshold_cpu:
                    issues.append(f"High CPU usage on {node_name}: {cpu_usage}%")
                
                if memory_usage > query.health_threshold_memory:
                    issues.append(f"High memory usage on {node_name}: {memory_usage}%")
        
        return issues

    def _count_healthy_components(self, health_data: dict, query: 'ClusterHealthQuery') -> int:
        """Count healthy components."""
        count = 0
        
        # Count healthy nodes
        connectivity = health_data.get("connectivity_status", {})
        connected_nodes = len([conn for conn in connectivity.get("node_connectivity", {}).values() 
                             if conn.get("status") == "connected"])
        count += connected_nodes
        
        # Count healthy services
        services = health_data.get("service_status", {})
        for node_services in services.values():
            if isinstance(node_services, dict):
                healthy_services = sum(1 for service in node_services.values() 
                                     if isinstance(service, dict) and service.get("status") == "active")
                count += healthy_services
        
        return count

    def _count_degraded_components(self, health_data: dict, query: 'ClusterHealthQuery') -> int:
        """Count degraded components."""
        count = 0
        
        # Count degraded nodes based on performance
        performance = health_data.get("performance_metrics", {})
        for metrics in performance.values():
            if isinstance(metrics, dict):
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                if (cpu_usage > query.health_threshold_cpu * 0.8 or 
                    memory_usage > query.health_threshold_memory * 0.8):
                    count += 1
        
        return count

    def _count_failed_components(self, health_data: dict, query: 'ClusterHealthQuery') -> int:
        """Count failed components."""
        count = 0
        
        # Count disconnected nodes
        connectivity = health_data.get("connectivity_status", {})
        failed_nodes = len([conn for conn in connectivity.get("node_connectivity", {}).values() 
                          if conn.get("status") in ["disconnected", "error"]])
        count += failed_nodes
        
        # Count failed services
        services = health_data.get("service_status", {})
        for node_services in services.values():
            if isinstance(node_services, dict):
                failed_services = sum(1 for service in node_services.values() 
                                    if isinstance(service, dict) and service.get("status") == "failed")
                count += failed_services
        
        return count

    def _calculate_data_completeness(self, health_data: dict) -> float:
        """Calculate data completeness percentage."""
        total_components = 8  # Expected components
        collected_components = 0
        
        if health_data.get("cluster_nodes"):
            collected_components += 1
        if health_data.get("cluster_status"):
            collected_components += 1
        if health_data.get("performance_metrics"):
            collected_components += 1
        if health_data.get("connectivity_status"):
            collected_components += 1
        if health_data.get("resource_utilization"):
            collected_components += 1
        if health_data.get("service_status"):
            collected_components += 1
        if health_data.get("disk_usage"):
            collected_components += 1
        if health_data.get("network_statistics"):
            collected_components += 1
        
        return round((collected_components / total_components) * 100, 2)

    def _apply_summary_format(self, analysis: dict) -> dict:
        """Apply summary format filtering."""
        return {
            "query_parameters": analysis["query_parameters"],
            "cluster_overview": analysis["cluster_overview"],
            "health_summary": analysis["health_summary"],
            "health_score": analysis["health_score"],
            "recommendations": analysis["recommendations"][:5],  # Top 5 recommendations
            "analysis_metadata": analysis["analysis_metadata"]
        }

    def _apply_minimal_format(self, analysis: dict) -> dict:
        """Apply minimal format filtering."""
        return {
            "cluster_overview": {
                "total_nodes": analysis["cluster_overview"]["total_nodes"],
                "cluster_mode": analysis["cluster_overview"]["cluster_mode"]
            },
            "health_summary": {
                "overall_status": analysis["health_summary"]["overall_status"],
                "critical_issues": len(analysis["health_summary"]["critical_issues"])
            },
            "health_score": {
                "overall_score": analysis["health_score"]["overall_score"],
                "health_rating": analysis["health_score"]["health_rating"]
            },
            "analysis_metadata": analysis["analysis_metadata"]
        }

    # Additional helper methods for comprehensive data extraction
    def _generate_node_analysis(self, health_data: dict, query: 'ClusterHealthQuery') -> dict:
        """Generate detailed node analysis."""
        nodes = health_data.get("cluster_nodes", [])
        node_analysis = {}
        
        for node in nodes:
            node_name = node.get("name", "unknown")
            node_analysis[node_name] = {
                "basic_info": {
                    "name": node_name,
                    "type": node.get("type", "unknown"),
                    "ip": node.get("ip", "unknown"),
                    "status": node.get("status", "unknown")
                },
                "health_status": self._determine_node_health(node_name, health_data, query),
                "performance_summary": self._summarize_node_performance(node_name, health_data),
                "resource_summary": self._summarize_node_resources(node_name, health_data),
                "service_summary": self._summarize_node_services(node_name, health_data),
                "connectivity_status": self._get_node_connectivity_status(node_name, health_data)
            }
        
        return node_analysis

    def _generate_performance_analysis(self, health_data: dict, query: 'ClusterHealthQuery') -> dict:
        """Generate performance analysis."""
        performance_metrics = health_data.get("performance_metrics", {})
        
        if not performance_metrics:
            return {"status": "no_data", "message": "Performance metrics not available"}
        
        # Calculate cluster-wide performance statistics
        cpu_values = []
        memory_values = []
        load_values = []
        
        for node_name, metrics in performance_metrics.items():
            if isinstance(metrics, dict):
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                load_avg = metrics.get("load_average", {})
                
                cpu_values.append(cpu_usage)
                memory_values.append(memory_usage)
                if isinstance(load_avg, dict):
                    load_values.append(load_avg.get("1m", 0))
        
        return {
            "cluster_performance": {
                "cpu": {
                    "average": sum(cpu_values) / len(cpu_values) if cpu_values else 0,
                    "max": max(cpu_values) if cpu_values else 0,
                    "min": min(cpu_values) if cpu_values else 0,
                    "nodes_above_threshold": sum(1 for cpu in cpu_values if cpu > query.health_threshold_cpu)
                },
                "memory": {
                    "average": sum(memory_values) / len(memory_values) if memory_values else 0,
                    "max": max(memory_values) if memory_values else 0,
                    "min": min(memory_values) if memory_values else 0,
                    "nodes_above_threshold": sum(1 for mem in memory_values if mem > query.health_threshold_memory)
                },
                "load": {
                    "average": sum(load_values) / len(load_values) if load_values else 0,
                    "max": max(load_values) if load_values else 0,
                    "min": min(load_values) if load_values else 0
                }
            },
            "node_performance": performance_metrics,
            "performance_trends": self._analyze_performance_trends(performance_metrics),
            "bottlenecks": self._identify_performance_bottlenecks(performance_metrics, query)
        }

    def _generate_connectivity_analysis(self, health_data: dict) -> dict:
        """Generate connectivity analysis."""
        connectivity = health_data.get("connectivity_status", {})
        
        if not connectivity:
            return {"status": "no_data", "message": "Connectivity data not available"}
        
        return {
            "overall_connectivity": {
                "status": connectivity.get("overall_status", "unknown"),
                "connected_nodes": len([conn for conn in connectivity.get("node_connectivity", {}).values() 
                                      if conn.get("status") == "connected"]),
                "total_nodes": len(connectivity.get("node_connectivity", {})),
                "failed_connections": len(connectivity.get("failed_connections", []))
            },
            "node_connectivity": connectivity.get("node_connectivity", {}),
            "connection_issues": connectivity.get("failed_connections", []),
            "latency_analysis": self._analyze_connectivity_latency(connectivity)
        }

    def _generate_resource_analysis(self, health_data: dict, query: 'ClusterHealthQuery') -> dict:
        """Generate resource utilization analysis."""
        resources = health_data.get("resource_utilization", {})
        
        if not resources:
            return {"status": "no_data", "message": "Resource utilization data not available"}
        
        return {
            "cluster_resources": self._aggregate_cluster_resources(resources),
            "node_resources": resources,
            "resource_alerts": self._identify_resource_alerts(resources, query),
            "capacity_planning": self._analyze_capacity_planning(resources)
        }

    def _generate_service_analysis(self, health_data: dict) -> dict:
        """Generate service status analysis."""
        services = health_data.get("service_status", {})
        
        if not services:
            return {"status": "no_data", "message": "Service status data not available"}
        
        return {
            "service_overview": self._aggregate_service_status(services),
            "node_services": services,
            "service_issues": self._identify_service_issues(services),
            "critical_services": self._identify_critical_services(services)
        }

    def _generate_disk_analysis(self, health_data: dict, query: 'ClusterHealthQuery') -> dict:
        """Generate disk usage analysis."""
        disk_usage = health_data.get("disk_usage", {})
        
        if not disk_usage:
            return {"status": "no_data", "message": "Disk usage data not available"}
        
        return {
            "cluster_disk_usage": self._aggregate_disk_usage(disk_usage),
            "node_disk_usage": disk_usage,
            "disk_alerts": self._identify_disk_alerts(disk_usage, query),
            "storage_recommendations": self._generate_storage_recommendations(disk_usage)
        }

    def _generate_network_analysis(self, health_data: dict) -> dict:
        """Generate network statistics analysis."""
        network_stats = health_data.get("network_statistics", {})
        
        if not network_stats:
            return {"status": "no_data", "message": "Network statistics data not available"}
        
        return {
            "network_overview": self._aggregate_network_stats(network_stats),
            "node_network": network_stats,
            "network_issues": self._identify_network_issues(network_stats),
            "bandwidth_analysis": self._analyze_bandwidth_usage(network_stats)
        }

    # Additional helper methods for comprehensive analysis
    def _determine_node_health(self, node_name: str, health_data: dict, query: 'ClusterHealthQuery') -> str:
        """Determine individual node health status."""
        # Check connectivity
        connectivity = health_data.get("connectivity_status", {})
        node_connectivity = connectivity.get("node_connectivity", {}).get(node_name, {})
        
        if node_connectivity.get("status") != "connected":
            return "Critical"
        
        # Check performance
        performance = health_data.get("performance_metrics", {}).get(node_name, {})
        if isinstance(performance, dict):
            cpu_usage = performance.get("cpu_usage", 0)
            memory_usage = performance.get("memory_usage", 0)
            
            if cpu_usage > query.health_threshold_cpu or memory_usage > query.health_threshold_memory:
                return "Warning"
        
        # Check services
        services = health_data.get("service_status", {}).get(node_name, {})
        if isinstance(services, dict):
            for service_data in services.values():
                if isinstance(service_data, dict) and service_data.get("status") == "failed":
                    return "Critical"
        
        return "Healthy"

    def _summarize_node_performance(self, node_name: str, health_data: dict) -> dict:
        """Summarize node performance metrics."""
        performance = health_data.get("performance_metrics", {}).get(node_name, {})
        
        if not isinstance(performance, dict):
            return {"status": "no_data"}
        
        return {
            "cpu_usage": performance.get("cpu_usage", 0),
            "memory_usage": performance.get("memory_usage", 0),
            "load_average": performance.get("load_average", {}),
            "uptime": performance.get("uptime", "unknown"),
            "processes": performance.get("process_count", 0)
        }

    def _summarize_node_resources(self, node_name: str, health_data: dict) -> dict:
        """Summarize node resource utilization."""
        resources = health_data.get("resource_utilization", {}).get(node_name, {})
        
        if not isinstance(resources, dict):
            return {"status": "no_data"}
        
        return {
            "cpu": resources.get("cpu", {}),
            "memory": resources.get("memory", {}),
            "disk": resources.get("disk", {}),
            "network": resources.get("network", {})
        }

    def _summarize_node_services(self, node_name: str, health_data: dict) -> dict:
        """Summarize node service status."""
        services = health_data.get("service_status", {}).get(node_name, {})
        
        if not isinstance(services, dict):
            return {"status": "no_data"}
        
        service_summary = {
            "total_services": 0,
            "active_services": 0,
            "failed_services": 0,
            "services": {}
        }
        
        for service_name, service_data in services.items():
            if isinstance(service_data, dict):
                service_summary["total_services"] += 1
                status = service_data.get("status", "unknown")
                service_summary["services"][service_name] = status
                
                if status == "active":
                    service_summary["active_services"] += 1
                elif status == "failed":
                    service_summary["failed_services"] += 1
        
        return service_summary

    def _get_node_connectivity_status(self, node_name: str, health_data: dict) -> dict:
        """Get node connectivity status."""
        connectivity = health_data.get("connectivity_status", {})
        return connectivity.get("node_connectivity", {}).get(node_name, {"status": "unknown"})

    # Additional data extraction helper methods
    def _extract_cpu_cores(self, stats: dict) -> int:
        """Extract CPU core count."""
        try:
            return int(stats.get("cpu", {}).get("cores", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_load_1m(self, stats: dict) -> float:
        """Extract 1-minute load average."""
        try:
            return float(stats.get("load", {}).get("1m", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_load_5m(self, stats: dict) -> float:
        """Extract 5-minute load average."""
        try:
            return float(stats.get("load", {}).get("5m", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_load_15m(self, stats: dict) -> float:
        """Extract 15-minute load average."""
        try:
            return float(stats.get("load", {}).get("15m", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_memory_total(self, stats: dict) -> float:
        """Extract total memory in GB."""
        try:
            return float(stats.get("memory", {}).get("total_gb", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_memory_available(self, stats: dict) -> float:
        """Extract available memory in GB."""
        try:
            return float(stats.get("memory", {}).get("available_gb", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_memory_cached(self, stats: dict) -> float:
        """Extract cached memory in GB."""
        try:
            return float(stats.get("memory", {}).get("cached_gb", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_swap_usage(self, stats: dict) -> float:
        """Extract swap usage percentage."""
        try:
            return float(stats.get("memory", {}).get("swap_usage_percent", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_disk_usage(self, stats: dict) -> float:
        """Extract disk usage percentage."""
        try:
            return float(stats.get("disk", {}).get("usage_percent", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_disk_total(self, stats: dict) -> float:
        """Extract total disk space in GB."""
        try:
            return float(stats.get("disk", {}).get("total_gb", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_disk_available(self, stats: dict) -> float:
        """Extract available disk space in GB."""
        try:
            return float(stats.get("disk", {}).get("available_gb", 0))
        except (ValueError, TypeError):
            return 0.0

    def _extract_disk_read_ops(self, stats: dict) -> int:
        """Extract disk read operations."""
        try:
            return int(stats.get("disk", {}).get("read_ops", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_disk_write_ops(self, stats: dict) -> int:
        """Extract disk write operations."""
        try:
            return int(stats.get("disk", {}).get("write_ops", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_network_bytes_sent(self, stats: dict) -> int:
        """Extract network bytes sent."""
        try:
            return int(stats.get("network", {}).get("bytes_sent", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_network_bytes_received(self, stats: dict) -> int:
        """Extract network bytes received."""
        try:
            return int(stats.get("network", {}).get("bytes_received", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_network_packets_sent(self, stats: dict) -> int:
        """Extract network packets sent."""
        try:
            return int(stats.get("network", {}).get("packets_sent", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_network_packets_received(self, stats: dict) -> int:
        """Extract network packets received."""
        try:
            return int(stats.get("network", {}).get("packets_received", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_network_errors(self, stats: dict) -> int:
        """Extract network errors."""
        try:
            return int(stats.get("network", {}).get("errors", 0))
        except (ValueError, TypeError):
            return 0

    def _extract_service_status(self, stats: dict, service_name: str) -> dict:
        """Extract service status."""
        try:
            service_data = stats.get("services", {}).get(service_name, {})
            return {
                "status": service_data.get("status", "unknown"),
                "pid": service_data.get("pid", 0),
                "uptime": service_data.get("uptime", "unknown"),
                "memory_usage": service_data.get("memory_usage", 0),
                "cpu_usage": service_data.get("cpu_usage", 0)
            }
        except (ValueError, TypeError):
            return {"status": "unknown"}

    def _extract_system_services(self, stats: dict) -> dict:
        """Extract system service information."""
        try:
            return stats.get("system_services", {})
        except (ValueError, TypeError):
            return {}

    def _extract_daemon_status(self, stats: dict) -> dict:
        """Extract daemon status information."""
        try:
            return stats.get("daemon_status", {})
        except (ValueError, TypeError):
            return {}

    def _extract_filesystem_usage(self, stats: dict) -> list:
        """Extract filesystem usage information."""
        try:
            return stats.get("filesystems", [])
        except (ValueError, TypeError):
            return []

    def _extract_log_dir_usage(self, stats: dict) -> dict:
        """Extract log directory usage."""
        try:
            return stats.get("log_directory", {})
        except (ValueError, TypeError):
            return {}

    def _extract_data_dir_usage(self, stats: dict) -> dict:
        """Extract data directory usage."""
        try:
            return stats.get("data_directory", {})
        except (ValueError, TypeError):
            return {}

    def _extract_backup_dir_usage(self, stats: dict) -> dict:
        """Extract backup directory usage."""
        try:
            return stats.get("backup_directory", {})
        except (ValueError, TypeError):
            return {}

    def _extract_temp_dir_usage(self, stats: dict) -> dict:
        """Extract temporary directory usage."""
        try:
            return stats.get("temp_directory", {})
        except (ValueError, TypeError):
            return {}

    def _analyze_disk_health(self, stats: dict) -> dict:
        """Analyze disk health."""
        try:
            return stats.get("disk_health", {})
        except (ValueError, TypeError):
            return {}

    def _analyze_disk_growth(self, stats: dict) -> dict:
        """Analyze disk growth trends."""
        try:
            return stats.get("growth_trend", {})
        except (ValueError, TypeError):
            return {}

    def _extract_network_interfaces(self, stats: dict) -> list:
        """Extract network interface information."""
        try:
            return stats.get("interfaces", [])
        except (ValueError, TypeError):
            return []

    def _extract_bandwidth_usage(self, stats: dict) -> dict:
        """Extract bandwidth usage information."""
        try:
            return stats.get("bandwidth_usage", {})
        except (ValueError, TypeError):
            return {}

    def _extract_connection_counts(self, stats: dict) -> dict:
        """Extract connection count information."""
        try:
            return stats.get("connection_counts", {})
        except (ValueError, TypeError):
            return {}

    def _extract_packet_statistics(self, stats: dict) -> dict:
        """Extract packet statistics."""
        try:
            return stats.get("packet_statistics", {})
        except (ValueError, TypeError):
            return {}

    def _extract_network_error_stats(self, stats: dict) -> dict:
        """Extract network error statistics."""
        try:
            return stats.get("error_statistics", {})
        except (ValueError, TypeError):
            return {}

    def _extract_latency_metrics(self, stats: dict) -> dict:
        """Extract latency metrics."""
        try:
            return stats.get("latency_metrics", {})
        except (ValueError, TypeError):
            return {}

    # Additional analysis helper methods
    def _analyze_performance_trends(self, performance_metrics: dict) -> dict:
        """Analyze performance trends."""
        # Placeholder for trend analysis
        return {
            "cpu_trend": "stable",
            "memory_trend": "stable",
            "load_trend": "stable",
            "trend_analysis": "Performance metrics are within normal ranges"
        }

    def _identify_performance_bottlenecks(self, performance_metrics: dict, query: 'ClusterHealthQuery') -> list:
        """Identify performance bottlenecks."""
        bottlenecks = []
        
        for node_name, metrics in performance_metrics.items():
            if isinstance(metrics, dict):
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                if cpu_usage > query.health_threshold_cpu:
                    bottlenecks.append({
                        "node": node_name,
                        "type": "cpu",
                        "value": cpu_usage,
                        "threshold": query.health_threshold_cpu
                    })
                
                if memory_usage > query.health_threshold_memory:
                    bottlenecks.append({
                        "node": node_name,
                        "type": "memory",
                        "value": memory_usage,
                        "threshold": query.health_threshold_memory
                    })
        
        return bottlenecks

    def _analyze_connectivity_latency(self, connectivity: dict) -> dict:
        """Analyze connectivity latency."""
        latencies = []
        
        for node_name, conn_data in connectivity.get("node_connectivity", {}).items():
            if isinstance(conn_data, dict) and conn_data.get("response_time_ms") is not None:
                latencies.append(conn_data["response_time_ms"])
        
        if latencies:
            return {
                "average_latency_ms": sum(latencies) / len(latencies),
                "max_latency_ms": max(latencies),
                "min_latency_ms": min(latencies),
                "nodes_with_latency": len(latencies)
            }
        
        return {"status": "no_data"}

    def _aggregate_cluster_resources(self, resources: dict) -> dict:
        """Aggregate cluster-wide resource information."""
        total_cpu_usage = 0
        total_memory_usage = 0
        total_disk_usage = 0
        node_count = 0
        
        for node_name, node_resources in resources.items():
            if isinstance(node_resources, dict):
                cpu_data = node_resources.get("cpu", {})
                memory_data = node_resources.get("memory", {})
                disk_data = node_resources.get("disk", {})
                
                if isinstance(cpu_data, dict):
                    total_cpu_usage += cpu_data.get("usage_percent", 0)
                if isinstance(memory_data, dict):
                    total_memory_usage += memory_data.get("usage_percent", 0)
                if isinstance(disk_data, dict):
                    total_disk_usage += disk_data.get("usage_percent", 0)
                
                node_count += 1
        
        return {
            "average_cpu_usage": total_cpu_usage / node_count if node_count > 0 else 0,
            "average_memory_usage": total_memory_usage / node_count if node_count > 0 else 0,
            "average_disk_usage": total_disk_usage / node_count if node_count > 0 else 0,
            "total_nodes": node_count
        }

    def _identify_resource_alerts(self, resources: dict, query: 'ClusterHealthQuery') -> list:
        """Identify resource-related alerts."""
        alerts = []
        
        for node_name, node_resources in resources.items():
            if isinstance(node_resources, dict):
                cpu_data = node_resources.get("cpu", {})
                memory_data = node_resources.get("memory", {})
                disk_data = node_resources.get("disk", {})
                
                if isinstance(cpu_data, dict):
                    cpu_usage = cpu_data.get("usage_percent", 0)
                    if cpu_usage > query.health_threshold_cpu:
                        alerts.append({
                            "node": node_name,
                            "type": "cpu",
                            "severity": "high" if cpu_usage > 90 else "medium",
                            "value": cpu_usage,
                            "threshold": query.health_threshold_cpu
                        })
                
                if isinstance(memory_data, dict):
                    memory_usage = memory_data.get("usage_percent", 0)
                    if memory_usage > query.health_threshold_memory:
                        alerts.append({
                            "node": node_name,
                            "type": "memory",
                            "severity": "high" if memory_usage > 95 else "medium",
                            "value": memory_usage,
                            "threshold": query.health_threshold_memory
                        })
                
                if isinstance(disk_data, dict):
                    disk_usage = disk_data.get("usage_percent", 0)
                    if disk_usage > query.health_threshold_disk:
                        alerts.append({
                            "node": node_name,
                            "type": "disk",
                            "severity": "high" if disk_usage > 95 else "medium",
                            "value": disk_usage,
                            "threshold": query.health_threshold_disk
                        })
        
        return alerts

    def _analyze_capacity_planning(self, resources: dict) -> dict:
        """Analyze capacity planning needs."""
        # Placeholder for capacity planning analysis
        return {
            "cpu_capacity": "sufficient",
            "memory_capacity": "sufficient",
            "disk_capacity": "sufficient",
            "recommendations": "Monitor resource usage trends for future capacity planning"
        }

    def _aggregate_service_status(self, services: dict) -> dict:
        """Aggregate service status across all nodes."""
        total_services = 0
        active_services = 0
        failed_services = 0
        degraded_services = 0
        
        for node_name, node_services in services.items():
            if isinstance(node_services, dict):
                for service_name, service_data in node_services.items():
                    if isinstance(service_data, dict):
                        total_services += 1
                        status = service_data.get("status", "unknown")
                        
                        if status == "active":
                            active_services += 1
                        elif status == "failed":
                            failed_services += 1
                        elif status in ["degraded", "warning"]:
                            degraded_services += 1
        
        return {
            "total_services": total_services,
            "active_services": active_services,
            "failed_services": failed_services,
            "degraded_services": degraded_services,
            "service_availability": round((active_services / total_services * 100) if total_services > 0 else 0, 2)
        }

    def _identify_service_issues(self, services: dict) -> list:
        """Identify service issues."""
        issues = []
        
        for node_name, node_services in services.items():
            if isinstance(node_services, dict):
                for service_name, service_data in node_services.items():
                    if isinstance(service_data, dict):
                        status = service_data.get("status", "unknown")
                        
                        if status == "failed":
                            issues.append({
                                "node": node_name,
                                "service": service_name,
                                "status": status,
                                "severity": "high"
                            })
                        elif status in ["degraded", "warning"]:
                            issues.append({
                                "node": node_name,
                                "service": service_name,
                                "status": status,
                                "severity": "medium"
                            })
        
        return issues

    def _identify_critical_services(self, services: dict) -> list:
        """Identify critical services."""
        critical_services = ["wazuh-manager", "wazuh-indexer", "wazuh-dashboard"]
        critical_issues = []
        
        for node_name, node_services in services.items():
            if isinstance(node_services, dict):
                for service_name, service_data in node_services.items():
                    if service_name in critical_services and isinstance(service_data, dict):
                        status = service_data.get("status", "unknown")
                        
                        if status != "active":
                            critical_issues.append({
                                "node": node_name,
                                "service": service_name,
                                "status": status,
                                "criticality": "high"
                            })
        
        return critical_issues

    def _aggregate_disk_usage(self, disk_usage: dict) -> dict:
        """Aggregate disk usage across all nodes."""
        total_disk_space = 0
        total_used_space = 0
        filesystem_count = 0
        
        for node_name, node_disk in disk_usage.items():
            if isinstance(node_disk, dict) and "filesystems" in node_disk:
                for fs in node_disk["filesystems"]:
                    if isinstance(fs, dict):
                        total_gb = fs.get("total_gb", 0)
                        used_gb = fs.get("used_gb", 0)
                        
                        total_disk_space += total_gb
                        total_used_space += used_gb
                        filesystem_count += 1
        
        return {
            "total_disk_space_gb": total_disk_space,
            "total_used_space_gb": total_used_space,
            "total_available_space_gb": total_disk_space - total_used_space,
            "average_usage_percent": round((total_used_space / total_disk_space * 100) if total_disk_space > 0 else 0, 2),
            "filesystem_count": filesystem_count
        }

    def _identify_disk_alerts(self, disk_usage: dict, query: 'ClusterHealthQuery') -> list:
        """Identify disk usage alerts."""
        alerts = []
        
        for node_name, node_disk in disk_usage.items():
            if isinstance(node_disk, dict) and "filesystems" in node_disk:
                for fs in node_disk["filesystems"]:
                    if isinstance(fs, dict):
                        usage_percent = fs.get("usage_percent", 0)
                        mount_point = fs.get("mount_point", "unknown")
                        
                        if usage_percent > query.health_threshold_disk:
                            alerts.append({
                                "node": node_name,
                                "mount_point": mount_point,
                                "usage_percent": usage_percent,
                                "threshold": query.health_threshold_disk,
                                "severity": "high" if usage_percent > 95 else "medium"
                            })
        
        return alerts

    def _generate_storage_recommendations(self, disk_usage: dict) -> list:
        """Generate storage recommendations."""
        recommendations = []
        
        for node_name, node_disk in disk_usage.items():
            if isinstance(node_disk, dict) and "filesystems" in node_disk:
                for fs in node_disk["filesystems"]:
                    if isinstance(fs, dict):
                        usage_percent = fs.get("usage_percent", 0)
                        mount_point = fs.get("mount_point", "unknown")
                        
                        if usage_percent > 90:
                            recommendations.append({
                                "node": node_name,
                                "mount_point": mount_point,
                                "action": "expand_storage",
                                "priority": "high",
                                "description": f"Storage usage is {usage_percent}% on {mount_point}"
                            })
                        elif usage_percent > 80:
                            recommendations.append({
                                "node": node_name,
                                "mount_point": mount_point,
                                "action": "monitor_storage",
                                "priority": "medium",
                                "description": f"Storage usage is {usage_percent}% on {mount_point}"
                            })
        
        return recommendations

    def _aggregate_network_stats(self, network_stats: dict) -> dict:
        """Aggregate network statistics."""
        total_bytes_sent = 0
        total_bytes_received = 0
        total_errors = 0
        node_count = 0
        
        for node_name, node_network in network_stats.items():
            if isinstance(node_network, dict):
                bandwidth = node_network.get("bandwidth_usage", {})
                error_stats = node_network.get("error_statistics", {})
                
                if isinstance(bandwidth, dict):
                    total_bytes_sent += bandwidth.get("bytes_sent", 0)
                    total_bytes_received += bandwidth.get("bytes_received", 0)
                
                if isinstance(error_stats, dict):
                    total_errors += error_stats.get("total_errors", 0)
                
                node_count += 1
        
        return {
            "total_bytes_sent": total_bytes_sent,
            "total_bytes_received": total_bytes_received,
            "total_network_errors": total_errors,
            "average_errors_per_node": total_errors / node_count if node_count > 0 else 0,
            "total_nodes": node_count
        }

    def _identify_network_issues(self, network_stats: dict) -> list:
        """Identify network issues."""
        issues = []
        
        for node_name, node_network in network_stats.items():
            if isinstance(node_network, dict):
                error_stats = node_network.get("error_statistics", {})
                
                if isinstance(error_stats, dict):
                    error_rate = error_stats.get("error_rate", 0)
                    packet_loss = error_stats.get("packet_loss", 0)
                    
                    if error_rate > 1.0:  # 1% error rate threshold
                        issues.append({
                            "node": node_name,
                            "issue": "high_error_rate",
                            "value": error_rate,
                            "severity": "high" if error_rate > 5.0 else "medium"
                        })
                    
                    if packet_loss > 0.1:  # 0.1% packet loss threshold
                        issues.append({
                            "node": node_name,
                            "issue": "packet_loss",
                            "value": packet_loss,
                            "severity": "high" if packet_loss > 1.0 else "medium"
                        })
        
        return issues

    def _analyze_bandwidth_usage(self, network_stats: dict) -> dict:
        """Analyze bandwidth usage patterns."""
        # Placeholder for bandwidth analysis
        return {
            "peak_usage": "moderate",
            "average_usage": "normal",
            "trend": "stable",
            "recommendations": "Monitor bandwidth usage during peak hours"
        }

    async def _handle_get_wazuh_cluster_nodes(self, arguments: dict) -> list[types.TextContent]:
        """Handle individual node monitoring and management with comprehensive performance tracking."""
        start_time = datetime.utcnow()
        
        try:
            # Import validation function
            from .utils.validation import validate_cluster_nodes_query
            
            # Validate input parameters
            validated_query = validate_cluster_nodes_query(arguments)
            
            # Collect cluster nodes data
            nodes_data = await self._collect_cluster_nodes_data(validated_query)
            
            # Analyze cluster nodes
            analysis = await self._analyze_cluster_nodes(nodes_data, validated_query, start_time)
            
            return [types.TextContent(
                type="text",
                text=json.dumps(analysis, indent=2, default=str)
            )]
            
        except Exception as e:
            self.logger.error(f"Error in cluster nodes analysis: {str(e)}")
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            return [types.TextContent(
                type="text",
                text=self._format_error_response(e, execution_time=execution_time)
            )]

    async def _collect_cluster_nodes_data(self, query) -> dict:
        """Collect comprehensive cluster nodes data."""
        nodes_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "cluster_nodes": [],
            "node_details": {},
            "performance_metrics": {},
            "sync_status": {},
            "load_metrics": {},
            "agent_distribution": {},
            "configuration_data": {},
            "collection_errors": []
        }
        
        try:
            # Get cluster nodes information
            cluster_response = await self.api_client.get_cluster_nodes()
            all_nodes = cluster_response.get("data", [])
            
            # Filter nodes based on query parameters
            filtered_nodes = self._filter_cluster_nodes(all_nodes, query)
            nodes_data["cluster_nodes"] = filtered_nodes
            
            # Collect detailed node information
            for node in filtered_nodes:
                node_name = node.get("name", "unknown")
                
                try:
                    # Collect basic node details
                    nodes_data["node_details"][node_name] = await self._collect_node_details(node, query)
                    
                    # Collect performance metrics if requested
                    if query.include_performance:
                        nodes_data["performance_metrics"][node_name] = await self._collect_node_performance_data(node_name)
                    
                    # Collect sync status if requested
                    if query.include_sync_status:
                        nodes_data["sync_status"][node_name] = await self._collect_node_sync_status(node_name)
                    
                    # Collect load metrics if requested
                    if query.include_load_metrics:
                        nodes_data["load_metrics"][node_name] = await self._collect_node_load_metrics(node_name)
                    
                    # Collect agent distribution if requested
                    if query.include_agent_distribution:
                        nodes_data["agent_distribution"][node_name] = await self._collect_node_agent_distribution(node_name)
                    
                    # Collect configuration if requested
                    if query.include_configuration:
                        nodes_data["configuration_data"][node_name] = await self._collect_node_configuration(node_name)
                        
                except Exception as e:
                    nodes_data["collection_errors"].append({
                        "node": node_name,
                        "error": str(e)
                    })
            
        except Exception as e:
            self.logger.error(f"Error collecting cluster nodes data: {str(e)}")
            nodes_data["collection_errors"].append({
                "component": "general",
                "error": str(e)
            })
        
        return nodes_data

    def _filter_cluster_nodes(self, nodes: list, query) -> list:
        """Filter cluster nodes based on query parameters."""
        filtered_nodes = nodes
        
        # Filter by node type
        if query.node_type and "all" not in query.node_type:
            filtered_nodes = [
                node for node in filtered_nodes
                if node.get("type", "unknown") in query.node_type
            ]
        
        # Filter by status
        if query.status_filter and "all" not in query.status_filter:
            filtered_nodes = [
                node for node in filtered_nodes
                if node.get("status", "unknown") in query.status_filter
            ]
        
        # Filter by node name
        if query.node_name:
            filtered_nodes = [
                node for node in filtered_nodes
                if node.get("name", "").lower() == query.node_name.lower()
            ]
        
        return filtered_nodes

    async def _collect_node_details(self, node: dict, query) -> dict:
        """Collect detailed information for a specific node."""
        details = {
            "name": node.get("name", "unknown"),
            "type": node.get("type", "unknown"),
            "status": node.get("status", "unknown"),
            "ip_address": node.get("ip", "unknown"),
            "version": node.get("version", "unknown"),
            "last_keep_alive": node.get("last_keep_alive", "unknown"),
            "uptime": node.get("uptime", "unknown"),
            "cluster_version": node.get("cluster_version", "unknown")
        }
        
        return details

    async def _collect_node_performance_data(self, node_name: str) -> dict:
        """Collect performance data for a specific node."""
        try:
            # Get node statistics
            node_stats = await self.api_client.get_node_stats(node_name)
            
            return {
                "cpu_usage": self._extract_cpu_usage(node_stats),
                "memory_usage": self._extract_memory_usage(node_stats),
                "disk_usage": self._extract_disk_usage(node_stats),
                "load_average": self._extract_load_average(node_stats),
                "uptime": self._extract_uptime(node_stats),
                "process_count": self._extract_process_count(node_stats),
                "network_stats": self._extract_network_stats(node_stats)
            }
        except Exception as e:
            return {"error": str(e), "status": "collection_failed"}

    async def _collect_node_sync_status(self, node_name: str) -> dict:
        """Collect synchronization status for a specific node."""
        try:
            # Get cluster sync status
            sync_data = await self.api_client.get_cluster_sync_status(node_name)
            
            return {
                "sync_status": sync_data.get("sync_status", "unknown"),
                "last_sync": sync_data.get("last_sync", "unknown"),
                "sync_lag_seconds": sync_data.get("sync_lag", 0),
                "files_to_sync": sync_data.get("files_to_sync", 0),
                "integrity_check": sync_data.get("integrity_check", "unknown"),
                "sync_errors": sync_data.get("errors", [])
            }
        except Exception as e:
            return {"error": str(e), "status": "collection_failed"}

    async def _collect_node_load_metrics(self, node_name: str) -> dict:
        """Collect load balancing and capacity metrics for a specific node."""
        try:
            # Get load metrics
            load_data = await self.api_client.get_node_load_metrics(node_name)
            
            return {
                "current_load": load_data.get("current_load", 0),
                "max_capacity": load_data.get("max_capacity", 0),
                "utilization_percent": load_data.get("utilization_percent", 0),
                "queue_size": load_data.get("queue_size", 0),
                "processed_events": load_data.get("processed_events", 0),
                "failed_events": load_data.get("failed_events", 0),
                "average_processing_time": load_data.get("avg_processing_time", 0)
            }
        except Exception as e:
            return {"error": str(e), "status": "collection_failed"}

    async def _collect_node_agent_distribution(self, node_name: str) -> dict:
        """Collect agent distribution information for a specific node."""
        try:
            # Get agent distribution
            agents_data = await self.api_client.get_node_agents(node_name)
            
            return {
                "total_agents": agents_data.get("total_agents", 0),
                "active_agents": agents_data.get("active_agents", 0),
                "disconnected_agents": agents_data.get("disconnected_agents", 0),
                "agent_types": agents_data.get("agent_types", {}),
                "agent_versions": agents_data.get("agent_versions", {}),
                "load_distribution": agents_data.get("load_distribution", "balanced")
            }
        except Exception as e:
            return {"error": str(e), "status": "collection_failed"}

    async def _collect_node_configuration(self, node_name: str) -> dict:
        """Collect configuration details for a specific node."""
        try:
            # Get node configuration
            config_data = await self.api_client.get_node_configuration(node_name)
            
            return {
                "cluster_config": config_data.get("cluster", {}),
                "logging_config": config_data.get("logging", {}),
                "auth_config": config_data.get("auth", {}),
                "api_config": config_data.get("api", {}),
                "resource_limits": config_data.get("limits", {})
            }
        except Exception as e:
            return {"error": str(e), "status": "collection_failed"}

    async def _analyze_cluster_nodes(self, nodes_data: dict, query, start_time: datetime) -> dict:
        """Analyze cluster nodes data and generate comprehensive insights."""
        analysis = {
            "summary": {
                "timestamp": datetime.utcnow().isoformat(),
                "execution_time_seconds": (datetime.utcnow() - start_time).total_seconds(),
                "total_nodes": len(nodes_data["cluster_nodes"]),
                "node_types": {},
                "status_distribution": {},
                "overall_health": "unknown"
            },
            "node_analysis": {},
            "performance_insights": {},
            "sync_analysis": {},
            "load_analysis": {},
            "agent_distribution_analysis": {},
            "recommendations": [],
            "alerts": [],
            "quality_indicators": {
                "data_completeness": 0,
                "collection_success_rate": 0,
                "analysis_confidence": "high"
            }
        }
        
        try:
            # Generate summary statistics
            analysis["summary"] = self._generate_nodes_summary(nodes_data, query)
            
            # Analyze individual nodes
            analysis["node_analysis"] = self._analyze_individual_nodes(nodes_data, query)
            
            # Performance analysis
            if query.include_performance:
                analysis["performance_insights"] = self._analyze_nodes_performance(nodes_data, query)
            
            # Sync analysis
            if query.include_sync_status:
                analysis["sync_analysis"] = self._analyze_nodes_sync_status(nodes_data, query)
            
            # Load analysis
            if query.include_load_metrics:
                analysis["load_analysis"] = self._analyze_nodes_load_distribution(nodes_data, query)
            
            # Agent distribution analysis
            if query.include_agent_distribution:
                analysis["agent_distribution_analysis"] = self._analyze_agent_distribution(nodes_data, query)
            
            # Generate recommendations and alerts
            analysis["recommendations"] = self._generate_nodes_recommendations(nodes_data, query)
            analysis["alerts"] = self._generate_nodes_alerts(nodes_data, query)
            
            # Calculate quality indicators
            analysis["quality_indicators"] = self._calculate_nodes_quality_indicators(nodes_data)
            
        except Exception as e:
            self.logger.error(f"Error in cluster nodes analysis: {str(e)}")
            analysis["analysis_errors"] = [str(e)]
        
        return analysis

    def _generate_nodes_summary(self, nodes_data: dict, query) -> dict:
        """Generate summary statistics for cluster nodes."""
        nodes = nodes_data["cluster_nodes"]
        
        # Count node types
        node_types = {}
        status_distribution = {}
        
        for node in nodes:
            node_type = node.get("type", "unknown")
            status = node.get("status", "unknown")
            
            node_types[node_type] = node_types.get(node_type, 0) + 1
            status_distribution[status] = status_distribution.get(status, 0) + 1
        
        # Determine overall health
        active_nodes = status_distribution.get("active", 0)
        total_nodes = len(nodes)
        health_ratio = active_nodes / total_nodes if total_nodes > 0 else 0
        
        if health_ratio >= 0.9:
            overall_health = "healthy"
        elif health_ratio >= 0.7:
            overall_health = "degraded"
        else:
            overall_health = "critical"
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "total_nodes": total_nodes,
            "node_types": node_types,
            "status_distribution": status_distribution,
            "overall_health": overall_health,
            "health_ratio": round(health_ratio * 100, 2)
        }

    def _analyze_individual_nodes(self, nodes_data: dict, query) -> dict:
        """Analyze individual node details."""
        node_analysis = {}
        
        for node in nodes_data["cluster_nodes"]:
            node_name = node.get("name", "unknown")
            
            # Basic node analysis
            node_info = {
                "basic_info": nodes_data["node_details"].get(node_name, {}),
                "health_status": "unknown",
                "issues": [],
                "strengths": []
            }
            
            # Determine node health
            status = node.get("status", "unknown")
            if status == "active":
                node_info["health_status"] = "healthy"
                node_info["strengths"].append("Node is active and responding")
            else:
                node_info["health_status"] = "unhealthy"
                node_info["issues"].append(f"Node status: {status}")
            
            node_analysis[node_name] = node_info
        
        return node_analysis

    def _analyze_nodes_performance(self, nodes_data: dict, query) -> dict:
        """Analyze performance metrics across nodes."""
        performance_insights = {
            "overall_performance": "unknown",
            "node_performance": {},
            "performance_alerts": [],
            "resource_trends": {}
        }
        
        total_cpu_usage = 0
        total_memory_usage = 0
        node_count = 0
        
        for node_name, metrics in nodes_data["performance_metrics"].items():
            if "error" not in metrics:
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                # Individual node performance
                performance_insights["node_performance"][node_name] = {
                    "cpu_usage": cpu_usage,
                    "memory_usage": memory_usage,
                    "performance_score": self._calculate_performance_score(metrics),
                    "status": self._determine_performance_status(cpu_usage, memory_usage, query)
                }
                
                # Check for performance alerts
                if cpu_usage > query.performance_threshold_cpu:
                    performance_insights["performance_alerts"].append({
                        "node": node_name,
                        "type": "high_cpu_usage",
                        "value": cpu_usage,
                        "threshold": query.performance_threshold_cpu,
                        "severity": "high" if cpu_usage > 90 else "medium"
                    })
                
                if memory_usage > query.performance_threshold_memory:
                    performance_insights["performance_alerts"].append({
                        "node": node_name,
                        "type": "high_memory_usage",
                        "value": memory_usage,
                        "threshold": query.performance_threshold_memory,
                        "severity": "high" if memory_usage > 95 else "medium"
                    })
                
                total_cpu_usage += cpu_usage
                total_memory_usage += memory_usage
                node_count += 1
        
        # Calculate overall performance
        if node_count > 0:
            avg_cpu = total_cpu_usage / node_count
            avg_memory = total_memory_usage / node_count
            
            if avg_cpu < 70 and avg_memory < 80:
                performance_insights["overall_performance"] = "excellent"
            elif avg_cpu < 85 and avg_memory < 90:
                performance_insights["overall_performance"] = "good"
            elif avg_cpu < 95 and avg_memory < 95:
                performance_insights["overall_performance"] = "concerning"
            else:
                performance_insights["overall_performance"] = "critical"
        
        return performance_insights

    def _analyze_nodes_sync_status(self, nodes_data: dict, query) -> dict:
        """Analyze synchronization status across nodes."""
        sync_analysis = {
            "overall_sync_health": "unknown",
            "node_sync_status": {},
            "sync_alerts": [],
            "integrity_status": "unknown"
        }
        
        nodes_in_sync = 0
        total_nodes = 0
        max_lag = 0
        
        for node_name, sync_status in nodes_data["sync_status"].items():
            if "error" not in sync_status:
                sync_lag = sync_status.get("sync_lag_seconds", 0)
                files_to_sync = sync_status.get("files_to_sync", 0)
                
                # Individual node sync analysis
                sync_analysis["node_sync_status"][node_name] = {
                    "sync_status": sync_status.get("sync_status", "unknown"),
                    "sync_lag_seconds": sync_lag,
                    "files_to_sync": files_to_sync,
                    "health": "good" if sync_lag <= query.sync_lag_threshold else "poor"
                }
                
                # Check for sync alerts
                if sync_lag > query.sync_lag_threshold:
                    sync_analysis["sync_alerts"].append({
                        "node": node_name,
                        "type": "high_sync_lag",
                        "lag_seconds": sync_lag,
                        "threshold": query.sync_lag_threshold,
                        "severity": "high" if sync_lag > 120 else "medium"
                    })
                
                if files_to_sync > 100:
                    sync_analysis["sync_alerts"].append({
                        "node": node_name,
                        "type": "sync_backlog",
                        "files_pending": files_to_sync,
                        "severity": "high" if files_to_sync > 1000 else "medium"
                    })
                
                if sync_lag <= query.sync_lag_threshold:
                    nodes_in_sync += 1
                
                max_lag = max(max_lag, sync_lag)
                total_nodes += 1
        
        # Determine overall sync health
        if total_nodes > 0:
            sync_ratio = nodes_in_sync / total_nodes
            if sync_ratio >= 0.9:
                sync_analysis["overall_sync_health"] = "excellent"
            elif sync_ratio >= 0.7:
                sync_analysis["overall_sync_health"] = "good"
            elif sync_ratio >= 0.5:
                sync_analysis["overall_sync_health"] = "degraded"
            else:
                sync_analysis["overall_sync_health"] = "critical"
        
        return sync_analysis

    def _analyze_nodes_load_distribution(self, nodes_data: dict, query) -> dict:
        """Analyze load distribution across nodes."""
        load_analysis = {
            "load_balance_status": "unknown",
            "node_loads": {},
            "load_alerts": [],
            "rebalancing_recommendations": []
        }
        
        node_loads = []
        
        for node_name, load_metrics in nodes_data["load_metrics"].items():
            if "error" not in load_metrics:
                utilization = load_metrics.get("utilization_percent", 0)
                current_load = load_metrics.get("current_load", 0)
                queue_size = load_metrics.get("queue_size", 0)
                
                load_analysis["node_loads"][node_name] = {
                    "utilization_percent": utilization,
                    "current_load": current_load,
                    "queue_size": queue_size,
                    "status": "overloaded" if utilization > 90 else ("high" if utilization > 75 else "normal")
                }
                
                node_loads.append(utilization)
                
                # Check for load alerts
                if utilization > 90:
                    load_analysis["load_alerts"].append({
                        "node": node_name,
                        "type": "overloaded",
                        "utilization": utilization,
                        "severity": "critical"
                    })
                elif queue_size > 1000:
                    load_analysis["load_alerts"].append({
                        "node": node_name,
                        "type": "queue_backlog",
                        "queue_size": queue_size,
                        "severity": "high"
                    })
        
        # Analyze load distribution
        if node_loads:
            avg_load = sum(node_loads) / len(node_loads)
            load_variance = sum((load - avg_load) ** 2 for load in node_loads) / len(node_loads)
            
            if load_variance < 100:  # Low variance indicates good balance
                load_analysis["load_balance_status"] = "well_balanced"
            elif load_variance < 400:
                load_analysis["load_balance_status"] = "moderately_balanced"
            else:
                load_analysis["load_balance_status"] = "poorly_balanced"
                load_analysis["rebalancing_recommendations"].append(
                    "Consider rebalancing agents across nodes to improve load distribution"
                )
        
        return load_analysis

    def _analyze_agent_distribution(self, nodes_data: dict, query) -> dict:
        """Analyze agent distribution across nodes."""
        distribution_analysis = {
            "total_agents": 0,
            "distribution_balance": "unknown",
            "node_distributions": {},
            "distribution_recommendations": []
        }
        
        node_agent_counts = []
        total_agents = 0
        
        for node_name, agent_data in nodes_data["agent_distribution"].items():
            if "error" not in agent_data:
                node_total = agent_data.get("total_agents", 0)
                active_agents = agent_data.get("active_agents", 0)
                
                distribution_analysis["node_distributions"][node_name] = {
                    "total_agents": node_total,
                    "active_agents": active_agents,
                    "disconnected_agents": agent_data.get("disconnected_agents", 0),
                    "activity_ratio": round((active_agents / node_total * 100) if node_total > 0 else 0, 2)
                }
                
                node_agent_counts.append(node_total)
                total_agents += node_total
        
        distribution_analysis["total_agents"] = total_agents
        
        # Analyze distribution balance
        if node_agent_counts and len(node_agent_counts) > 1:
            avg_agents = sum(node_agent_counts) / len(node_agent_counts)
            max_deviation = max(abs(count - avg_agents) for count in node_agent_counts)
            deviation_percent = (max_deviation / avg_agents * 100) if avg_agents > 0 else 0
            
            if deviation_percent < 20:
                distribution_analysis["distribution_balance"] = "excellent"
            elif deviation_percent < 40:
                distribution_analysis["distribution_balance"] = "good"
            elif deviation_percent < 60:
                distribution_analysis["distribution_balance"] = "uneven"
                distribution_analysis["distribution_recommendations"].append(
                    "Consider redistributing agents for better load balancing"
                )
            else:
                distribution_analysis["distribution_balance"] = "severely_uneven"
                distribution_analysis["distribution_recommendations"].append(
                    "Urgent: Redistribute agents to prevent node overload"
                )
        
        return distribution_analysis

    def _generate_nodes_recommendations(self, nodes_data: dict, query) -> list:
        """Generate actionable recommendations for cluster nodes."""
        recommendations = []
        
        # Performance-based recommendations
        for node_name, metrics in nodes_data.get("performance_metrics", {}).items():
            if "error" not in metrics:
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                if cpu_usage > 85:
                    recommendations.append({
                        "type": "performance",
                        "node": node_name,
                        "priority": "high",
                        "recommendation": f"Reduce CPU load on {node_name} (current: {cpu_usage}%)",
                        "actions": ["Scale down agent load", "Optimize process configuration", "Consider hardware upgrade"]
                    })
                
                if memory_usage > 90:
                    recommendations.append({
                        "type": "performance",
                        "node": node_name,
                        "priority": "critical",
                        "recommendation": f"Address memory pressure on {node_name} (current: {memory_usage}%)",
                        "actions": ["Increase memory allocation", "Optimize memory usage", "Restart services if needed"]
                    })
        
        # Sync-based recommendations
        for node_name, sync_status in nodes_data.get("sync_status", {}).items():
            if "error" not in sync_status:
                sync_lag = sync_status.get("sync_lag_seconds", 0)
                files_to_sync = sync_status.get("files_to_sync", 0)
                
                if sync_lag > query.sync_lag_threshold * 2:
                    recommendations.append({
                        "type": "synchronization",
                        "node": node_name,
                        "priority": "high",
                        "recommendation": f"Address synchronization lag on {node_name} ({sync_lag}s)",
                        "actions": ["Check network connectivity", "Review sync configuration", "Monitor cluster health"]
                    })
                
                if files_to_sync > 500:
                    recommendations.append({
                        "type": "synchronization",
                        "node": node_name,
                        "priority": "medium",
                        "recommendation": f"Large sync backlog on {node_name} ({files_to_sync} files)",
                        "actions": ["Monitor sync progress", "Check for disk space", "Consider sync optimization"]
                    })
        
        return recommendations

    def _generate_nodes_alerts(self, nodes_data: dict, query) -> list:
        """Generate alerts for cluster nodes issues."""
        alerts = []
        
        # Node status alerts
        for node in nodes_data["cluster_nodes"]:
            node_name = node.get("name", "unknown")
            status = node.get("status", "unknown")
            
            if status != "active":
                alerts.append({
                    "type": "node_status",
                    "node": node_name,
                    "severity": "critical" if status == "disconnected" else "high",
                    "message": f"Node {node_name} is {status}",
                    "timestamp": datetime.utcnow().isoformat()
                })
        
        # Performance alerts
        for node_name, metrics in nodes_data.get("performance_metrics", {}).items():
            if "error" not in metrics:
                cpu_usage = metrics.get("cpu_usage", 0)
                memory_usage = metrics.get("memory_usage", 0)
                
                if cpu_usage > query.performance_threshold_cpu:
                    alerts.append({
                        "type": "high_cpu",
                        "node": node_name,
                        "severity": "critical" if cpu_usage > 95 else "high",
                        "message": f"High CPU usage on {node_name}: {cpu_usage}%",
                        "value": cpu_usage,
                        "timestamp": datetime.utcnow().isoformat()
                    })
                
                if memory_usage > query.performance_threshold_memory:
                    alerts.append({
                        "type": "high_memory",
                        "node": node_name,
                        "severity": "critical" if memory_usage > 95 else "high",
                        "message": f"High memory usage on {node_name}: {memory_usage}%",
                        "value": memory_usage,
                        "timestamp": datetime.utcnow().isoformat()
                    })
        
        return alerts

    def _calculate_nodes_quality_indicators(self, nodes_data: dict) -> dict:
        """Calculate quality indicators for the cluster nodes analysis."""
        total_nodes = len(nodes_data["cluster_nodes"])
        successful_collections = 0
        total_collections = 0
        
        # Count successful data collections
        for collection_type in ["node_details", "performance_metrics", "sync_status", "load_metrics", "agent_distribution"]:
            collection_data = nodes_data.get(collection_type, {})
            for node_name, data in collection_data.items():
                total_collections += 1
                if "error" not in data:
                    successful_collections += 1
        
        # Calculate success rate
        success_rate = (successful_collections / total_collections * 100) if total_collections > 0 else 0
        
        # Calculate data completeness
        expected_data_points = total_nodes * 5  # 5 types of data per node
        actual_data_points = len(nodes_data.get("node_details", {})) + \
                           len(nodes_data.get("performance_metrics", {})) + \
                           len(nodes_data.get("sync_status", {})) + \
                           len(nodes_data.get("load_metrics", {})) + \
                           len(nodes_data.get("agent_distribution", {}))
        
        data_completeness = (actual_data_points / expected_data_points * 100) if expected_data_points > 0 else 0
        
        # Determine analysis confidence
        if success_rate >= 90 and data_completeness >= 80:
            confidence = "high"
        elif success_rate >= 70 and data_completeness >= 60:
            confidence = "medium"
        else:
            confidence = "low"
        
        return {
            "data_completeness": round(data_completeness, 2),
            "collection_success_rate": round(success_rate, 2),
            "analysis_confidence": confidence,
            "total_nodes_analyzed": total_nodes,
            "collection_errors": len(nodes_data.get("collection_errors", []))
        }

    def _calculate_performance_score(self, metrics: dict) -> float:
        """Calculate a performance score for a node."""
        cpu_usage = metrics.get("cpu_usage", 0)
        memory_usage = metrics.get("memory_usage", 0)
        
        # Simple scoring: 100 - weighted average of usage percentages
        score = 100 - (cpu_usage * 0.4 + memory_usage * 0.6)
        return max(0, round(score, 2))

    def _determine_performance_status(self, cpu_usage: float, memory_usage: float, query) -> str:
        """Determine performance status based on thresholds."""
        if cpu_usage > query.performance_threshold_cpu or memory_usage > query.performance_threshold_memory:
            return "concerning"
        elif cpu_usage > 70 or memory_usage > 75:
            return "moderate"
        else:
            return "good"


    async def _perform_forensic_log_search(self, pattern, log_types: List[str], start_dt: datetime, 
                                          end_dt: datetime, context_lines: int, max_results: int,
                                          include_forensics: bool, correlation_window: int) -> Dict[str, Any]:
        """Enhanced log search with forensic capabilities and correlation analysis."""
        search_results = {
            "matches": [],
            "total_files_searched": 0,
            "total_lines_searched": 0,
            "search_metadata": {
                "pattern": pattern.pattern,
                "start_time": start_dt.isoformat(),
                "end_time": end_dt.isoformat(),
                "log_types": log_types,
                "context_lines": context_lines
            },
            "forensic_data": {
                "timeline_events": [],
                "correlations": [],
                "ioc_matches": [],
                "evidence_chains": []
            } if include_forensics else None
        }
        
        try:
            # Mock implementation - in real scenario would search actual log files
            # This maintains backward compatibility while adding forensic features
            
            # Simulate log file search across different log types
            for log_type in log_types:
                if log_type == "all":
                    search_log_types = ["ossec", "api", "cluster", "analysisd", "remoted"]
                else:
                    search_log_types = [log_type]
                
                for search_type in search_log_types:
                    # Mock log entries with forensic data
                    mock_entries = self._generate_mock_log_entries(
                        search_type, pattern, start_dt, end_dt, max_results // len(search_log_types)
                    )
                    
                    search_results["total_files_searched"] += 1
                    search_results["total_lines_searched"] += len(mock_entries) * 100  # Mock line count
                    
                    for entry in mock_entries:
                        # Add context lines
                        entry["context"] = self._get_log_context(entry, context_lines)
                        search_results["matches"].append(entry)
                        
                        # Add to timeline for forensic analysis
                        if include_forensics:
                            search_results["forensic_data"]["timeline_events"].append({
                                "timestamp": entry["timestamp"],
                                "event_type": entry["log_type"],
                                "event_data": entry["message"],
                                "severity": entry.get("severity", "info"),
                                "source_file": entry["file_path"],
                                "line_number": entry["line_number"]
                            })
            
            # Limit results
            search_results["matches"] = search_results["matches"][:max_results]
            
            # Perform forensic analysis if requested
            if include_forensics:
                search_results["forensic_data"] = await self._enhance_with_forensic_analysis(
                    search_results["forensic_data"], correlation_window
                )
            
        except Exception as e:
            self.logger.error(f"Error in forensic log search: {str(e)}")
            search_results["error"] = str(e)
        
        return search_results
    
    def _generate_mock_log_entries(self, log_type: str, pattern, start_dt: datetime, 
                                  end_dt: datetime, max_entries: int) -> List[Dict[str, Any]]:
        """Generate mock log entries for demonstration (replace with actual log parsing)."""
        entries = []
        
        # Sample log patterns based on the search pattern
        sample_logs = {
            "ossec": [
                "2024-01-15 10:30:45 ossec-analysisd: INFO: Alert triggered for rule 5712",
                "2024-01-15 10:31:02 ossec-remoted: WARNING: Agent disconnection detected",
                "2024-01-15 10:31:15 ossec-logcollector: INFO: Started monitoring new log file",
                "2024-01-15 10:32:30 ossec-analysisd: ERROR: Failed to decode log entry"
            ],
            "api": [
                "2024-01-15 10:30:45 wazuh-api: INFO: Authentication successful for user admin",
                "2024-01-15 10:31:02 wazuh-api: WARNING: Rate limit exceeded for IP 192.168.1.100",
                "2024-01-15 10:31:15 wazuh-api: ERROR: Database connection failed",
                "2024-01-15 10:32:30 wazuh-api: INFO: Agent configuration updated"
            ],
            "cluster": [
                "2024-01-15 10:30:45 wazuh-clusterd: INFO: Node synchronization started",
                "2024-01-15 10:31:02 wazuh-clusterd: WARNING: Sync lag detected on worker node",
                "2024-01-15 10:31:15 wazuh-clusterd: INFO: File integrity check completed",
                "2024-01-15 10:32:30 wazuh-clusterd: ERROR: Communication failure with worker-02"
            ]
        }
        
        log_samples = sample_logs.get(log_type, sample_logs["ossec"])
        
        for i, log_line in enumerate(log_samples[:max_entries]):
            if pattern.search(log_line):
                entries.append({
                    "timestamp": (start_dt + timedelta(minutes=i)).isoformat(),
                    "log_type": log_type,
                    "message": log_line,
                    "file_path": str(get_wazuh_log_path(log_type)),
                    "line_number": 1000 + i,
                    "severity": self._extract_severity(log_line),
                    "source_component": log_type,
                    "match_groups": pattern.findall(log_line)
                })
        
        return entries
    
    def _get_log_context(self, entry: Dict[str, Any], context_lines: int) -> Dict[str, Any]:
        """Get context lines around a log match."""
        # Mock context - in real implementation would read actual file context
        return {
            "before": [f"Context line {i} before match" for i in range(context_lines)],
            "after": [f"Context line {i} after match" for i in range(context_lines)],
            "line_numbers": {
                "start": entry["line_number"] - context_lines,
                "end": entry["line_number"] + context_lines
            }
        }
    
    def _extract_severity(self, log_line: str) -> str:
        """Extract severity level from log line."""
        if "ERROR" in log_line:
            return "error"
        elif "WARNING" in log_line:
            return "warning"
        elif "INFO" in log_line:
            return "info"
        else:
            return "debug"
    
    async def _enhance_with_forensic_analysis(self, forensic_data: Dict[str, Any], 
                                            correlation_window: int) -> Dict[str, Any]:
        """Enhance forensic data with correlations, IoC matching, and evidence chains."""
        
        # Sort timeline events by timestamp
        timeline_events = sorted(forensic_data["timeline_events"], 
                               key=lambda x: x["timestamp"])
        
        # Correlation analysis - find events within correlation window
        correlations = []
        for i, event in enumerate(timeline_events):
            event_time = datetime.fromisoformat(event["timestamp"])
            related_events = []
            
            # Look for events within correlation window
            for j, other_event in enumerate(timeline_events):
                if i != j:
                    other_time = datetime.fromisoformat(other_event["timestamp"])
                    time_diff = abs((event_time - other_time).total_seconds() / 60)
                    
                    if time_diff <= correlation_window:
                        related_events.append({
                            "event_index": j,
                            "time_difference_minutes": time_diff,
                            "correlation_type": self._determine_correlation_type(event, other_event),
                            "confidence_score": self._calculate_correlation_confidence(event, other_event)
                        })
            
            if related_events:
                correlations.append({
                    "primary_event": event,
                    "related_events": related_events,
                    "correlation_strength": len(related_events),
                    "analysis": self._analyze_event_correlation(event, related_events)
                })
        
        # IoC (Indicators of Compromise) matching
        ioc_matches = []
        for event in timeline_events:
            iocs = self._detect_iocs(event)
            if iocs:
                ioc_matches.append({
                    "event": event,
                    "indicators": iocs,
                    "threat_level": self._assess_threat_level(iocs),
                    "recommended_actions": self._get_ioc_recommendations(iocs)
                })
        
        # Evidence chain construction
        evidence_chains = self._build_evidence_chains(timeline_events, correlations)
        
        forensic_data.update({
            "correlations": correlations,
            "ioc_matches": ioc_matches,
            "evidence_chains": evidence_chains,
            "attack_timeline": self._construct_attack_timeline(timeline_events, correlations),
            "forensic_summary": self._generate_forensic_summary(timeline_events, correlations, ioc_matches)
        })
        
        return forensic_data
    
    def _determine_correlation_type(self, event1: Dict[str, Any], event2: Dict[str, Any]) -> str:
        """Determine the type of correlation between two events."""
        if event1["event_type"] == event2["event_type"]:
            return "same_component"
        elif "error" in event1.get("severity", "") and "warning" in event2.get("severity", ""):
            return "escalation"
        elif "auth" in event1["event_data"].lower() and "auth" in event2["event_data"].lower():
            return "authentication_sequence"
        elif "fail" in event1["event_data"].lower() and "success" in event2["event_data"].lower():
            return "retry_sequence"
        else:
            return "temporal"
    
    def _calculate_correlation_confidence(self, event1: Dict[str, Any], event2: Dict[str, Any]) -> float:
        """Calculate confidence score for event correlation."""
        confidence = 0.5  # Base confidence
        
        # Increase confidence for same component
        if event1["event_type"] == event2["event_type"]:
            confidence += 0.2
        
        # Increase confidence for severity escalation
        severity_map = {"info": 1, "warning": 2, "error": 3}
        if (severity_map.get(event1.get("severity"), 1) < 
            severity_map.get(event2.get("severity"), 1)):
            confidence += 0.15
        
        # Increase confidence for keyword matches
        keywords1 = set(event1["event_data"].lower().split())
        keywords2 = set(event2["event_data"].lower().split())
        overlap = len(keywords1.intersection(keywords2))
        if overlap > 0:
            confidence += min(0.2, overlap * 0.05)
        
        return min(1.0, confidence)
    
    def _analyze_event_correlation(self, primary_event: Dict[str, Any], 
                                 related_events: List[Dict[str, Any]]) -> str:
        """Analyze the significance of event correlations."""
        if len(related_events) >= 3:
            return "High correlation - potential attack sequence or system cascade failure"
        elif len(related_events) == 2:
            return "Medium correlation - related system events detected"
        else:
            return "Low correlation - single related event"
    
    def _detect_iocs(self, event: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect Indicators of Compromise in log events."""
        indicators = []
        event_data = event["event_data"].lower()
        
        # IP address patterns
        import re
        ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
        ips = re.findall(ip_pattern, event_data)
        for ip in ips:
            if not ip.startswith(('192.168.', '10.', '172.')):  # External IPs
                indicators.append({
                    "type": "ip_address",
                    "value": ip,
                    "threat_level": "medium",
                    "description": "External IP address detected in logs"
                })
        
        # Suspicious keywords
        suspicious_keywords = {
            "fail": {"type": "authentication_failure", "threat_level": "low"},
            "denied": {"type": "access_denied", "threat_level": "medium"},
            "attack": {"type": "attack_keyword", "threat_level": "high"},
            "malware": {"type": "malware_keyword", "threat_level": "high"},
            "breach": {"type": "breach_keyword", "threat_level": "critical"}
        }
        
        for keyword, info in suspicious_keywords.items():
            if keyword in event_data:
                indicators.append({
                    "type": info["type"],
                    "value": keyword,
                    "threat_level": info["threat_level"],
                    "description": f"Suspicious keyword '{keyword}' detected"
                })
        
        return indicators
    
    def _assess_threat_level(self, indicators: List[Dict[str, Any]]) -> str:
        """Assess overall threat level based on indicators."""
        if not indicators:
            return "none"
        
        levels = {"none": 0, "low": 1, "medium": 2, "high": 3, "critical": 4}
        max_level = max(levels.get(ioc["threat_level"], 0) for ioc in indicators)
        
        for level, value in levels.items():
            if value == max_level:
                return level
        return "low"
    
    def _get_ioc_recommendations(self, indicators: List[Dict[str, Any]]) -> List[str]:
        """Get recommended actions based on detected IoCs."""
        recommendations = []
        
        for ioc in indicators:
            if ioc["type"] == "ip_address":
                recommendations.append(f"Investigate IP {ioc['value']} for malicious activity")
            elif ioc["type"] == "authentication_failure":
                recommendations.append("Review authentication logs for brute force attempts")
            elif ioc["threat_level"] in ["high", "critical"]:
                recommendations.append(f"Immediate investigation required for {ioc['type']}")
        
        return list(set(recommendations))  # Remove duplicates
    
    def _build_evidence_chains(self, timeline_events: List[Dict[str, Any]], 
                             correlations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Build evidence chains from correlated events."""
        evidence_chains = []
        processed_events = set()
        
        for correlation in correlations:
            primary_event = correlation["primary_event"]
            if primary_event["timestamp"] in processed_events:
                continue
            
            chain = {
                "chain_id": f"chain_{len(evidence_chains) + 1}",
                "start_time": primary_event["timestamp"],
                "events": [primary_event],
                "chain_type": "investigation",
                "confidence": correlation.get("correlation_strength", 1) / 10.0
            }
            
            # Add related events to chain
            for related in correlation["related_events"]:
                if related["confidence_score"] > 0.6:
                    event_index = related["event_index"]
                    if event_index < len(timeline_events):
                        chain["events"].append(timeline_events[event_index])
            
            # Determine chain type
            if len(chain["events"]) >= 3:
                chain["chain_type"] = "attack_sequence"
            elif any("error" in e.get("severity", "") for e in chain["events"]):
                chain["chain_type"] = "incident_escalation"
            
            evidence_chains.append(chain)
            processed_events.update(e["timestamp"] for e in chain["events"])
        
        return evidence_chains
    
    def _construct_attack_timeline(self, timeline_events: List[Dict[str, Any]], 
                                 correlations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Construct a comprehensive attack timeline."""
        return {
            "timeline_start": timeline_events[0]["timestamp"] if timeline_events else None,
            "timeline_end": timeline_events[-1]["timestamp"] if timeline_events else None,
            "total_events": len(timeline_events),
            "correlated_sequences": len(correlations),
            "attack_phases": self._identify_attack_phases(timeline_events, correlations),
            "critical_moments": self._identify_critical_moments(timeline_events)
        }
    
    def _identify_attack_phases(self, timeline_events: List[Dict[str, Any]], 
                              correlations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify potential attack phases in the timeline."""
        phases = []
        
        # Simple phase detection based on event patterns
        reconnaissance_keywords = ["scan", "probe", "enum"]
        initial_access_keywords = ["auth", "login", "access"]
        escalation_keywords = ["error", "fail", "denied"]
        
        for i, event in enumerate(timeline_events):
            event_data = event["event_data"].lower()
            
            if any(keyword in event_data for keyword in reconnaissance_keywords):
                phases.append({
                    "phase": "reconnaissance",
                    "timestamp": event["timestamp"],
                    "evidence": event["event_data"],
                    "confidence": 0.7
                })
            elif any(keyword in event_data for keyword in initial_access_keywords):
                phases.append({
                    "phase": "initial_access",
                    "timestamp": event["timestamp"],
                    "evidence": event["event_data"],
                    "confidence": 0.8
                })
            elif any(keyword in event_data for keyword in escalation_keywords):
                phases.append({
                    "phase": "privilege_escalation",
                    "timestamp": event["timestamp"],
                    "evidence": event["event_data"],
                    "confidence": 0.6
                })
        
        return phases
    
    def _identify_critical_moments(self, timeline_events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify critical moments in the timeline."""
        critical_moments = []
        
        for event in timeline_events:
            if event.get("severity") in ["error"]:
                critical_moments.append({
                    "timestamp": event["timestamp"],
                    "event": event["event_data"],
                    "criticality": "high",
                    "reason": "Error event detected"
                })
            elif "fail" in event["event_data"].lower():
                critical_moments.append({
                    "timestamp": event["timestamp"],
                    "event": event["event_data"],
                    "criticality": "medium", 
                    "reason": "Failure event detected"
                })
        
        return critical_moments
    
    def _generate_forensic_summary(self, timeline_events: List[Dict[str, Any]], 
                                 correlations: List[Dict[str, Any]], 
                                 ioc_matches: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate a forensic analysis summary."""
        return {
            "total_events_analyzed": len(timeline_events),
            "correlations_found": len(correlations),
            "indicators_detected": len(ioc_matches),
            "threat_assessment": self._assess_overall_threat(ioc_matches),
            "investigation_priority": self._determine_investigation_priority(correlations, ioc_matches),
            "key_findings": self._extract_key_findings(timeline_events, correlations, ioc_matches),
            "recommended_actions": self._generate_investigation_recommendations(correlations, ioc_matches)
        }
    
    def _assess_overall_threat(self, ioc_matches: List[Dict[str, Any]]) -> str:
        """Assess overall threat level from all IoC matches."""
        if not ioc_matches:
            return "minimal"
        
        threat_levels = [match["threat_level"] for match in ioc_matches]
        if "critical" in threat_levels:
            return "critical"
        elif "high" in threat_levels:
            return "high"
        elif "medium" in threat_levels:
            return "medium"
        else:
            return "low"
    
    def _determine_investigation_priority(self, correlations: List[Dict[str, Any]], 
                                        ioc_matches: List[Dict[str, Any]]) -> str:
        """Determine investigation priority."""
        if len(ioc_matches) > 3 or len(correlations) > 5:
            return "immediate"
        elif len(ioc_matches) > 1 or len(correlations) > 2:
            return "high"
        elif len(ioc_matches) > 0 or len(correlations) > 0:
            return "medium"
        else:
            return "low"
    
    def _extract_key_findings(self, timeline_events: List[Dict[str, Any]], 
                            correlations: List[Dict[str, Any]], 
                            ioc_matches: List[Dict[str, Any]]) -> List[str]:
        """Extract key findings from forensic analysis."""
        findings = []
        
        if timeline_events:
            findings.append(f"Analyzed {len(timeline_events)} log events across timeframe")
        
        if correlations:
            findings.append(f"Found {len(correlations)} correlated event sequences")
        
        if ioc_matches:
            findings.append(f"Detected {len(ioc_matches)} indicators of compromise")
        
        # Analyze event patterns
        error_count = sum(1 for e in timeline_events if e.get("severity") == "error")
        if error_count > 0:
            findings.append(f"Identified {error_count} error events requiring investigation")
        
        return findings
    
    def _generate_investigation_recommendations(self, correlations: List[Dict[str, Any]], 
                                              ioc_matches: List[Dict[str, Any]]) -> List[str]:
        """Generate investigation recommendations."""
        recommendations = []
        
        if ioc_matches:
            recommendations.append("Investigate all detected indicators of compromise")
            recommendations.append("Cross-reference IoCs with threat intelligence feeds")
        
        if correlations:
            recommendations.append("Analyze correlated events for attack pattern identification")
            recommendations.append("Review system logs around correlated event timeframes")
        
        if len(correlations) > 3:
            recommendations.append("Consider this a potential security incident requiring immediate response")
        
        recommendations.append("Preserve log evidence for potential forensic analysis")
        recommendations.append("Monitor for similar patterns in future log entries")
        
        return recommendations
    
    async def _generate_forensic_analysis(self, search_results: Dict[str, Any], search_pattern: str,
                                        start_dt: datetime, end_dt: datetime, include_forensics: bool,
                                        correlation_window: int, start_time: datetime) -> Dict[str, Any]:
        """Generate comprehensive forensic analysis report."""
        current_time = datetime.utcnow()
        
        forensic_report = {
            "search_summary": {
                "pattern": search_pattern,
                "timeframe": {
                    "start": start_dt.isoformat(),
                    "end": end_dt.isoformat(),
                    "duration_hours": (end_dt - start_dt).total_seconds() / 3600
                },
                "results_found": len(search_results["matches"]),
                "files_searched": search_results["total_files_searched"],
                "lines_searched": search_results["total_lines_searched"]
            },
            "matches": search_results["matches"],
            "forensic_analysis": search_results.get("forensic_data") if include_forensics else None,
            "search_metadata": search_results["search_metadata"],
            "analysis_metadata": {
                "analysis_timestamp": current_time.isoformat(),
                "processing_time_seconds": (current_time - start_time).total_seconds(),
                "correlation_window_minutes": correlation_window,
                "forensic_features_enabled": include_forensics
            }
        }
        
        # Add executive summary
        if include_forensics and search_results.get("forensic_data"):
            forensic_data = search_results["forensic_data"]
            forensic_report["executive_summary"] = {
                "threat_level": forensic_data["forensic_summary"]["threat_assessment"],
                "investigation_priority": forensic_data["forensic_summary"]["investigation_priority"],
                "key_findings": forensic_data["forensic_summary"]["key_findings"],
                "immediate_actions": forensic_data["forensic_summary"]["recommended_actions"][:3]
            }
        
        return forensic_report

    async def _perform_error_log_analysis(self, error_levels: List[str], start_dt: datetime, 
                                         end_dt: datetime, component_filter: List[str],
                                         pattern_filter: str, max_errors: int, include_analysis: bool,
                                         include_trends: bool, correlation_analysis: bool) -> Dict[str, Any]:
        """Perform comprehensive error log analysis with pattern detection and trending."""
        try:
            # Collect error logs from different sources
            error_logs = await self._collect_error_logs(
                error_levels, start_dt, end_dt, component_filter, pattern_filter, max_errors
            )
            
            analysis_result = {
                "raw_errors": error_logs,
                "error_summary": self._analyze_error_patterns(error_logs),
                "component_analysis": self._analyze_error_components(error_logs),
                "severity_analysis": self._analyze_error_severity(error_logs, error_levels)
            }
            
            if include_analysis:
                analysis_result["root_cause_analysis"] = self._perform_root_cause_analysis(error_logs)
                analysis_result["impact_assessment"] = self._assess_error_impact(error_logs)
                analysis_result["remediation_suggestions"] = self._generate_remediation_suggestions(error_logs)
            
            if include_trends:
                analysis_result["trend_analysis"] = self._analyze_error_trends(error_logs, start_dt, end_dt)
                analysis_result["predictive_insights"] = self._generate_predictive_insights(error_logs)
            
            if correlation_analysis:
                analysis_result["correlation_analysis"] = await self._perform_error_correlation(error_logs)
                analysis_result["cascade_detection"] = self._detect_error_cascades(error_logs)
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"Error in error log analysis: {str(e)}")
            return {"error": f"Analysis failed: {str(e)}", "raw_errors": []}

    async def _collect_error_logs(self, error_levels: List[str], start_dt: datetime, 
                                 end_dt: datetime, component_filter: List[str],
                                 pattern_filter: str, max_errors: int) -> List[Dict[str, Any]]:
        """Collect error logs from various Wazuh components."""
        all_errors = []
        
        try:
            # Common log paths for Wazuh manager - platform-agnostic
            wazuh_paths = get_wazuh_paths()
            log_paths = [
                str(wazuh_paths["ossec_log"]),
                str(wazuh_paths["api_log"]), 
                str(wazuh_paths["cluster_log"]),
                str(wazuh_paths["modulesd_log"]),
                str(wazuh_paths["authd_log"]),
                str(wazuh_paths["monitord_log"]),
                str(wazuh_paths["remoted_log"])
            ]
            
            error_pattern = self._build_error_search_pattern(error_levels, pattern_filter)
            
            for log_path in log_paths:
                try:
                    # Extract component from log path
                    component = self._extract_component_from_path(log_path)
                    
                    # Skip if component filter specified and doesn't match
                    if component_filter and component not in component_filter:
                        continue
                    
                    # Read and parse log entries
                    log_entries = await self._parse_log_file(
                        log_path, error_pattern, start_dt, end_dt, max_errors
                    )
                    
                    # Enhance entries with component and classification
                    for entry in log_entries:
                        entry["component"] = component
                        entry["log_source"] = log_path
                        entry["error_classification"] = self._classify_error(entry)
                        all_errors.append(entry)
                        
                except Exception as e:
                    self.logger.warning(f"Could not read log {log_path}: {str(e)}")
                    continue
            
            # Sort by timestamp and limit results
            all_errors.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
            return all_errors[:max_errors]
            
        except Exception as e:
            self.logger.error(f"Error collecting error logs: {str(e)}")
            return []

    def _build_error_search_pattern(self, error_levels: List[str], pattern_filter: str) -> str:
        """Build regex pattern for error log search."""
        level_pattern = "|".join(error_levels)
        base_pattern = f"({level_pattern})"
        
        if pattern_filter:
            # Combine level filter with custom pattern
            return f"{base_pattern}.*{pattern_filter}"
        
        return base_pattern

    def _extract_component_from_path(self, log_path: str) -> str:
        """Extract component name from log file path."""
        path_mapping = {
            "ossec.log": "core",
            "api.log": "api",
            "cluster.log": "cluster", 
            "wazuh-modulesd.log": "modules",
            "wazuh-authd.log": "authentication",
            "wazuh-monitord.log": "monitor",
            "wazuh-remoted.log": "remote"
        }
        
        for filename, component in path_mapping.items():
            if filename in log_path:
                return component
        
        return "unknown"

    async def _parse_log_file(self, log_path: str, pattern: str, start_dt: datetime, 
                            end_dt: datetime, max_entries: int) -> List[Dict[str, Any]]:
        """Parse log file and extract matching entries."""
        entries = []
        
        try:
            # This is a simplified implementation - in production, you'd need
            # to handle log rotation, compression, and different log formats
            import re
            
            # For now, we'll simulate log entries since we can't directly read files
            # In a real implementation, you'd read the actual log files
            simulated_entries = self._generate_simulated_error_entries(
                pattern, start_dt, end_dt, max_entries
            )
            
            return simulated_entries
            
        except Exception as e:
            self.logger.error(f"Error parsing log file {log_path}: {str(e)}")
            return []

    def _generate_simulated_error_entries(self, pattern: str, start_dt: datetime, 
                                        end_dt: datetime, max_entries: int) -> List[Dict[str, Any]]:
        """Generate simulated error entries for demonstration."""
        import random
        
        error_types = [
            "Authentication failed for user",
            "Database connection timeout", 
            "API request rate limit exceeded",
            "Cluster synchronization failed",
            "Agent connection lost",
            "Memory allocation failed",
            "Configuration parsing error",
            "SSL certificate validation failed"
        ]
        
        components = ["core", "api", "cluster", "modules", "authentication", "monitor", "remote"]
        severities = ["ERROR", "CRITICAL", "WARNING"]
        
        entries = []
        time_span = (end_dt - start_dt).total_seconds()
        
        for i in range(min(50, max_entries)):  # Generate up to 50 simulated errors
            # Random timestamp within range
            random_offset = random.uniform(0, time_span)
            timestamp = start_dt + timedelta(seconds=random_offset)
            
            error_type = random.choice(error_types)
            component = random.choice(components)
            severity = random.choice(severities)
            
            entry = {
                "timestamp": timestamp.isoformat(),
                "level": severity,
                "message": f"{error_type} in {component} component",
                "raw_line": f"[{timestamp.strftime('%Y-%m-%d %H:%M:%S')}] {severity}: {error_type}",
                "line_number": i + 1,
                "parsed_data": {
                    "error_code": f"E{random.randint(1000, 9999)}",
                    "process_id": random.randint(1000, 9999),
                    "thread_id": random.randint(100, 999)
                }
            }
            
            entries.append(entry)
        
        return sorted(entries, key=lambda x: x["timestamp"], reverse=True)

    def _classify_error(self, entry: Dict[str, Any]) -> Dict[str, Any]:
        """Classify error based on content and patterns."""
        message = entry.get("message", "").lower()
        level = entry.get("level", "")
        
        # Security-related classification
        security_keywords = ["authentication", "authorization", "ssl", "certificate", "login", "access denied"]
        is_security = any(keyword in message for keyword in security_keywords)
        
        # Performance-related classification  
        performance_keywords = ["timeout", "memory", "cpu", "slow", "bottleneck", "latency"]
        is_performance = any(keyword in message for keyword in performance_keywords)
        
        # Connectivity-related classification
        connectivity_keywords = ["connection", "network", "socket", "unreachable", "disconnect"]
        is_connectivity = any(keyword in message for keyword in connectivity_keywords)
        
        # Determine category
        categories = []
        if is_security:
            categories.append("security")
        if is_performance:
            categories.append("performance") 
        if is_connectivity:
            categories.append("connectivity")
        if not categories:
            categories.append("general")
        
        # Determine criticality
        criticality = "low"
        if level == "CRITICAL":
            criticality = "critical"
        elif level == "ERROR":
            criticality = "high" if is_security else "medium"
        elif level == "WARNING":
            criticality = "medium" if is_security else "low"
        
        return {
            "categories": categories,
            "criticality": criticality,
            "is_security_related": is_security,
            "is_performance_related": is_performance,
            "is_connectivity_related": is_connectivity,
            "requires_immediate_action": criticality in ["critical", "high"]
        }

    def _analyze_error_patterns(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze patterns in error logs."""
        if not error_logs:
            return {"total_errors": 0, "message": "No errors found in the specified time range"}
        
        # Count by level
        level_counts = {}
        for error in error_logs:
            level = error.get("level", "UNKNOWN")
            level_counts[level] = level_counts.get(level, 0) + 1
        
        # Count by component
        component_counts = {}
        for error in error_logs:
            component = error.get("component", "unknown")
            component_counts[component] = component_counts.get(component, 0) + 1
        
        # Find most common error messages
        message_counts = {}
        for error in error_logs:
            message = error.get("message", "")
            # Normalize message for counting (remove specifics like IDs, timestamps)
            normalized = self._normalize_error_message(message)
            message_counts[normalized] = message_counts.get(normalized, 0) + 1
        
        # Get top error patterns
        top_errors = sorted(message_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            "total_errors": len(error_logs),
            "level_distribution": level_counts,
            "component_distribution": component_counts,
            "top_error_patterns": [{"pattern": pattern, "count": count} for pattern, count in top_errors],
            "error_rate_analysis": self._calculate_error_rates(error_logs)
        }

    def _normalize_error_message(self, message: str) -> str:
        """Normalize error message for pattern analysis."""
        import re
        
        # Remove timestamps, IDs, IP addresses, etc.
        normalized = re.sub(r'\d{4}-\d{2}-\d{2}[\s\d:.-]*', '', message)  # Timestamps
        normalized = re.sub(r'\b\d+\b', 'N', normalized)  # Numbers
        normalized = re.sub(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', 'IP', normalized)  # IP addresses
        normalized = re.sub(r'[a-fA-F0-9]{8,}', 'ID', normalized)  # Hex IDs
        normalized = re.sub(r'\s+', ' ', normalized).strip()  # Multiple spaces
        
        return normalized

    def _calculate_error_rates(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate error rates and frequency analysis."""
        if not error_logs:
            return {"errors_per_hour": 0, "peak_hours": []}
        
        # Group errors by hour
        hourly_counts = {}
        for error in error_logs:
            try:
                timestamp = datetime.fromisoformat(error["timestamp"].replace('Z', '+00:00'))
                hour_key = timestamp.strftime('%Y-%m-%d %H:00')
                hourly_counts[hour_key] = hourly_counts.get(hour_key, 0) + 1
            except:
                continue
        
        if not hourly_counts:
            return {"errors_per_hour": 0, "peak_hours": []}
        
        # Calculate average
        total_hours = len(hourly_counts)
        total_errors = sum(hourly_counts.values())
        avg_per_hour = total_errors / total_hours if total_hours > 0 else 0
        
        # Find peak hours
        peak_hours = sorted(hourly_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "errors_per_hour": round(avg_per_hour, 2),
            "peak_hours": [{"hour": hour, "count": count} for hour, count in peak_hours],
            "total_unique_hours": total_hours
        }

    def _analyze_error_components(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze errors by component with health scoring."""
        component_analysis = {}
        
        for error in error_logs:
            component = error.get("component", "unknown")
            classification = error.get("error_classification", {})
            
            if component not in component_analysis:
                component_analysis[component] = {
                    "total_errors": 0,
                    "critical_errors": 0,
                    "security_errors": 0,
                    "performance_errors": 0,
                    "connectivity_errors": 0,
                    "error_types": {},
                    "health_score": 100
                }
            
            comp_data = component_analysis[component]
            comp_data["total_errors"] += 1
            
            # Count by criticality
            if classification.get("criticality") == "critical":
                comp_data["critical_errors"] += 1
            
            # Count by category
            categories = classification.get("categories", [])
            if "security" in categories:
                comp_data["security_errors"] += 1
            if "performance" in categories:
                comp_data["performance_errors"] += 1
            if "connectivity" in categories:
                comp_data["connectivity_errors"] += 1
            
            # Track error types
            error_type = self._extract_error_type(error.get("message", ""))
            comp_data["error_types"][error_type] = comp_data["error_types"].get(error_type, 0) + 1
        
        # Calculate health scores
        for component, data in component_analysis.items():
            health_score = self._calculate_component_health_score(data)
            data["health_score"] = health_score
            data["health_status"] = self._get_health_status(health_score)
        
        return component_analysis

    def _extract_error_type(self, message: str) -> str:
        """Extract error type from message."""
        message_lower = message.lower()
        
        if "authentication" in message_lower or "login" in message_lower:
            return "authentication"
        elif "connection" in message_lower or "network" in message_lower:
            return "connectivity"
        elif "timeout" in message_lower or "slow" in message_lower:
            return "performance"
        elif "memory" in message_lower or "cpu" in message_lower:
            return "resource"
        elif "configuration" in message_lower or "config" in message_lower:
            return "configuration"
        elif "database" in message_lower or "db" in message_lower:
            return "database"
        else:
            return "general"

    def _calculate_component_health_score(self, component_data: Dict[str, Any]) -> float:
        """Calculate health score for a component based on error patterns."""
        total_errors = component_data["total_errors"]
        critical_errors = component_data["critical_errors"]
        security_errors = component_data["security_errors"]
        
        if total_errors == 0:
            return 100.0
        
        # Start with base score
        score = 100.0
        
        # Penalize based on error count (logarithmic scale)
        import math
        error_penalty = min(50, 10 * math.log10(total_errors + 1))
        score -= error_penalty
        
        # Heavy penalty for critical errors
        critical_penalty = critical_errors * 15
        score -= critical_penalty
        
        # Additional penalty for security errors
        security_penalty = security_errors * 10
        score -= security_penalty
        
        return max(0.0, round(score, 1))

    def _get_health_status(self, score: float) -> str:
        """Convert health score to status description."""
        if score >= 90:
            return "Excellent"
        elif score >= 75:
            return "Good"
        elif score >= 60:
            return "Fair"
        elif score >= 40:
            return "Poor"
        else:
            return "Critical"

    def _analyze_error_severity(self, error_logs: List[Dict[str, Any]], 
                              error_levels: List[str]) -> Dict[str, Any]:
        """Analyze severity distribution and escalation patterns."""
        severity_data = {
            "level_distribution": {},
            "escalation_patterns": [],
            "severity_trends": {},
            "critical_error_analysis": {}
        }
        
        # Count by severity level
        for error in error_logs:
            level = error.get("level", "UNKNOWN")
            severity_data["level_distribution"][level] = severity_data["level_distribution"].get(level, 0) + 1
        
        # Analyze critical errors in detail
        critical_errors = [e for e in error_logs if e.get("level") == "CRITICAL"]
        if critical_errors:
            severity_data["critical_error_analysis"] = {
                "count": len(critical_errors),
                "components_affected": list(set(e.get("component", "unknown") for e in critical_errors)),
                "most_recent": critical_errors[0].get("timestamp") if critical_errors else None,
                "requires_immediate_attention": True
            }
        
        # Calculate severity score
        total_errors = len(error_logs)
        if total_errors > 0:
            critical_ratio = len(critical_errors) / total_errors
            error_ratio = len([e for e in error_logs if e.get("level") == "ERROR"]) / total_errors
            
            severity_score = 100 - (critical_ratio * 50 + error_ratio * 30)
            severity_data["overall_severity_score"] = max(0, round(severity_score, 1))
        else:
            severity_data["overall_severity_score"] = 100
        
        return severity_data

    def _perform_root_cause_analysis(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform root cause analysis on error patterns."""
        root_causes = {
            "identified_causes": [],
            "correlation_chains": [],
            "system_impact_analysis": {},
            "failure_modes": []
        }
        
        if not error_logs:
            return root_causes
        
        # Analyze temporal patterns
        temporal_analysis = self._analyze_temporal_error_patterns(error_logs)
        root_causes["temporal_patterns"] = temporal_analysis
        
        # Identify common failure scenarios
        failure_scenarios = self._identify_failure_scenarios(error_logs)
        root_causes["failure_scenarios"] = failure_scenarios
        
        # Analyze cascading failures
        cascade_analysis = self._analyze_error_cascades(error_logs)
        root_causes["cascade_analysis"] = cascade_analysis
        
        return root_causes

    def _analyze_temporal_error_patterns(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze temporal patterns in errors."""
        patterns = {
            "burst_detection": [],
            "periodic_patterns": [],
            "time_correlation": {}
        }
        
        # Sort errors by timestamp
        sorted_errors = sorted(error_logs, key=lambda x: x.get("timestamp", ""))
        
        # Detect error bursts (multiple errors in short time span)
        bursts = []
        current_burst = []
        
        for i, error in enumerate(sorted_errors):
            if i == 0:
                current_burst = [error]
                continue
            
            try:
                current_time = datetime.fromisoformat(error["timestamp"].replace('Z', '+00:00'))
                prev_time = datetime.fromisoformat(sorted_errors[i-1]["timestamp"].replace('Z', '+00:00'))
                
                # If errors are within 5 minutes, consider them part of a burst
                if (current_time - prev_time).total_seconds() <= 300:
                    current_burst.append(error)
                else:
                    if len(current_burst) >= 3:  # Burst threshold
                        bursts.append({
                            "start_time": current_burst[0]["timestamp"],
                            "end_time": current_burst[-1]["timestamp"], 
                            "error_count": len(current_burst),
                            "components_affected": list(set(e.get("component") for e in current_burst))
                        })
                    current_burst = [error]
            except:
                continue
        
        # Check last burst
        if len(current_burst) >= 3:
            bursts.append({
                "start_time": current_burst[0]["timestamp"],
                "end_time": current_burst[-1]["timestamp"],
                "error_count": len(current_burst),
                "components_affected": list(set(e.get("component") for e in current_burst))
            })
        
        patterns["burst_detection"] = bursts
        return patterns

    def _identify_failure_scenarios(self, error_logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify common failure scenarios from error patterns."""
        scenarios = []
        
        # Group errors by component and analyze patterns
        component_errors = {}
        for error in error_logs:
            component = error.get("component", "unknown")
            if component not in component_errors:
                component_errors[component] = []
            component_errors[component].append(error)
        
        # Analyze each component for failure patterns
        for component, errors in component_errors.items():
            if len(errors) >= 3:  # Minimum errors to identify a scenario
                scenario = {
                    "component": component,
                    "error_count": len(errors),
                    "failure_type": self._classify_failure_type(errors),
                    "impact_level": self._assess_failure_impact(errors),
                    "recommended_action": self._recommend_failure_action(errors)
                }
                scenarios.append(scenario)
        
        return scenarios

    def _classify_failure_type(self, errors: List[Dict[str, Any]]) -> str:
        """Classify the type of failure based on error patterns."""
        security_count = sum(1 for e in errors if e.get("error_classification", {}).get("is_security_related"))
        performance_count = sum(1 for e in errors if e.get("error_classification", {}).get("is_performance_related"))
        connectivity_count = sum(1 for e in errors if e.get("error_classification", {}).get("is_connectivity_related"))
        
        total = len(errors)
        if security_count / total >= 0.6:
            return "security_breach"
        elif performance_count / total >= 0.6:
            return "performance_degradation"
        elif connectivity_count / total >= 0.6:
            return "connectivity_failure"
        else:
            return "general_system_failure"

    def _assess_failure_impact(self, errors: List[Dict[str, Any]]) -> str:
        """Assess the impact level of failures."""
        critical_count = sum(1 for e in errors if e.get("level") == "CRITICAL")
        error_count = sum(1 for e in errors if e.get("level") == "ERROR")
        
        if critical_count > 0:
            return "high"
        elif error_count >= len(errors) * 0.7:
            return "medium"
        else:
            return "low"

    def _recommend_failure_action(self, errors: List[Dict[str, Any]]) -> str:
        """Recommend action based on failure analysis."""
        failure_type = self._classify_failure_type(errors)
        impact = self._assess_failure_impact(errors)
        
        if failure_type == "security_breach":
            return "Immediate security review and incident response required"
        elif failure_type == "performance_degradation" and impact == "high":
            return "Performance tuning and resource scaling needed"
        elif failure_type == "connectivity_failure":
            return "Network connectivity and configuration review required"
        else:
            return "System health check and log analysis recommended"

    def _analyze_error_cascades(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze cascading error patterns."""
        cascade_analysis = {
            "detected_cascades": [],
            "cascade_risk_score": 0,
            "component_dependencies": {}
        }
        
        # Sort by timestamp
        sorted_errors = sorted(error_logs, key=lambda x: x.get("timestamp", ""))
        
        # Look for cascading patterns (errors in multiple components within short timeframe)
        cascades = []
        window_minutes = 10  # Look for errors within 10 minutes
        
        for i, error in enumerate(sorted_errors):
            try:
                error_time = datetime.fromisoformat(error["timestamp"].replace('Z', '+00:00'))
                component = error.get("component")
                
                # Look ahead for related errors
                related_errors = [error]
                for j in range(i + 1, len(sorted_errors)):
                    next_error = sorted_errors[j]
                    next_time = datetime.fromisoformat(next_error["timestamp"].replace('Z', '+00:00'))
                    
                    if (next_time - error_time).total_seconds() <= window_minutes * 60:
                        if next_error.get("component") != component:  # Different component
                            related_errors.append(next_error)
                    else:
                        break
                
                # If we found errors across multiple components, it's a potential cascade
                if len(set(e.get("component") for e in related_errors)) >= 3:
                    cascade = {
                        "start_time": error["timestamp"],
                        "duration_minutes": window_minutes,
                        "components_affected": list(set(e.get("component") for e in related_errors)),
                        "error_count": len(related_errors),
                        "cascade_score": len(related_errors) * len(set(e.get("component") for e in related_errors))
                    }
                    cascades.append(cascade)
            except:
                continue
        
        cascade_analysis["detected_cascades"] = cascades
        if cascades:
            cascade_analysis["cascade_risk_score"] = max(c["cascade_score"] for c in cascades)
        
        return cascade_analysis

    def _assess_error_impact(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Assess the impact of errors on system operation."""
        impact_assessment = {
            "business_impact": "low",
            "operational_impact": "low", 
            "security_impact": "low",
            "affected_services": [],
            "user_impact_estimate": "minimal",
            "recovery_time_estimate": "immediate"
        }
        
        if not error_logs:
            return impact_assessment
        
        # Analyze security impact
        security_errors = [e for e in error_logs if e.get("error_classification", {}).get("is_security_related")]
        if security_errors:
            critical_security = len([e for e in security_errors if e.get("level") == "CRITICAL"])
            if critical_security > 0:
                impact_assessment["security_impact"] = "critical"
            elif len(security_errors) >= 5:
                impact_assessment["security_impact"] = "high"
            else:
                impact_assessment["security_impact"] = "medium"
        
        # Analyze operational impact
        critical_errors = [e for e in error_logs if e.get("level") == "CRITICAL"]
        components_affected = set(e.get("component") for e in error_logs)
        
        if len(critical_errors) > 0:
            impact_assessment["operational_impact"] = "critical"
            impact_assessment["recovery_time_estimate"] = "hours"
        elif len(components_affected) >= 4:
            impact_assessment["operational_impact"] = "high"
            impact_assessment["recovery_time_estimate"] = "30-60 minutes"
        elif len(error_logs) >= 20:
            impact_assessment["operational_impact"] = "medium"
            impact_assessment["recovery_time_estimate"] = "15-30 minutes"
        
        # Estimate business impact
        if impact_assessment["security_impact"] == "critical" or impact_assessment["operational_impact"] == "critical":
            impact_assessment["business_impact"] = "high"
            impact_assessment["user_impact_estimate"] = "significant service disruption"
        elif impact_assessment["operational_impact"] == "high":
            impact_assessment["business_impact"] = "medium"
            impact_assessment["user_impact_estimate"] = "partial service degradation"
        
        impact_assessment["affected_services"] = list(components_affected)
        
        return impact_assessment

    def _generate_remediation_suggestions(self, error_logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate specific remediation suggestions based on error analysis."""
        suggestions = []
        
        if not error_logs:
            return suggestions
        
        # Analyze error patterns to generate specific suggestions
        component_errors = {}
        for error in error_logs:
            component = error.get("component", "unknown")
            if component not in component_errors:
                component_errors[component] = []
            component_errors[component].append(error)
        
        # Generate component-specific suggestions
        for component, errors in component_errors.items():
            if len(errors) >= 3:  # Threshold for generating suggestions
                suggestion = self._create_component_remediation(component, errors)
                if suggestion:
                    suggestions.append(suggestion)
        
        # Add general recommendations based on overall error patterns
        general_suggestions = self._create_general_remediations(error_logs)
        suggestions.extend(general_suggestions)
        
        # Sort by priority
        suggestions.sort(key=lambda x: x.get("priority_score", 0), reverse=True)
        
        return suggestions[:10]  # Return top 10 suggestions

    def _create_component_remediation(self, component: str, errors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create remediation suggestion for a specific component."""
        error_count = len(errors)
        critical_count = len([e for e in errors if e.get("level") == "CRITICAL"])
        
        # Component-specific remediation mapping
        remediation_map = {
            "authentication": {
                "title": "Authentication Service Issues",
                "description": f"Detected {error_count} authentication errors with {critical_count} critical issues",
                "actions": [
                    "Review authentication service configuration",
                    "Check certificate validity and expiration",
                    "Verify LDAP/AD connectivity if applicable",
                    "Review failed login patterns for security threats"
                ],
                "priority_score": 90 if critical_count > 0 else 60
            },
            "cluster": {
                "title": "Cluster Communication Problems", 
                "description": f"Cluster showing {error_count} errors affecting synchronization",
                "actions": [
                    "Check cluster node connectivity",
                    "Verify cluster key configuration",
                    "Review network latency between nodes",
                    "Check disk space on cluster nodes"
                ],
                "priority_score": 85 if critical_count > 0 else 55
            },
            "api": {
                "title": "API Service Degradation",
                "description": f"API experiencing {error_count} errors impacting service availability",
                "actions": [
                    "Review API service logs for bottlenecks",
                    "Check database connectivity",
                    "Verify API rate limiting configuration",
                    "Monitor API response times and memory usage"
                ],
                "priority_score": 80 if critical_count > 0 else 50
            }
        }
        
        template = remediation_map.get(component, {
            "title": f"{component.title()} Component Issues",
            "description": f"Component {component} showing {error_count} errors requiring attention",
            "actions": [
                f"Review {component} service configuration",
                f"Check {component} service logs for patterns",
                f"Verify {component} service dependencies",
                "Consider service restart if errors persist"
            ],
            "priority_score": 70 if critical_count > 0 else 40
        })
        
        return {
            "component": component,
            "title": template["title"],
            "description": template["description"],
            "recommended_actions": template["actions"],
            "priority_score": template["priority_score"],
            "urgency": "high" if critical_count > 0 else "medium",
            "estimated_effort": "30-60 minutes",
            "risk_if_ignored": "Service degradation and potential outages"
        }

    def _create_general_remediations(self, error_logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create general remediation suggestions."""
        suggestions = []
        
        total_errors = len(error_logs)
        critical_count = len([e for e in error_logs if e.get("level") == "CRITICAL"])
        security_count = len([e for e in error_logs if e.get("error_classification", {}).get("is_security_related")])
        
        # High error volume suggestion
        if total_errors >= 20:
            suggestions.append({
                "component": "system",
                "title": "High Error Volume Detected",
                "description": f"System generating {total_errors} errors indicating systemic issues",
                "recommended_actions": [
                    "Perform comprehensive system health check",
                    "Review system resource utilization",
                    "Check for configuration drift",
                    "Consider temporary load reduction"
                ],
                "priority_score": 85,
                "urgency": "high",
                "estimated_effort": "1-2 hours",
                "risk_if_ignored": "System instability and service outages"
            })
        
        # Security-focused suggestion
        if security_count >= 5:
            suggestions.append({
                "component": "security",
                "title": "Security Incidents Detected",
                "description": f"Found {security_count} security-related errors requiring investigation",
                "recommended_actions": [
                    "Initiate security incident response procedures",
                    "Review authentication and access logs",
                    "Check for indicators of compromise",
                    "Verify security control effectiveness"
                ],
                "priority_score": 95,
                "urgency": "critical",
                "estimated_effort": "2-4 hours",
                "risk_if_ignored": "Potential security breach and data compromise"
            })
        
        # Critical error suggestion
        if critical_count >= 3:
            suggestions.append({
                "component": "system", 
                "title": "Multiple Critical Errors",
                "description": f"System experiencing {critical_count} critical errors affecting stability",
                "recommended_actions": [
                    "Implement emergency response procedures",
                    "Consider service failover if available",
                    "Engage vendor support if applicable",
                    "Prepare for potential service restart"
                ],
                "priority_score": 98,
                "urgency": "critical",
                "estimated_effort": "Immediate action required",
                "risk_if_ignored": "Complete service failure"
            })
        
        return suggestions

    def _analyze_error_trends(self, error_logs: List[Dict[str, Any]], 
                           start_dt: datetime, end_dt: datetime) -> Dict[str, Any]:
        """Analyze error trends over time."""
        trend_analysis = {
            "temporal_distribution": {},
            "trend_direction": "stable",
            "peak_periods": [],
            "anomaly_detection": {},
            "forecasting": {}
        }
        
        if not error_logs:
            return trend_analysis
        
        # Create time buckets for analysis
        time_buckets = self._create_time_buckets(start_dt, end_dt, error_logs)
        trend_analysis["temporal_distribution"] = time_buckets
        
        # Analyze trend direction
        trend_direction = self._calculate_trend_direction(time_buckets)
        trend_analysis["trend_direction"] = trend_direction
        
        # Identify peak periods
        peak_periods = self._identify_peak_periods(time_buckets)
        trend_analysis["peak_periods"] = peak_periods
        
        # Perform anomaly detection
        anomalies = self._detect_trend_anomalies(time_buckets)
        trend_analysis["anomaly_detection"] = anomalies
        
        return trend_analysis

    def _create_time_buckets(self, start_dt: datetime, end_dt: datetime, 
                           error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create time buckets for trend analysis."""
        total_duration = (end_dt - start_dt).total_seconds()
        
        # Choose bucket size based on total duration
        if total_duration <= 3600:  # 1 hour - use 5-minute buckets
            bucket_size = 300
            bucket_format = "%H:%M"
        elif total_duration <= 86400:  # 1 day - use 1-hour buckets
            bucket_size = 3600
            bucket_format = "%H:00"
        else:  # Multiple days - use 4-hour buckets
            bucket_size = 14400
            bucket_format = "%m-%d %H:00"
        
        # Initialize buckets
        buckets = {}
        current_time = start_dt
        while current_time < end_dt:
            bucket_key = current_time.strftime(bucket_format)
            buckets[bucket_key] = {
                "timestamp": current_time.isoformat(),
                "error_count": 0,
                "critical_count": 0,
                "error_levels": {},
                "components": set()
            }
            current_time += timedelta(seconds=bucket_size)
        
        # Populate buckets with error data
        for error in error_logs:
            try:
                error_time = datetime.fromisoformat(error["timestamp"].replace('Z', '+00:00'))
                # Find appropriate bucket
                bucket_time = start_dt
                while bucket_time < end_dt:
                    if bucket_time <= error_time < bucket_time + timedelta(seconds=bucket_size):
                        bucket_key = bucket_time.strftime(bucket_format)
                        if bucket_key in buckets:
                            buckets[bucket_key]["error_count"] += 1
                            if error.get("level") == "CRITICAL":
                                buckets[bucket_key]["critical_count"] += 1
                            
                            level = error.get("level", "UNKNOWN")
                            buckets[bucket_key]["error_levels"][level] = buckets[bucket_key]["error_levels"].get(level, 0) + 1
                            buckets[bucket_key]["components"].add(error.get("component", "unknown"))
                        break
                    bucket_time += timedelta(seconds=bucket_size)
            except:
                continue
        
        # Convert sets to lists for JSON serialization
        for bucket_data in buckets.values():
            bucket_data["components"] = list(bucket_data["components"])
        
        return buckets

    def _calculate_trend_direction(self, time_buckets: Dict[str, Any]) -> str:
        """Calculate overall trend direction."""
        if len(time_buckets) < 2:
            return "insufficient_data"
        
        # Get error counts in chronological order
        sorted_buckets = sorted(time_buckets.items(), key=lambda x: x[1]["timestamp"])
        error_counts = [bucket[1]["error_count"] for bucket in sorted_buckets]
        
        # Calculate simple trend
        first_half = error_counts[:len(error_counts)//2]
        second_half = error_counts[len(error_counts)//2:]
        
        avg_first = sum(first_half) / len(first_half) if first_half else 0
        avg_second = sum(second_half) / len(second_half) if second_half else 0
        
        if avg_second > avg_first * 1.2:
            return "increasing"
        elif avg_second < avg_first * 0.8:
            return "decreasing"
        else:
            return "stable"

    def _identify_peak_periods(self, time_buckets: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identify peak error periods."""
        if not time_buckets:
            return []
        
        # Calculate threshold for peak detection
        error_counts = [bucket["error_count"] for bucket in time_buckets.values()]
        avg_errors = sum(error_counts) / len(error_counts) if error_counts else 0
        peak_threshold = avg_errors * 2  # Peak is 2x average
        
        peaks = []
        for time_key, bucket_data in time_buckets.items():
            if bucket_data["error_count"] >= peak_threshold and bucket_data["error_count"] >= 5:
                peaks.append({
                    "time_period": time_key,
                    "error_count": bucket_data["error_count"],
                    "critical_count": bucket_data["critical_count"],
                    "components_affected": bucket_data["components"],
                    "severity": "high" if bucket_data["critical_count"] > 0 else "medium"
                })
        
        # Sort by error count
        peaks.sort(key=lambda x: x["error_count"], reverse=True)
        return peaks[:5]  # Return top 5 peaks

    def _detect_trend_anomalies(self, time_buckets: Dict[str, Any]) -> Dict[str, Any]:
        """Detect anomalies in error trends."""
        if len(time_buckets) < 3:
            return {"anomalies_detected": False, "anomalies": []}
        
        error_counts = [bucket["error_count"] for bucket in time_buckets.values()]
        
        # Calculate basic statistics
        mean_errors = sum(error_counts) / len(error_counts)
        variance = sum((x - mean_errors) ** 2 for x in error_counts) / len(error_counts)
        std_dev = variance ** 0.5
        
        # Detect anomalies (values beyond 2 standard deviations)
        anomalies = []
        threshold = mean_errors + (2 * std_dev)
        
        for time_key, bucket_data in time_buckets.items():
            if bucket_data["error_count"] > threshold and bucket_data["error_count"] > 0:
                anomalies.append({
                    "time_period": time_key,
                    "error_count": bucket_data["error_count"],
                    "expected_range": f"0-{int(threshold)}",
                    "anomaly_score": (bucket_data["error_count"] - mean_errors) / std_dev if std_dev > 0 else 0,
                    "components_involved": bucket_data["components"]
                })
        
        return {
            "anomalies_detected": len(anomalies) > 0,
            "anomalies": anomalies,
            "statistical_summary": {
                "mean_errors_per_period": round(mean_errors, 2),
                "standard_deviation": round(std_dev, 2),
                "anomaly_threshold": round(threshold, 2)
            }
        }

    def _generate_predictive_insights(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate predictive insights based on error patterns."""
        insights = {
            "risk_assessment": "low",
            "predicted_issues": [],
            "preventive_recommendations": [],
            "monitoring_suggestions": []
        }
        
        if not error_logs:
            return insights
        
        # Analyze component reliability
        component_analysis = self._analyze_component_reliability(error_logs)
        
        # Predict potential issues
        predicted_issues = self._predict_potential_issues(error_logs, component_analysis)
        insights["predicted_issues"] = predicted_issues
        
        # Generate preventive recommendations
        preventive_recommendations = self._generate_preventive_recommendations(component_analysis)
        insights["preventive_recommendations"] = preventive_recommendations
        
        # Calculate overall risk
        risk_level = self._calculate_predictive_risk(error_logs, component_analysis)
        insights["risk_assessment"] = risk_level
        
        # Generate monitoring suggestions
        monitoring_suggestions = self._generate_monitoring_suggestions(component_analysis)
        insights["monitoring_suggestions"] = monitoring_suggestions
        
        return insights

    def _analyze_component_reliability(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze reliability of different components."""
        component_stats = {}
        
        for error in error_logs:
            component = error.get("component", "unknown")
            if component not in component_stats:
                component_stats[component] = {
                    "total_errors": 0,
                    "critical_errors": 0,
                    "error_frequency": 0,
                    "reliability_score": 100
                }
            
            component_stats[component]["total_errors"] += 1
            if error.get("level") == "CRITICAL":
                component_stats[component]["critical_errors"] += 1
        
        # Calculate reliability scores
        for component, stats in component_stats.items():
            # Simple reliability scoring
            error_penalty = min(30, stats["total_errors"] * 2)
            critical_penalty = stats["critical_errors"] * 15
            reliability_score = max(0, 100 - error_penalty - critical_penalty)
            stats["reliability_score"] = reliability_score
        
        return component_stats

    def _predict_potential_issues(self, error_logs: List[Dict[str, Any]], 
                                component_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Predict potential future issues based on current patterns."""
        predictions = []
        
        # Predict component failures
        for component, stats in component_analysis.items():
            if stats["reliability_score"] < 70:
                probability = "high" if stats["reliability_score"] < 50 else "medium"
                predictions.append({
                    "type": "component_failure",
                    "component": component,
                    "probability": probability,
                    "timeframe": "24-48 hours" if probability == "high" else "1-7 days",
                    "description": f"{component} showing degraded reliability ({stats['reliability_score']}%)",
                    "preventive_action": f"Immediate attention required for {component} component"
                })
        
        # Predict cascading failures
        if len(component_analysis) >= 3 and sum(s["critical_errors"] for s in component_analysis.values()) >= 3:
            predictions.append({
                "type": "cascading_failure",
                "components": list(component_analysis.keys()),
                "probability": "medium",
                "timeframe": "6-24 hours",
                "description": "Multiple components showing stress, risk of cascading failure",
                "preventive_action": "System-wide health check and load balancing review"
            })
        
        return predictions

    def _generate_preventive_recommendations(self, component_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate preventive recommendations."""
        recommendations = []
        
        for component, stats in component_analysis.items():
            if stats["reliability_score"] < 80:
                recommendations.append({
                    "component": component,
                    "action": f"Proactive maintenance for {component}",
                    "description": f"Component reliability at {stats['reliability_score']}%, preventive action recommended",
                    "priority": "high" if stats["reliability_score"] < 60 else "medium",
                    "estimated_effort": "1-2 hours"
                })
        
        # General system recommendations
        total_errors = sum(stats["total_errors"] for stats in component_analysis.values())
        if total_errors >= 15:
            recommendations.append({
                "component": "system",
                "action": "System-wide optimization",
                "description": f"High error volume ({total_errors}) indicates system stress",
                "priority": "high",
                "estimated_effort": "2-4 hours"
            })
        
        return recommendations

    def _calculate_predictive_risk(self, error_logs: List[Dict[str, Any]], 
                                 component_analysis: Dict[str, Any]) -> str:
        """Calculate overall predictive risk level."""
        total_errors = len(error_logs)
        critical_errors = len([e for e in error_logs if e.get("level") == "CRITICAL"])
        
        # Components with low reliability
        unreliable_components = len([c for c, s in component_analysis.items() if s["reliability_score"] < 70])
        
        risk_score = 0
        
        # Add risk factors
        if critical_errors > 0:
            risk_score += critical_errors * 25
        if total_errors >= 20:
            risk_score += 30
        if unreliable_components >= 2:
            risk_score += 25
        
        if risk_score >= 75:
            return "critical"
        elif risk_score >= 50:
            return "high"
        elif risk_score >= 25:
            return "medium"
        else:
            return "low"

    def _generate_monitoring_suggestions(self, component_analysis: Dict[str, Any]) -> List[str]:
        """Generate monitoring suggestions."""
        suggestions = []
        
        for component, stats in component_analysis.items():
            if stats["total_errors"] >= 5:
                suggestions.append(f"Increase monitoring frequency for {component} component")
                
        if len(component_analysis) >= 4:
            suggestions.append("Implement cross-component health correlation monitoring")
            
        suggestions.append("Consider setting up automated alerting for error rate thresholds")
        suggestions.append("Implement trend analysis for proactive issue detection")
        
        return suggestions

    async def _perform_error_correlation(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform correlation analysis between different error types."""
        correlation_analysis = {
            "cross_component_correlations": [],
            "temporal_correlations": [],
            "pattern_correlations": [],
            "correlation_score": 0
        }
        
        if len(error_logs) < 2:
            return correlation_analysis
        
        # Analyze cross-component correlations
        cross_component = self._analyze_cross_component_correlations(error_logs)
        correlation_analysis["cross_component_correlations"] = cross_component
        
        # Analyze temporal correlations
        temporal = self._analyze_temporal_correlations(error_logs)
        correlation_analysis["temporal_correlations"] = temporal
        
        # Calculate correlation score
        score = len(cross_component) * 20 + len(temporal) * 15
        correlation_analysis["correlation_score"] = min(100, score)
        
        return correlation_analysis

    def _analyze_cross_component_correlations(self, error_logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Analyze correlations between different components."""
        correlations = []
        
        # Group errors by component
        component_errors = {}
        for error in error_logs:
            component = error.get("component", "unknown")
            if component not in component_errors:
                component_errors[component] = []
            component_errors[component].append(error)
        
        # Look for temporal correlations between components
        components = list(component_errors.keys())
        for i in range(len(components)):
            for j in range(i + 1, len(components)):
                comp1, comp2 = components[i], components[j]
                correlation = self._calculate_component_correlation(
                    component_errors[comp1], component_errors[comp2]
                )
                if correlation["correlation_strength"] > 0.3:
                    correlations.append({
                        "component_1": comp1,
                        "component_2": comp2,
                        "correlation_strength": correlation["correlation_strength"],
                        "common_timeframes": correlation["common_timeframes"],
                        "relationship_type": correlation["relationship_type"]
                    })
        
        return correlations

    def _calculate_component_correlation(self, errors1: List[Dict[str, Any]], 
                                       errors2: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate correlation between two components' errors."""
        correlation_window = 600  # 10 minutes
        common_timeframes = []
        
        for error1 in errors1:
            try:
                time1 = datetime.fromisoformat(error1["timestamp"].replace('Z', '+00:00'))
                for error2 in errors2:
                    try:
                        time2 = datetime.fromisoformat(error2["timestamp"].replace('Z', '+00:00'))
                        time_diff = abs((time1 - time2).total_seconds())
                        if time_diff <= correlation_window:
                            common_timeframes.append({
                                "time1": error1["timestamp"],
                                "time2": error2["timestamp"],
                                "time_difference_seconds": time_diff
                            })
                    except:
                        continue
            except:
                continue
        
        # Calculate correlation strength
        total_pairs = len(errors1) * len(errors2)
        correlated_pairs = len(common_timeframes)
        strength = correlated_pairs / total_pairs if total_pairs > 0 else 0
        
        # Determine relationship type
        if strength > 0.7:
            relationship = "strong_dependency"
        elif strength > 0.4:
            relationship = "moderate_dependency"
        elif strength > 0.1:
            relationship = "weak_correlation"
        else:
            relationship = "no_correlation"
        
        return {
            "correlation_strength": round(strength, 3),
            "common_timeframes": common_timeframes[:5],  # Limit for response size
            "relationship_type": relationship
        }

    def _analyze_temporal_correlations(self, error_logs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Analyze temporal patterns in error occurrence."""
        temporal_patterns = []
        
        # Sort errors by timestamp
        sorted_errors = sorted(error_logs, key=lambda x: x.get("timestamp", ""))
        
        # Look for sequences of errors
        sequences = []
        current_sequence = []
        
        for i, error in enumerate(sorted_errors):
            if i == 0:
                current_sequence = [error]
                continue
            
            try:
                current_time = datetime.fromisoformat(error["timestamp"].replace('Z', '+00:00'))
                prev_time = datetime.fromisoformat(sorted_errors[i-1]["timestamp"].replace('Z', '+00:00'))
                
                # If errors are within 2 minutes, consider them part of sequence
                if (current_time - prev_time).total_seconds() <= 120:
                    current_sequence.append(error)
                else:
                    if len(current_sequence) >= 3:
                        sequences.append(current_sequence)
                    current_sequence = [error]
            except:
                continue
        
        # Check last sequence
        if len(current_sequence) >= 3:
            sequences.append(current_sequence)
        
        # Analyze sequences
        for sequence in sequences:
            if len(sequence) >= 3:
                temporal_patterns.append({
                    "sequence_start": sequence[0]["timestamp"],
                    "sequence_end": sequence[-1]["timestamp"],
                    "error_count": len(sequence),
                    "components_involved": list(set(e.get("component") for e in sequence)),
                    "pattern_type": "error_burst",
                    "duration_seconds": (
                        datetime.fromisoformat(sequence[-1]["timestamp"].replace('Z', '+00:00')) -
                        datetime.fromisoformat(sequence[0]["timestamp"].replace('Z', '+00:00'))
                    ).total_seconds()
                })
        
        return temporal_patterns

    def _detect_error_cascades(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect cascading error patterns."""
        cascade_detection = {
            "cascades_detected": False,
            "cascade_events": [],
            "cascade_risk_level": "low"
        }
        
        if len(error_logs) < 3:
            return cascade_detection
        
        # Sort by timestamp
        sorted_errors = sorted(error_logs, key=lambda x: x.get("timestamp", ""))
        
        # Look for cascade patterns: errors spreading across components
        cascades = []
        cascade_window = 900  # 15 minutes
        
        for i, error in enumerate(sorted_errors):
            try:
                error_time = datetime.fromisoformat(error["timestamp"].replace('Z', '+00:00'))
                component = error.get("component")
                
                # Look for subsequent errors in other components
                cascade_errors = [error]
                components_affected = {component}
                
                for j in range(i + 1, len(sorted_errors)):
                    next_error = sorted_errors[j]
                    next_time = datetime.fromisoformat(next_error["timestamp"].replace('Z', '+00:00'))
                    next_component = next_error.get("component")
                    
                    if (next_time - error_time).total_seconds() <= cascade_window:
                        if next_component not in components_affected:
                            cascade_errors.append(next_error)
                            components_affected.add(next_component)
                    else:
                        break
                
                # If we affected multiple components, it's a cascade
                if len(components_affected) >= 3:
                    cascade = {
                        "start_time": error["timestamp"],
                        "end_time": cascade_errors[-1]["timestamp"],
                        "components_affected": list(components_affected),
                        "error_count": len(cascade_errors),
                        "cascade_severity": self._assess_cascade_severity(cascade_errors),
                        "propagation_speed": len(components_affected) / ((
                            datetime.fromisoformat(cascade_errors[-1]["timestamp"].replace('Z', '+00:00')) -
                            error_time
                        ).total_seconds() / 60)  # Components per minute
                    }
                    cascades.append(cascade)
            except:
                continue
        
        if cascades:
            cascade_detection["cascades_detected"] = True
            cascade_detection["cascade_events"] = cascades
            
            # Determine risk level
            max_components = max(len(c["components_affected"]) for c in cascades)
            if max_components >= 5:
                cascade_detection["cascade_risk_level"] = "critical"
            elif max_components >= 4:
                cascade_detection["cascade_risk_level"] = "high"
            else:
                cascade_detection["cascade_risk_level"] = "medium"
        
        return cascade_detection

    def _assess_cascade_severity(self, cascade_errors: List[Dict[str, Any]]) -> str:
        """Assess the severity of a cascade event."""
        critical_count = len([e for e in cascade_errors if e.get("level") == "CRITICAL"])
        total_count = len(cascade_errors)
        
        if critical_count >= total_count * 0.5:
            return "critical"
        elif critical_count > 0 or total_count >= 10:
            return "high"
        elif total_count >= 5:
            return "medium"
        else:
            return "low"

    async def _generate_error_analysis_report(self, error_analysis: Dict[str, Any], 
                                            start_dt: datetime, end_dt: datetime,
                                            error_levels: List[str], include_analysis: bool,
                                            include_trends: bool, correlation_analysis: bool,
                                            start_time: datetime) -> Dict[str, Any]:
        """Generate comprehensive error analysis report."""
        current_time = datetime.utcnow()
        
        # Build comprehensive report
        report = {
            "query_parameters": {
                "error_levels": error_levels,
                "time_range": {
                    "start": start_dt.isoformat(),
                    "end": end_dt.isoformat(),
                    "duration_hours": round((end_dt - start_dt).total_seconds() / 3600, 2)
                },
                "analysis_options": {
                    "include_analysis": include_analysis,
                    "include_trends": include_trends,
                    "correlation_analysis": correlation_analysis
                }
            },
            "executive_summary": self._create_error_executive_summary(error_analysis),
            "error_summary": error_analysis.get("error_summary", {}),
            "component_analysis": error_analysis.get("component_analysis", {}),
            "severity_analysis": error_analysis.get("severity_analysis", {}),
            "analysis_metadata": {
                "analysis_timestamp": current_time.isoformat(),
                "processing_time_seconds": round((current_time - start_time).total_seconds(), 2),
                "total_errors_analyzed": len(error_analysis.get("raw_errors", [])),
                "analysis_completeness": "full" if include_analysis else "basic"
            }
        }
        
        # Add optional sections
        if include_analysis:
            report["root_cause_analysis"] = error_analysis.get("root_cause_analysis", {})
            report["impact_assessment"] = error_analysis.get("impact_assessment", {})
            report["remediation_suggestions"] = error_analysis.get("remediation_suggestions", [])
        
        if include_trends:
            report["trend_analysis"] = error_analysis.get("trend_analysis", {})
            report["predictive_insights"] = error_analysis.get("predictive_insights", {})
        
        if correlation_analysis:
            report["correlation_analysis"] = error_analysis.get("correlation_analysis", {})
            report["cascade_detection"] = error_analysis.get("cascade_detection", {})
        
        return report

    def _create_error_executive_summary(self, error_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Create executive summary of error analysis."""
        raw_errors = error_analysis.get("raw_errors", [])
        error_summary = error_analysis.get("error_summary", {})
        severity_analysis = error_analysis.get("severity_analysis", {})
        
        # Calculate key metrics
        total_errors = len(raw_errors)
        critical_errors = len([e for e in raw_errors if e.get("level") == "CRITICAL"])
        security_errors = len([e for e in raw_errors if e.get("error_classification", {}).get("is_security_related")])
        
        # Determine system health
        health_score = 100
        if critical_errors > 0:
            health_score -= critical_errors * 20
        if security_errors > 0:
            health_score -= security_errors * 15
        if total_errors >= 20:
            health_score -= 30
        health_score = max(0, health_score)
        
        # Determine alert level
        if critical_errors >= 3 or security_errors >= 5:
            alert_level = "CRITICAL"
        elif critical_errors > 0 or security_errors >= 2 or total_errors >= 15:
            alert_level = "HIGH"
        elif total_errors >= 5:
            alert_level = "MEDIUM"
        else:
            alert_level = "LOW"
        
        # Key findings
        key_findings = []
        if total_errors == 0:
            key_findings.append("No errors detected in the specified time range")
        else:
            key_findings.append(f"Total of {total_errors} errors detected across system components")
            
            if critical_errors > 0:
                key_findings.append(f"{critical_errors} critical errors requiring immediate attention")
            
            if security_errors > 0:
                key_findings.append(f"{security_errors} security-related errors detected")
            
            # Top error components
            component_dist = error_summary.get("component_distribution", {})
            if component_dist:
                top_component = max(component_dist, key=component_dist.get)
                key_findings.append(f"'{top_component}' component shows highest error rate ({component_dist[top_component]} errors)")
        
        # Recommended actions
        actions = []
        if alert_level == "CRITICAL":
            actions.append("Initiate emergency response procedures")
            actions.append("Engage senior technical staff immediately")
        elif alert_level == "HIGH":
            actions.append("Escalate to technical team for immediate review")
            actions.append("Implement monitoring and remediation measures")
        elif alert_level == "MEDIUM":
            actions.append("Schedule technical review within 24 hours")
            actions.append("Monitor for trend escalation")
        else:
            actions.append("Continue standard monitoring procedures")
        
        return {
            "alert_level": alert_level,
            "system_health_score": health_score,
            "total_errors": total_errors,
            "critical_errors": critical_errors,
            "security_errors": security_errors,
            "key_findings": key_findings,
            "recommended_immediate_actions": actions,
            "requires_attention": alert_level in ["CRITICAL", "HIGH"]
        }

    async def _perform_comprehensive_process_analysis(self, agent_id: str, process_filter: str,
                                                    include_children: bool, include_hashes: bool,
                                                    threat_detection: bool, include_network_activity: bool,
                                                    baseline_comparison: bool, max_processes: int) -> Dict[str, Any]:
        """Perform comprehensive process analysis with threat detection."""
        try:
            # Fetch raw process data from agent
            raw_processes = await self._collect_agent_process_data(agent_id, process_filter, max_processes)
            
            analysis_result = {
                "raw_processes": raw_processes,
                "process_summary": self._analyze_process_summary(raw_processes),
                "hierarchy_analysis": self._analyze_process_hierarchy(raw_processes) if include_children else {},
                "resource_analysis": self._analyze_process_resources(raw_processes)
            }
            
            if include_hashes:
                analysis_result["hash_analysis"] = await self._perform_hash_analysis(raw_processes)
                analysis_result["reputation_analysis"] = await self._perform_reputation_analysis(raw_processes)
            
            if threat_detection:
                analysis_result["threat_analysis"] = await self._perform_threat_detection(raw_processes)
                analysis_result["behavior_analysis"] = await self._perform_behavior_analysis(raw_processes)
                analysis_result["anomaly_detection"] = self._detect_process_anomalies(raw_processes)
            
            if include_network_activity:
                analysis_result["network_analysis"] = await self._analyze_network_activity(agent_id, raw_processes)
            
            if baseline_comparison:
                analysis_result["baseline_analysis"] = await self._perform_baseline_comparison(agent_id, raw_processes)
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive process analysis: {str(e)}")
            return {"error": f"Analysis failed: {str(e)}", "raw_processes": []}

    async def _collect_agent_process_data(self, agent_id: str, process_filter: str, max_processes: int) -> List[Dict[str, Any]]:
        """Collect process data from the specified agent."""
        try:
            # Get process data from Wazuh API
            api_response = await self.api_client.get_agent_processes(agent_id)
            
            if not api_response or "data" not in api_response:
                return []
            
            processes = api_response.get("data", {}).get("affected_items", [])
            
            # Apply process filter if specified
            if process_filter:
                import re
                pattern = re.compile(process_filter, re.IGNORECASE)
                processes = [p for p in processes if pattern.search(p.get("name", ""))]
            
            # Enhance process data with additional metadata
            enhanced_processes = []
            for process in processes[:max_processes]:
                enhanced_process = self._enhance_process_data(process)
                enhanced_processes.append(enhanced_process)
            
            return enhanced_processes
            
        except Exception as e:
            self.logger.error(f"Error collecting agent process data: {str(e)}")
            return []

    def _enhance_process_data(self, process: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance process data with additional security metadata."""
        enhanced = process.copy()
        
        # Add security classification
        enhanced["security_classification"] = self._classify_process_security(process)
        
        # Add process metadata
        enhanced["metadata"] = {
            "collection_time": datetime.utcnow().isoformat(),
            "platform": self._detect_process_platform(process),
            "process_type": self._classify_process_type(process),
            "trust_level": self._calculate_process_trust_level(process)
        }
        
        # Add command line analysis
        if "cmd" in process:
            enhanced["command_analysis"] = self._analyze_command_line(process["cmd"])
        
        return enhanced

    def _classify_process_security(self, process: Dict[str, Any]) -> Dict[str, Any]:
        """Classify process from security perspective."""
        name = process.get("name", "").lower()
        cmd = process.get("cmd", "").lower()
        
        # Security-related indicators
        security_indicators = {
            "is_system_process": self._is_system_process(name),
            "is_network_process": self._has_network_capability(name, cmd),
            "is_encrypted_communication": self._uses_encryption(name, cmd),
            "has_persistence_mechanism": self._has_persistence(name, cmd),
            "is_suspicious_location": self._is_suspicious_location(process.get("path", "")),
            "is_packed_executable": self._is_packed_executable(name, process.get("path", "")),
            "has_injection_capability": self._has_injection_capability(name, cmd)
        }
        
        # Calculate suspicion score
        suspicion_score = self._calculate_suspicion_score(process, security_indicators)
        
        return {
            "indicators": security_indicators,
            "suspicion_score": suspicion_score,
            "risk_level": self._get_risk_level(suspicion_score),
            "threat_categories": self._identify_threat_categories(process, security_indicators)
        }

    def _is_system_process(self, name: str) -> bool:
        """Check if process is a known system process."""
        system_processes = {
            "svchost.exe", "winlogon.exe", "csrss.exe", "lsass.exe", "smss.exe",
            "wininit.exe", "services.exe", "spoolsv.exe", "explorer.exe",
            "systemd", "init", "kthreadd", "ksoftirqd", "migration", "rcu_",
            "bash", "sh", "zsh", "ssh", "sshd", "networkd", "resolved"
        }
        return name in system_processes or any(sys_proc in name for sys_proc in system_processes)

    def _has_network_capability(self, name: str, cmd: str) -> bool:
        """Check if process has network capabilities."""
        network_indicators = [
            "http", "tcp", "udp", "socket", "port", "bind", "connect",
            "curl", "wget", "nc", "netcat", "telnet", "ssh", "ftp"
        ]
        return any(indicator in name or indicator in cmd for indicator in network_indicators)

    def _uses_encryption(self, name: str, cmd: str) -> bool:
        """Check if process uses encryption."""
        crypto_indicators = [
            "ssl", "tls", "https", "ssh", "gpg", "pgp", "crypto", "encrypt",
            "cipher", "aes", "rsa", "cert", "key"
        ]
        return any(indicator in name or indicator in cmd for indicator in crypto_indicators)

    def _has_persistence(self, name: str, cmd: str) -> bool:
        """Check if process has persistence mechanisms."""
        persistence_indicators = [
            "startup", "autorun", "service", "daemon", "cron", "task",
            "registry", "boot", "init", "systemd", "launchd"
        ]
        return any(indicator in name or indicator in cmd for indicator in persistence_indicators)

    def _is_suspicious_location(self, path: str) -> bool:
        """Check if process is running from suspicious location."""
        if not path:
            return False
        
        path_lower = path.lower()
        suspicious_paths = [
            "/tmp/", "/var/tmp/", "\\temp\\", "\\tmp\\",
            "\\users\\public\\", "\\programdata\\", "\\appdata\\roaming\\",
            "/home/*/downloads/", "/downloads/", "\\downloads\\"
        ]
        return any(sus_path in path_lower for sus_path in suspicious_paths)

    def _is_packed_executable(self, name: str, path: str) -> bool:
        """Check if executable might be packed/obfuscated."""
        # Simple heuristics for packed executables
        packed_indicators = [
            "upx", "packed", "compressed", "obfuscated", "crypted"
        ]
        return any(indicator in name.lower() or indicator in path.lower() for indicator in packed_indicators)

    def _has_injection_capability(self, name: str, cmd: str) -> bool:
        """Check if process has injection capabilities."""
        injection_indicators = [
            "inject", "dll", "hook", "patch", "debug", "attach",
            "ptrace", "gdb", "windbg", "process", "memory"
        ]
        return any(indicator in name or indicator in cmd for indicator in injection_indicators)

    def _calculate_suspicion_score(self, process: Dict[str, Any], indicators: Dict[str, bool]) -> float:
        """Calculate overall suspicion score for process."""
        score = 0.0
        
        # Base scoring
        if indicators["is_suspicious_location"]:
            score += 30
        if indicators["is_packed_executable"]:
            score += 25
        if indicators["has_injection_capability"]:
            score += 20
        if not indicators["is_system_process"]:
            score += 10
        
        # Network-related scoring
        if indicators["is_network_process"]:
            if not indicators["uses_encryption"]:
                score += 15  # Unencrypted network communication
            else:
                score += 5   # Encrypted is less suspicious
        
        # Persistence scoring
        if indicators["has_persistence_mechanism"]:
            score += 10
        
        # Additional heuristics
        name = process.get("name", "").lower()
        if any(sus in name for sus in ["temp", "tmp", "test", "new", "copy", "backup"]):
            score += 15
        
        # Random-looking names
        if len(name) > 8 and name.isalnum() and any(c.isdigit() for c in name):
            if sum(1 for c in name if c.isdigit()) > len(name) * 0.3:
                score += 20
        
        return min(100.0, score)

    def _get_risk_level(self, suspicion_score: float) -> str:
        """Convert suspicion score to risk level."""
        if suspicion_score >= 80:
            return "critical"
        elif suspicion_score >= 60:
            return "high"
        elif suspicion_score >= 40:
            return "medium"
        elif suspicion_score >= 20:
            return "low"
        else:
            return "minimal"

    def _identify_threat_categories(self, process: Dict[str, Any], indicators: Dict[str, bool]) -> List[str]:
        """Identify potential threat categories."""
        categories = []
        
        if indicators["is_suspicious_location"] or indicators["is_packed_executable"]:
            categories.append("malware")
        
        if indicators["has_injection_capability"]:
            categories.append("code_injection")
        
        if indicators["is_network_process"] and not indicators["uses_encryption"]:
            categories.append("data_exfiltration")
        
        if indicators["has_persistence_mechanism"]:
            categories.append("persistence")
        
        # Command line analysis
        cmd = process.get("cmd", "").lower()
        if any(word in cmd for word in ["powershell", "cmd", "bash", "sh"]):
            if any(sus in cmd for sus in ["download", "invoke", "iex", "curl", "wget"]):
                categories.append("remote_execution")
        
        return categories if categories else ["unknown"]

    def _detect_process_platform(self, process: Dict[str, Any]) -> str:
        """Detect the platform/OS type."""
        name = process.get("name", "").lower()
        path = process.get("path", "").lower()
        
        if ".exe" in name or "\\" in path or "c:\\" in path:
            return "windows"
        elif "/" in path and not "\\" in path:
            return "linux"
        else:
            return "unknown"

    def _classify_process_type(self, process: Dict[str, Any]) -> str:
        """Classify the type of process."""
        name = process.get("name", "").lower()
        cmd = process.get("cmd", "").lower()
        
        if any(term in name for term in ["service", "daemon", "svc"]):
            return "service"
        elif any(term in name for term in ["browser", "chrome", "firefox", "edge", "safari"]):
            return "browser"
        elif any(term in name for term in ["explorer", "finder", "nautilus", "dolphin"]):
            return "file_manager"
        elif any(term in cmd for term in ["http", "web", "server"]):
            return "web_server"
        elif any(term in name for term in ["python", "java", "node", "php", "ruby"]):
            return "interpreter"
        else:
            return "application"

    def _calculate_process_trust_level(self, process: Dict[str, Any]) -> str:
        """Calculate trust level for process."""
        name = process.get("name", "").lower()
        path = process.get("path", "").lower()
        
        # System paths are generally trusted
        trusted_paths = [
            "c:\\windows\\system32\\", "c:\\windows\\syswow64\\",
            "/usr/bin/", "/bin/", "/sbin/", "/usr/sbin/",
            "/system/", "/usr/lib/", "/lib/"
        ]
        
        if any(trusted_path in path for trusted_path in trusted_paths):
            return "high"
        elif self._is_system_process(name):
            return "high"
        elif path.startswith(("c:\\program files\\", "/usr/local/", "/opt/")):
            return "medium"
        else:
            return "low"

    def _analyze_command_line(self, cmd: str) -> Dict[str, Any]:
        """Analyze command line for suspicious patterns."""
        if not cmd:
            return {}
        
        cmd_lower = cmd.lower()
        
        analysis = {
            "length": len(cmd),
            "argument_count": len(cmd.split()) - 1,
            "has_urls": bool(re.search(r'https?://', cmd)),
            "has_ips": bool(re.search(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', cmd)),
            "has_base64": bool(re.search(r'[A-Za-z0-9+/]{20,}={0,2}', cmd)),
            "has_obfuscation": self._detect_obfuscation(cmd),
            "suspicious_keywords": self._find_suspicious_keywords(cmd_lower),
            "encoding_detected": self._detect_encoding_attempts(cmd_lower)
        }
        
        # Calculate command line risk score
        risk_score = 0
        if analysis["length"] > 500:
            risk_score += 20
        if analysis["has_base64"]:
            risk_score += 30
        if analysis["has_obfuscation"]:
            risk_score += 25
        if analysis["suspicious_keywords"]:
            risk_score += len(analysis["suspicious_keywords"]) * 10
        if analysis["encoding_detected"]:
            risk_score += 15
        
        analysis["risk_score"] = min(100, risk_score)
        analysis["risk_level"] = self._get_risk_level(risk_score)
        
        return analysis

    def _detect_obfuscation(self, cmd: str) -> bool:
        """Detect command line obfuscation techniques."""
        obfuscation_patterns = [
            r'["\'^]{3,}',  # Multiple quotes
            r'[&|;]{2,}',   # Multiple operators
            r'\$\{[^}]*\}', # Variable substitution
            r'`[^`]*`',     # Backticks
            r'\\x[0-9a-fA-F]{2}',  # Hex encoding
            r'%[0-9a-fA-F]{2}',    # URL encoding
        ]
        
        return any(re.search(pattern, cmd) for pattern in obfuscation_patterns)

    def _find_suspicious_keywords(self, cmd: str) -> List[str]:
        """Find suspicious keywords in command line."""
        suspicious_keywords = [
            "powershell", "invoke", "downloadstring", "downloadfile",
            "iex", "bypass", "executionpolicy", "hidden", "windowstyle",
            "encoded", "noprofile", "noninteractive", "curl", "wget",
            "nc", "netcat", "reverse", "shell", "bind", "backdoor",
            "mimikatz", "metasploit", "payload", "shellcode", "exploit"
        ]
        
        found = []
        for keyword in suspicious_keywords:
            if keyword in cmd:
                found.append(keyword)
        
        return found

    def _detect_encoding_attempts(self, cmd: str) -> bool:
        """Detect encoding/encryption attempts in command line."""
        encoding_indicators = [
            "base64", "encode", "decode", "compress", "decompress",
            "encrypt", "decrypt", "cipher", "rot13", "hex"
        ]
        return any(indicator in cmd for indicator in encoding_indicators)

    def _analyze_process_summary(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze overall process summary."""
        if not processes:
            return {"total_processes": 0, "message": "No processes found"}
        
        summary = {
            "total_processes": len(processes),
            "platform_distribution": {},
            "type_distribution": {},
            "risk_distribution": {},
            "suspicious_count": 0,
            "high_risk_count": 0
        }
        
        for process in processes:
            # Platform distribution
            platform = process.get("metadata", {}).get("platform", "unknown")
            summary["platform_distribution"][platform] = summary["platform_distribution"].get(platform, 0) + 1
            
            # Type distribution
            proc_type = process.get("metadata", {}).get("process_type", "unknown")
            summary["type_distribution"][proc_type] = summary["type_distribution"].get(proc_type, 0) + 1
            
            # Risk distribution
            risk_level = process.get("security_classification", {}).get("risk_level", "minimal")
            summary["risk_distribution"][risk_level] = summary["risk_distribution"].get(risk_level, 0) + 1
            
            # Count suspicious and high-risk processes
            suspicion_score = process.get("security_classification", {}).get("suspicion_score", 0)
            if suspicion_score >= 40:
                summary["suspicious_count"] += 1
            if suspicion_score >= 60:
                summary["high_risk_count"] += 1
        
        return summary

    def _analyze_process_hierarchy(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze process parent-child relationships."""
        hierarchy = {
            "process_trees": [],
            "orphaned_processes": [],
            "suspicious_parent_child": [],
            "injection_candidates": []
        }
        
        # Group processes by parent PID
        parent_map = {}
        for process in processes:
            ppid = process.get("ppid")
            if ppid:
                if ppid not in parent_map:
                    parent_map[ppid] = []
                parent_map[ppid].append(process)
        
        # Identify suspicious parent-child relationships
        for process in processes:
            pid = process.get("pid")
            children = parent_map.get(pid, [])
            
            if children:
                # Check for suspicious spawning patterns
                suspicious_spawns = self._detect_suspicious_spawning(process, children)
                if suspicious_spawns:
                    hierarchy["suspicious_parent_child"].extend(suspicious_spawns)
        
        # Find potential injection targets
        hierarchy["injection_candidates"] = self._find_injection_candidates(processes)
        
        return hierarchy

    def _detect_suspicious_spawning(self, parent: Dict[str, Any], children: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect suspicious parent-child spawning patterns."""
        suspicious = []
        parent_name = parent.get("name", "").lower()
        
        for child in children:
            child_name = child.get("name", "").lower()
            
            # Office applications spawning shells/scripts
            if any(office in parent_name for office in ["word", "excel", "powerpoint", "outlook"]):
                if any(shell in child_name for shell in ["cmd", "powershell", "bash", "sh", "python"]):
                    suspicious.append({
                        "parent": parent_name,
                        "child": child_name,
                        "risk": "office_spawning_shell",
                        "severity": "high"
                    })
            
            # Browser spawning unexpected processes
            if any(browser in parent_name for browser in ["chrome", "firefox", "edge", "safari"]):
                if any(exec_type in child_name for exec_type in ["powershell", "cmd", "python", "java"]):
                    suspicious.append({
                        "parent": parent_name,
                        "child": child_name,
                        "risk": "browser_spawning_executable",
                        "severity": "medium"
                    })
            
            # System processes spawning unexpected children
            if parent_name in ["svchost.exe", "explorer.exe", "winlogon.exe"]:
                if child.get("security_classification", {}).get("suspicion_score", 0) > 50:
                    suspicious.append({
                        "parent": parent_name,
                        "child": child_name,
                        "risk": "system_spawning_suspicious",
                        "severity": "critical"
                    })
        
        return suspicious

    def _find_injection_candidates(self, processes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Find processes that could be injection targets."""
        candidates = []
        
        for process in processes:
            name = process.get("name", "").lower()
            
            # Common injection targets
            if any(target in name for target in [
                "explorer.exe", "svchost.exe", "winlogon.exe", "lsass.exe",
                "chrome.exe", "firefox.exe", "notepad.exe"
            ]):
                candidates.append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "reason": "common_injection_target",
                    "risk_level": "medium"
                })
        
        return candidates

    def _analyze_process_resources(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze process resource usage patterns."""
        resource_analysis = {
            "high_cpu_processes": [],
            "high_memory_processes": [],
            "resource_anomalies": [],
            "total_cpu_usage": 0,
            "total_memory_usage": 0
        }
        
        cpu_values = []
        memory_values = []
        
        for process in processes:
            cpu = process.get("cpu", 0) or 0
            memory = process.get("memory", 0) or 0
            
            # Convert memory to MB if it's in bytes
            if memory > 1024*1024:
                memory = memory / (1024*1024)
            
            cpu_values.append(cpu)
            memory_values.append(memory)
            
            # Identify high resource usage
            if cpu > 50:
                resource_analysis["high_cpu_processes"].append({
                    "name": process.get("name"),
                    "pid": process.get("pid"),
                    "cpu": cpu
                })
            
            if memory > 500:  # MB
                resource_analysis["high_memory_processes"].append({
                    "name": process.get("name"),
                    "pid": process.get("pid"),
                    "memory": memory
                })
        
        if cpu_values:
            resource_analysis["total_cpu_usage"] = sum(cpu_values)
            resource_analysis["average_cpu"] = sum(cpu_values) / len(cpu_values)
        
        if memory_values:
            resource_analysis["total_memory_usage"] = sum(memory_values)
            resource_analysis["average_memory"] = sum(memory_values) / len(memory_values)
        
        # Detect resource anomalies
        resource_analysis["resource_anomalies"] = self._detect_resource_anomalies(processes, cpu_values, memory_values)
        
        return resource_analysis

    def _detect_resource_anomalies(self, processes: List[Dict[str, Any]], cpu_values: List[float], memory_values: List[float]) -> List[Dict[str, Any]]:
        """Detect unusual resource usage patterns."""
        anomalies = []
        
        if not cpu_values or not memory_values:
            return anomalies
        
        # Calculate statistics
        avg_cpu = sum(cpu_values) / len(cpu_values)
        avg_memory = sum(memory_values) / len(memory_values)
        
        # Find outliers (simple approach)
        for i, process in enumerate(processes):
            cpu = cpu_values[i]
            memory = memory_values[i]
            
            # CPU anomalies
            if cpu > avg_cpu * 3 and cpu > 20:
                anomalies.append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "anomaly_type": "high_cpu",
                    "value": cpu,
                    "average": avg_cpu,
                    "severity": "high" if cpu > 80 else "medium"
                })
            
            # Memory anomalies
            if memory > avg_memory * 3 and memory > 100:
                anomalies.append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "anomaly_type": "high_memory",
                    "value": memory,
                    "average": avg_memory,
                    "severity": "high" if memory > 1000 else "medium"
                })
        
        return anomalies

    async def _perform_hash_analysis(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform hash analysis on process executables."""
        hash_analysis = {
            "hash_summary": {},
            "duplicate_hashes": [],
            "hash_reputation": {},
            "unsigned_executables": []
        }
        
        hash_counts = {}
        
        for process in processes:
            # Simulate hash calculation (in real implementation, you'd get actual file hashes)
            path = process.get("path", "")
            if path:
                # Generate a simulated hash based on path
                import hashlib
                simulated_hash = hashlib.md5(path.encode()).hexdigest()
                process["file_hash"] = simulated_hash
                
                hash_counts[simulated_hash] = hash_counts.get(simulated_hash, 0) + 1
                
                # Check if executable is unsigned (simulated)
                if self._is_likely_unsigned(process):
                    hash_analysis["unsigned_executables"].append({
                        "process": process.get("name"),
                        "path": path,
                        "hash": simulated_hash
                    })
        
        # Find duplicate hashes
        for hash_val, count in hash_counts.items():
            if count > 1:
                hash_analysis["duplicate_hashes"].append({
                    "hash": hash_val,
                    "count": count
                })
        
        hash_analysis["hash_summary"] = {
            "total_unique_hashes": len(hash_counts),
            "total_processes_with_hashes": len([p for p in processes if "file_hash" in p])
        }
        
        return hash_analysis

    def _is_likely_unsigned(self, process: Dict[str, Any]) -> bool:
        """Determine if executable is likely unsigned."""
        path = process.get("path", "").lower()
        name = process.get("name", "").lower()
        
        # Simple heuristics for unsigned executables
        if any(location in path for location in ["/tmp/", "\\temp\\", "\\users\\", "/home/"]):
            return True
        
        if any(indicator in name for indicator in ["temp", "test", "new", "copy"]):
            return True
        
        return False

    async def _perform_reputation_analysis(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform reputation analysis on process executables."""
        reputation_analysis = {
            "known_good": [],
            "known_bad": [],
            "unknown": [],
            "reputation_summary": {}
        }
        
        # Simulated reputation database
        known_good_hashes = set([
            "d41d8cd98f00b204e9800998ecf8427e",  # Example known good
            "5d41402abc4b2a76b9719d911017c592"   # Example known good
        ])
        
        known_bad_hashes = set([
            "098f6bcd4621d373cade4e832627b4f6",  # Example malware hash
            "5e884898da28047151d0e56f8dc6292"   # Example malware hash
        ])
        
        for process in processes:
            file_hash = process.get("file_hash")
            if not file_hash:
                continue
            
            if file_hash in known_good_hashes:
                reputation_analysis["known_good"].append({
                    "process": process.get("name"),
                    "hash": file_hash,
                    "reputation": "trusted"
                })
            elif file_hash in known_bad_hashes:
                reputation_analysis["known_bad"].append({
                    "process": process.get("name"),
                    "hash": file_hash,
                    "reputation": "malicious"
                })
            else:
                reputation_analysis["unknown"].append({
                    "process": process.get("name"),
                    "hash": file_hash,
                    "reputation": "unknown"
                })
        
        reputation_analysis["reputation_summary"] = {
            "known_good_count": len(reputation_analysis["known_good"]),
            "known_bad_count": len(reputation_analysis["known_bad"]),
            "unknown_count": len(reputation_analysis["unknown"])
        }
        
        return reputation_analysis

    async def _perform_threat_detection(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform advanced threat detection on processes."""
        threat_analysis = {
            "detected_threats": [],
            "suspicious_processes": [],
            "threat_indicators": {},
            "threat_summary": {}
        }
        
        for process in processes:
            threats = self._detect_process_threats(process)
            if threats:
                threat_analysis["detected_threats"].extend(threats)
            
            suspicion_score = process.get("security_classification", {}).get("suspicion_score", 0)
            if suspicion_score >= 40:
                threat_analysis["suspicious_processes"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "suspicion_score": suspicion_score,
                    "threat_categories": process.get("security_classification", {}).get("threat_categories", [])
                })
        
        # Aggregate threat indicators
        threat_analysis["threat_indicators"] = self._aggregate_threat_indicators(processes)
        
        # Create threat summary
        threat_analysis["threat_summary"] = {
            "total_threats_detected": len(threat_analysis["detected_threats"]),
            "suspicious_process_count": len(threat_analysis["suspicious_processes"]),
            "highest_risk_score": max([p.get("security_classification", {}).get("suspicion_score", 0) for p in processes], default=0)
        }
        
        return threat_analysis

    def _detect_process_threats(self, process: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect specific threats in a process."""
        threats = []
        name = process.get("name", "").lower()
        cmd = process.get("cmd", "").lower()
        path = process.get("path", "").lower()
        
        # Known malware signatures
        malware_signatures = [
            {"name": "mimikatz", "severity": "critical", "type": "credential_dumper"},
            {"name": "meterpreter", "severity": "critical", "type": "backdoor"},
            {"name": "cobalt", "severity": "high", "type": "c2_beacon"},
            {"name": "psexec", "severity": "medium", "type": "lateral_movement"},
            {"name": "netcat", "severity": "medium", "type": "remote_access"}
        ]
        
        for signature in malware_signatures:
            if signature["name"] in name or signature["name"] in cmd:
                threats.append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "threat_type": signature["type"],
                    "threat_name": signature["name"],
                    "severity": signature["severity"],
                    "confidence": "high"
                })
        
        # Suspicious command patterns
        if any(pattern in cmd for pattern in [
            "powershell.*downloadstring",
            "cmd.*echo.*>",
            "wmic.*process.*call.*create",
            "reg.*add.*run"
        ]):
            threats.append({
                "process": process.get("name"),
                "pid": process.get("pid"),
                "threat_type": "suspicious_command",
                "threat_name": "malicious_command_pattern",
                "severity": "high",
                "confidence": "medium"
            })
        
        return threats

    def _aggregate_threat_indicators(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate threat indicators across all processes."""
        indicators = {
            "network_activity": 0,
            "file_system_activity": 0,
            "registry_activity": 0,
            "process_injection": 0,
            "persistence_mechanisms": 0,
            "credential_access": 0
        }
        
        for process in processes:
            security_class = process.get("security_classification", {})
            cmd = process.get("cmd", "").lower()
            
            # Count various threat indicators
            if security_class.get("indicators", {}).get("is_network_process"):
                indicators["network_activity"] += 1
            
            if any(term in cmd for term in ["file", "write", "create", "delete"]):
                indicators["file_system_activity"] += 1
            
            if any(term in cmd for term in ["reg", "registry", "hkey"]):
                indicators["registry_activity"] += 1
            
            if security_class.get("indicators", {}).get("has_injection_capability"):
                indicators["process_injection"] += 1
            
            if security_class.get("indicators", {}).get("has_persistence_mechanism"):
                indicators["persistence_mechanisms"] += 1
            
            if any(term in cmd for term in ["password", "credential", "token", "hash"]):
                indicators["credential_access"] += 1
        
        return indicators

    async def _perform_behavior_analysis(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Perform behavioral analysis on processes."""
        behavior_analysis = {
            "behavior_patterns": [],
            "execution_chains": [],
            "communication_patterns": {},
            "persistence_analysis": {}
        }
        
        # Analyze execution patterns
        behavior_analysis["behavior_patterns"] = self._analyze_execution_patterns(processes)
        
        # Identify execution chains
        behavior_analysis["execution_chains"] = self._identify_execution_chains(processes)
        
        # Analyze communication patterns
        behavior_analysis["communication_patterns"] = self._analyze_communication_patterns(processes)
        
        # Analyze persistence mechanisms
        behavior_analysis["persistence_analysis"] = self._analyze_persistence_mechanisms(processes)
        
        return behavior_analysis

    def _analyze_execution_patterns(self, processes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Analyze process execution patterns."""
        patterns = []
        
        # Group processes by name
        process_groups = {}
        for process in processes:
            name = process.get("name")
            if name not in process_groups:
                process_groups[name] = []
            process_groups[name].append(process)
        
        # Identify unusual patterns
        for name, group in process_groups.items():
            if len(group) > 5:  # Multiple instances
                patterns.append({
                    "pattern_type": "multiple_instances",
                    "process_name": name,
                    "instance_count": len(group),
                    "risk_level": "medium" if len(group) > 10 else "low"
                })
        
        # Look for rapid spawning patterns
        # (In real implementation, you'd check timestamps)
        
        return patterns

    def _identify_execution_chains(self, processes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify process execution chains."""
        chains = []
        
        # Build parent-child relationships
        parent_map = {}
        for process in processes:
            ppid = process.get("ppid")
            if ppid:
                if ppid not in parent_map:
                    parent_map[ppid] = []
                parent_map[ppid].append(process)
        
        # Find suspicious chains
        for process in processes:
            pid = process.get("pid")
            children = parent_map.get(pid, [])
            
            if len(children) >= 3:  # Process with many children
                chains.append({
                    "parent_process": process.get("name"),
                    "parent_pid": pid,
                    "child_count": len(children),
                    "children": [{"name": child.get("name"), "pid": child.get("pid")} for child in children],
                    "risk_assessment": "high" if len(children) > 5 else "medium"
                })
        
        return chains

    def _analyze_communication_patterns(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze inter-process communication patterns."""
        communication = {
            "network_processes": [],
            "ipc_mechanisms": [],
            "file_based_communication": []
        }
        
        for process in processes:
            security_class = process.get("security_classification", {})
            
            if security_class.get("indicators", {}).get("is_network_process"):
                communication["network_processes"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "uses_encryption": security_class.get("indicators", {}).get("uses_encryption", False)
                })
        
        return communication

    def _analyze_persistence_mechanisms(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze persistence mechanisms used by processes."""
        persistence = {
            "startup_processes": [],
            "service_processes": [],
            "scheduled_tasks": [],
            "registry_persistence": []
        }
        
        for process in processes:
            name = process.get("name", "").lower()
            cmd = process.get("cmd", "").lower()
            
            if any(term in name for term in ["service", "svc"]):
                persistence["service_processes"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid")
                })
            
            if any(term in cmd for term in ["startup", "autorun"]):
                persistence["startup_processes"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid")
                })
            
            if any(term in cmd for term in ["schtasks", "cron", "at "]):
                persistence["scheduled_tasks"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid")
                })
            
            if any(term in cmd for term in ["reg add", "registry"]):
                persistence["registry_persistence"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid")
                })
        
        return persistence

    def _detect_process_anomalies(self, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect anomalies in process behavior."""
        anomalies = {
            "statistical_anomalies": [],
            "behavioral_anomalies": [],
            "temporal_anomalies": []
        }
        
        # Statistical anomalies (unusual resource usage, etc.)
        anomalies["statistical_anomalies"] = self._detect_statistical_anomalies(processes)
        
        # Behavioral anomalies (unusual process combinations, etc.)
        anomalies["behavioral_anomalies"] = self._detect_behavioral_anomalies(processes)
        
        return anomalies

    def _detect_statistical_anomalies(self, processes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect statistical anomalies in process data."""
        anomalies = []
        
        # Collect metrics
        cpu_values = [p.get("cpu", 0) or 0 for p in processes]
        memory_values = [p.get("memory", 0) or 0 for p in processes]
        
        if not cpu_values or not memory_values:
            return anomalies
        
        # Simple outlier detection
        avg_cpu = sum(cpu_values) / len(cpu_values)
        avg_memory = sum(memory_values) / len(memory_values)
        
        for i, process in enumerate(processes):
            cpu = cpu_values[i]
            memory = memory_values[i]
            
            # CPU outliers
            if cpu > avg_cpu * 4 and cpu > 30:
                anomalies.append({
                    "process": process.get("name"),
                    "anomaly_type": "cpu_outlier",
                    "value": cpu,
                    "severity": "high"
                })
            
            # Memory outliers
            if memory > avg_memory * 4 and memory > 200:
                anomalies.append({
                    "process": process.get("name"),
                    "anomaly_type": "memory_outlier",
                    "value": memory,
                    "severity": "high"
                })
        
        return anomalies

    def _detect_behavioral_anomalies(self, processes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect behavioral anomalies in processes."""
        anomalies = []
        
        # Check for unusual process combinations
        process_names = [p.get("name", "").lower() for p in processes]
        
        # Office + Shell combination
        has_office = any(office in name for name in process_names for office in ["word", "excel", "powerpoint"])
        has_shell = any(shell in name for name in process_names for shell in ["cmd", "powershell", "bash"])
        
        if has_office and has_shell:
            anomalies.append({
                "anomaly_type": "office_shell_combination",
                "description": "Office application running alongside shell processes",
                "severity": "medium"
            })
        
        # Multiple interpreters
        interpreters = [name for name in process_names if any(interp in name for interp in ["python", "java", "node", "php"])]
        if len(set(interpreters)) > 2:
            anomalies.append({
                "anomaly_type": "multiple_interpreters",
                "description": f"Multiple script interpreters running: {list(set(interpreters))}",
                "severity": "low"
            })
        
        return anomalies

    async def _analyze_network_activity(self, agent_id: str, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze network activity associated with processes."""
        network_analysis = {
            "network_processes": [],
            "suspicious_connections": [],
            "communication_summary": {}
        }
        
        try:
            # Get port/connection data for the agent
            ports_data = await self.api_client.get_agent_ports(agent_id)
            
            if ports_data and "data" in ports_data:
                connections = ports_data.get("data", {}).get("affected_items", [])
                
                # Correlate connections with processes
                for process in processes:
                    pid = process.get("pid")
                    if pid:
                        # Find connections for this process
                        process_connections = [conn for conn in connections if conn.get("pid") == pid]
                        
                        if process_connections:
                            network_analysis["network_processes"].append({
                                "process": process.get("name"),
                                "pid": pid,
                                "connections": process_connections,
                                "connection_count": len(process_connections)
                            })
                            
                            # Check for suspicious connections
                            for conn in process_connections:
                                if self._is_suspicious_connection(conn, process):
                                    network_analysis["suspicious_connections"].append({
                                        "process": process.get("name"),
                                        "pid": pid,
                                        "connection": conn,
                                        "suspicion_reason": self._get_connection_suspicion_reason(conn, process)
                                    })
                
                network_analysis["communication_summary"] = {
                    "total_network_processes": len(network_analysis["network_processes"]),
                    "total_connections": sum(np["connection_count"] for np in network_analysis["network_processes"]),
                    "suspicious_connection_count": len(network_analysis["suspicious_connections"])
                }
        
        except Exception as e:
            self.logger.warning(f"Could not analyze network activity: {str(e)}")
            network_analysis["error"] = "Network analysis unavailable"
        
        return network_analysis

    def _is_suspicious_connection(self, connection: Dict[str, Any], process: Dict[str, Any]) -> bool:
        """Check if a network connection is suspicious."""
        # Check for connections to suspicious ports
        port = connection.get("local_port") or connection.get("remote_port")
        if port in [4444, 6666, 31337, 1337, 8080]:  # Common backdoor ports
            return True
        
        # Check for external connections from system processes
        remote_ip = connection.get("remote_ip", "")
        if remote_ip and not remote_ip.startswith(("127.", "192.168.", "10.", "172.")):
            process_name = process.get("name", "").lower()
            if any(sys_proc in process_name for sys_proc in ["svchost", "lsass", "winlogon"]):
                return True
        
        return False

    def _get_connection_suspicion_reason(self, connection: Dict[str, Any], process: Dict[str, Any]) -> str:
        """Get the reason why a connection is suspicious."""
        port = connection.get("local_port") or connection.get("remote_port")
        remote_ip = connection.get("remote_ip", "")
        process_name = process.get("name", "")
        
        if port in [4444, 6666, 31337, 1337]:
            return f"Known backdoor port {port}"
        elif remote_ip and not remote_ip.startswith(("127.", "192.168.", "10.", "172.")):
            return f"External connection from system process {process_name}"
        else:
            return "Unknown suspicious pattern"

    async def _perform_baseline_comparison(self, agent_id: str, processes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compare current processes against known baseline."""
        baseline_analysis = {
            "baseline_deviation": [],
            "new_processes": [],
            "missing_processes": [],
            "baseline_summary": {}
        }
        
        # Simulate baseline data (in real implementation, this would come from database)
        baseline_processes = [
            "explorer.exe", "svchost.exe", "winlogon.exe", "csrss.exe",
            "systemd", "init", "bash", "ssh"
        ]
        
        current_process_names = [p.get("name", "").lower() for p in processes]
        
        # Find new processes (not in baseline)
        for process in processes:
            name = process.get("name", "").lower()
            if name not in baseline_processes:
                baseline_analysis["new_processes"].append({
                    "process": process.get("name"),
                    "pid": process.get("pid"),
                    "suspicion_score": process.get("security_classification", {}).get("suspicion_score", 0),
                    "first_seen": datetime.utcnow().isoformat()
                })
        
        # Find missing baseline processes
        for baseline_proc in baseline_processes:
            if baseline_proc not in current_process_names:
                baseline_analysis["missing_processes"].append({
                    "process": baseline_proc,
                    "expected": True,
                    "impact": "low"  # Most missing processes are low impact
                })
        
        baseline_analysis["baseline_summary"] = {
            "total_baseline_processes": len(baseline_processes),
            "new_process_count": len(baseline_analysis["new_processes"]),
            "missing_process_count": len(baseline_analysis["missing_processes"]),
            "baseline_compliance": len(current_process_names) / (len(current_process_names) + len(baseline_analysis["missing_processes"]))
        }
        
        return baseline_analysis

    async def _generate_process_analysis_report(self, process_analysis: Dict[str, Any], 
                                              validated_query, start_time: datetime) -> Dict[str, Any]:
        """Generate comprehensive process analysis report."""
        current_time = datetime.utcnow()
        
        # Build comprehensive report
        report = {
            "query_parameters": {
                "agent_id": validated_query.agent_id,
                "process_filter": validated_query.process_filter,
                "analysis_options": {
                    "include_children": validated_query.include_children,
                    "include_hashes": validated_query.include_hashes,
                    "threat_detection": validated_query.threat_detection,
                    "include_network_activity": validated_query.include_network_activity,
                    "baseline_comparison": validated_query.baseline_comparison
                }
            },
            "executive_summary": self._create_process_executive_summary(process_analysis),
            "process_summary": process_analysis.get("process_summary", {}),
            "security_analysis": self._create_security_analysis_summary(process_analysis),
            "analysis_metadata": {
                "analysis_timestamp": current_time.isoformat(),
                "processing_time_seconds": round((current_time - start_time).total_seconds(), 2),
                "total_processes_analyzed": len(process_analysis.get("raw_processes", [])),
                "analysis_completeness": "full"
            }
        }
        
        # Add detailed sections
        if process_analysis.get("hierarchy_analysis"):
            report["hierarchy_analysis"] = process_analysis["hierarchy_analysis"]
        
        if process_analysis.get("resource_analysis"):
            report["resource_analysis"] = process_analysis["resource_analysis"]
        
        if process_analysis.get("hash_analysis"):
            report["hash_analysis"] = process_analysis["hash_analysis"]
        
        if process_analysis.get("reputation_analysis"):
            report["reputation_analysis"] = process_analysis["reputation_analysis"]
        
        if process_analysis.get("threat_analysis"):
            report["threat_analysis"] = process_analysis["threat_analysis"]
        
        if process_analysis.get("behavior_analysis"):
            report["behavior_analysis"] = process_analysis["behavior_analysis"]
        
        if process_analysis.get("anomaly_detection"):
            report["anomaly_detection"] = process_analysis["anomaly_detection"]
        
        if process_analysis.get("network_analysis"):
            report["network_analysis"] = process_analysis["network_analysis"]
        
        if process_analysis.get("baseline_analysis"):
            report["baseline_analysis"] = process_analysis["baseline_analysis"]
        
        # Add process details
        report["process_details"] = process_analysis.get("raw_processes", [])
        
        return report

    def _create_process_executive_summary(self, process_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Create executive summary of process analysis."""
        raw_processes = process_analysis.get("raw_processes", [])
        threat_analysis = process_analysis.get("threat_analysis", {})
        
        # Calculate key metrics
        total_processes = len(raw_processes)
        suspicious_processes = len([p for p in raw_processes if p.get("security_classification", {}).get("suspicion_score", 0) >= 40])
        high_risk_processes = len([p for p in raw_processes if p.get("security_classification", {}).get("suspicion_score", 0) >= 60])
        threats_detected = len(threat_analysis.get("detected_threats", []))
        
        # Determine overall security posture
        security_score = 100
        if threats_detected > 0:
            security_score -= threats_detected * 25
        if high_risk_processes > 0:
            security_score -= high_risk_processes * 15
        if suspicious_processes > 0:
            security_score -= suspicious_processes * 10
        security_score = max(0, security_score)
        
        # Determine alert level
        if threats_detected >= 3 or high_risk_processes >= 2:
            alert_level = "CRITICAL"
        elif threats_detected > 0 or high_risk_processes > 0:
            alert_level = "HIGH"
        elif suspicious_processes >= 3:
            alert_level = "MEDIUM"
        else:
            alert_level = "LOW"
        
        # Key findings
        key_findings = []
        if total_processes == 0:
            key_findings.append("No processes found on the specified agent")
        else:
            key_findings.append(f"Total of {total_processes} processes analyzed on agent")
            
            if threats_detected > 0:
                key_findings.append(f"{threats_detected} active threats detected requiring immediate attention")
            
            if high_risk_processes > 0:
                key_findings.append(f"{high_risk_processes} high-risk processes identified")
            
            if suspicious_processes > 0:
                key_findings.append(f"{suspicious_processes} suspicious processes flagged for review")
        
        # Recommended actions
        actions = []
        if alert_level == "CRITICAL":
            actions.append("Immediately isolate agent and begin incident response")
            actions.append("Terminate suspicious processes and collect forensic evidence")
        elif alert_level == "HIGH":
            actions.append("Investigate detected threats and high-risk processes")
            actions.append("Implement additional monitoring and containment measures")
        elif alert_level == "MEDIUM":
            actions.append("Review suspicious processes for false positives")
            actions.append("Enhance monitoring for behavioral changes")
        else:
            actions.append("Continue standard monitoring procedures")
            actions.append("Maintain current security posture")
        
        return {
            "alert_level": alert_level,
            "security_score": security_score,
            "total_processes": total_processes,
            "suspicious_processes": suspicious_processes,
            "high_risk_processes": high_risk_processes,
            "threats_detected": threats_detected,
            "key_findings": key_findings,
            "recommended_immediate_actions": actions,
            "requires_attention": alert_level in ["CRITICAL", "HIGH"]
        }

    def _create_security_analysis_summary(self, process_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Create security analysis summary."""
        threat_analysis = process_analysis.get("threat_analysis", {})
        hash_analysis = process_analysis.get("hash_analysis", {})
        network_analysis = process_analysis.get("network_analysis", {})
        
        return {
            "threat_detection": {
                "total_threats": len(threat_analysis.get("detected_threats", [])),
                "suspicious_processes": len(threat_analysis.get("suspicious_processes", [])),
                "highest_threat_score": threat_analysis.get("threat_summary", {}).get("highest_risk_score", 0)
            },
            "hash_verification": {
                "total_hashes": hash_analysis.get("hash_summary", {}).get("total_unique_hashes", 0),
                "unsigned_executables": len(hash_analysis.get("unsigned_executables", [])),
                "known_malicious": len(hash_analysis.get("reputation_analysis", {}).get("known_bad", []))
            },
            "network_activity": {
                "network_processes": len(network_analysis.get("network_processes", [])),
                "suspicious_connections": len(network_analysis.get("suspicious_connections", [])),
                "total_connections": network_analysis.get("communication_summary", {}).get("total_connections", 0)
            }
        }

    def _filter_suspicious_processes(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Filter report to show only suspicious processes."""
        if "process_details" not in report:
            return report
        
        # Filter processes with suspicion score >= 40
        suspicious_processes = [
            p for p in report["process_details"] 
            if p.get("security_classification", {}).get("suspicion_score", 0) >= 40
        ]
        
        # Update report
        filtered_report = report.copy()
        filtered_report["process_details"] = suspicious_processes
        filtered_report["analysis_metadata"]["filtered_view"] = "suspicious_only"
        filtered_report["analysis_metadata"]["total_suspicious_processes"] = len(suspicious_processes)
        
        return filtered_report

    def _sort_process_results(self, report: Dict[str, Any], sort_by: str) -> Dict[str, Any]:
        """Sort process results by specified metric."""
        if "process_details" not in report:
            return report
        
        processes = report["process_details"]
        
        if sort_by == "threat_score":
            processes.sort(key=lambda x: x.get("security_classification", {}).get("suspicion_score", 0), reverse=True)
        elif sort_by == "cpu":
            processes.sort(key=lambda x: x.get("cpu", 0) or 0, reverse=True)
        elif sort_by == "memory":
            processes.sort(key=lambda x: x.get("memory", 0) or 0, reverse=True)
        elif sort_by == "pid":
            processes.sort(key=lambda x: x.get("pid", 0) or 0)
        elif sort_by == "name":
            processes.sort(key=lambda x: x.get("name", "").lower())
        
        # Update report
        sorted_report = report.copy()
        sorted_report["process_details"] = processes
        sorted_report["analysis_metadata"]["sorted_by"] = sort_by
        
        return sorted_report


    async def _perform_comprehensive_port_analysis(self, agent_id: str, port_state: List[str], 
                                                   protocol: List[str], include_process: bool,
                                                   known_services_only: bool, exposure_analysis: bool,
                                                   backdoor_detection: bool, baseline_comparison: bool,
                                                   include_firewall_analysis: bool, threat_intelligence: bool,
                                                   max_ports: int) -> Dict[str, Any]:
        """Perform comprehensive port analysis with exposure and backdoor detection."""
        
        # Initialize the analysis report
        analysis_start = datetime.utcnow()
        
        # Get raw port data from the API
        raw_port_data = await self.api_client.get_agent_ports(agent_id)
        ports = raw_port_data.get("data", {}).get("affected_items", [])
        
        # Filter ports based on criteria
        filtered_ports = self._filter_ports(ports, port_state, protocol, known_services_only, max_ports)
        
        # Build comprehensive report
        port_report = {
            "query_parameters": {
                "agent_id": agent_id,
                "port_state": port_state,
                "protocol": protocol,
                "include_process": include_process,
                "known_services_only": known_services_only,
                "exposure_analysis": exposure_analysis,
                "backdoor_detection": backdoor_detection,
                "baseline_comparison": baseline_comparison,
                "include_firewall_analysis": include_firewall_analysis,
                "threat_intelligence": threat_intelligence,
                "max_ports": max_ports
            },
            "summary": self._generate_port_summary(filtered_ports),
            "port_details": []
        }
        
        # Process each port with enhanced analysis
        for port in filtered_ports:
            port_analysis = await self._analyze_individual_port(
                port, agent_id, include_process, exposure_analysis, 
                backdoor_detection, threat_intelligence
            )
            port_report["port_details"].append(port_analysis)
        
        # Add enhanced analyses if requested
        if exposure_analysis:
            port_report["exposure_analysis"] = await self._perform_exposure_analysis(filtered_ports, agent_id)
        
        if backdoor_detection:
            port_report["backdoor_analysis"] = await self._perform_backdoor_detection(filtered_ports, agent_id)
        
        if baseline_comparison:
            port_report["baseline_analysis"] = await self._perform_baseline_comparison(filtered_ports, agent_id)
        
        if include_firewall_analysis:
            port_report["firewall_analysis"] = await self._perform_firewall_analysis(filtered_ports, agent_id)
        
        # Generate recommendations and security insights
        port_report["security_insights"] = self._generate_security_insights(filtered_ports, port_report)
        port_report["recommendations"] = self._generate_port_recommendations(filtered_ports, port_report)
        
        # Add analysis metadata
        analysis_end = datetime.utcnow()
        port_report["analysis_metadata"] = {
            "analysis_timestamp": analysis_start.isoformat(),
            "total_ports_found": len(ports),
            "ports_analyzed": len(filtered_ports),
            "processing_time_seconds": (analysis_end - analysis_start).total_seconds(),
            "filters_applied": {
                "port_state": port_state,
                "protocol": protocol,
                "known_services_only": known_services_only
            }
        }
        
        return port_report
    
    def _filter_ports(self, ports: List[Dict[str, Any]], port_state: List[str], 
                     protocol: List[str], known_services_only: bool, max_ports: int) -> List[Dict[str, Any]]:
        """Filter ports based on specified criteria."""
        filtered = []
        
        for port in ports:
            # Filter by port state
            if "all" not in port_state:
                if port.get("state") not in port_state:
                    continue
            
            # Filter by protocol
            if "all" not in protocol:
                if port.get("protocol") not in protocol:
                    continue
            
            # Filter by known services only
            if known_services_only:
                port_num = port.get("local_port")
                if not self._is_known_service_port(port_num):
                    continue
            
            filtered.append(port)
            
            # Respect max ports limit
            if len(filtered) >= max_ports:
                break
        
        return filtered
    
    def _is_known_service_port(self, port_num: int) -> bool:
        """Check if port number corresponds to a well-known service."""
        well_known_ports = {
            21: "FTP", 22: "SSH", 23: "Telnet", 25: "SMTP", 53: "DNS",
            80: "HTTP", 110: "POP3", 143: "IMAP", 443: "HTTPS", 993: "IMAPS",
            995: "POP3S", 3389: "RDP", 5432: "PostgreSQL", 3306: "MySQL",
            1521: "Oracle", 27017: "MongoDB", 6379: "Redis"
        }
        return port_num in well_known_ports
    
    def _generate_port_summary(self, ports: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate summary statistics for ports."""
        summary = {
            "total_ports": len(ports),
            "open_ports": 0,
            "listening_ports": 0,
            "protocols": {"tcp": 0, "udp": 0},
            "suspicious_ports": 0,
            "critical_services": 0
        }
        
        for port in ports:
            state = port.get("state", "").lower()
            if state == "open":
                summary["open_ports"] += 1
            elif state == "listening":
                summary["listening_ports"] += 1
            
            protocol = port.get("protocol", "").lower()
            if protocol in summary["protocols"]:
                summary["protocols"][protocol] += 1
            
            # Check for suspicious characteristics
            port_num = port.get("local_port", 0)
            if self._is_suspicious_port(port_num):
                summary["suspicious_ports"] += 1
            
            if self._is_critical_service(port_num):
                summary["critical_services"] += 1
        
        return summary
    
    def _is_suspicious_port(self, port_num: int) -> bool:
        """Check if port number is commonly associated with suspicious activity."""
        suspicious_ports = {
            4444, 8080, 8888, 9999, 31337, 12345, 6667, 6969, 1337, 7777
        }
        return port_num in suspicious_ports or port_num > 49152
    
    def _is_critical_service(self, port_num: int) -> bool:
        """Check if port corresponds to a critical service."""
        critical_ports = {22, 443, 3389, 1521, 3306, 5432, 27017}
        return port_num in critical_ports
    
    async def _analyze_individual_port(self, port: Dict[str, Any], agent_id: str, 
                                     include_process: bool, exposure_analysis: bool,
                                     backdoor_detection: bool, threat_intelligence: bool) -> Dict[str, Any]:
        """Analyze individual port with enhanced security checks."""
        port_analysis = {
            "port_number": port.get("local_port"),
            "protocol": port.get("protocol"),
            "state": port.get("state"),
            "service": self._identify_service(port.get("local_port")),
            "risk_score": 0,
            "risk_factors": []
        }
        
        # Add process information if requested
        if include_process and "process" in port:
            port_analysis["process"] = {
                "name": port.get("process", {}).get("name"),
                "pid": port.get("process", {}).get("pid"),
                "user": port.get("process", {}).get("user")
            }
        
        # Calculate risk score
        port_analysis["risk_score"] = self._calculate_port_risk_score(port)
        
        # Add exposure analysis
        if exposure_analysis:
            port_analysis["exposure_assessment"] = self._assess_port_exposure(port)
        
        # Add backdoor detection
        if backdoor_detection:
            port_analysis["backdoor_indicators"] = self._detect_backdoor_indicators(port)
        
        # Add threat intelligence
        if threat_intelligence:
            port_analysis["threat_intelligence"] = await self._check_port_threat_intelligence(port)
        
        return port_analysis
    
    def _identify_service(self, port_num: int) -> str:
        """Identify service based on port number."""
        services = {
            21: "FTP", 22: "SSH", 23: "Telnet", 25: "SMTP", 53: "DNS",
            80: "HTTP", 110: "POP3", 143: "IMAP", 443: "HTTPS", 993: "IMAPS",
            995: "POP3S", 3389: "RDP", 5432: "PostgreSQL", 3306: "MySQL",
            1521: "Oracle", 27017: "MongoDB", 6379: "Redis"
        }
        return services.get(port_num, "Unknown")
    
    def _calculate_port_risk_score(self, port: Dict[str, Any]) -> int:
        """Calculate risk score for a port (0-100)."""
        score = 0
        port_num = port.get("local_port", 0)
        
        # Base score based on port type
        if self._is_critical_service(port_num):
            score += 30
        elif self._is_suspicious_port(port_num):
            score += 50
        
        # Additional factors
        if port.get("state") == "open":
            score += 20
        
        if port_num > 49152:  # Dynamic/private ports
            score += 15
        
        return min(score, 100)
    
    def _assess_port_exposure(self, port: Dict[str, Any]) -> Dict[str, Any]:
        """Assess port exposure risk."""
        return {
            "internet_facing": False,  # Would need network topology info
            "internal_network": True,
            "exposure_level": "Internal",
            "accessibility": "Network accessible"
        }
    
    def _detect_backdoor_indicators(self, port: Dict[str, Any]) -> Dict[str, Any]:
        """Detect potential backdoor indicators."""
        indicators = {
            "suspicious_port": self._is_suspicious_port(port.get("local_port", 0)),
            "non_standard_service": False,
            "unusual_process": False,
            "risk_level": "Low"
        }
        
        if indicators["suspicious_port"]:
            indicators["risk_level"] = "High"
        
        return indicators
    
    async def _check_port_threat_intelligence(self, port: Dict[str, Any]) -> Dict[str, Any]:
        """Check port against threat intelligence."""
        return {
            "known_malicious": False,
            "reputation": "Clean",
            "intelligence_sources": [],
            "last_checked": datetime.utcnow().isoformat()
        }
    
    async def _perform_exposure_analysis(self, ports: List[Dict[str, Any]], agent_id: str) -> Dict[str, Any]:
        """Perform comprehensive exposure analysis."""
        return {
            "total_exposed_services": len([p for p in ports if p.get("state") == "open"]),
            "critical_exposures": len([p for p in ports if self._is_critical_service(p.get("local_port", 0))]),
            "exposure_summary": "Analysis of network exposure and attack surface",
            "recommendations": [
                "Review open ports and close unnecessary services",
                "Implement network segmentation",
                "Enable firewall protection"
            ]
        }
    
    async def _perform_backdoor_detection(self, ports: List[Dict[str, Any]], agent_id: str) -> Dict[str, Any]:
        """Perform backdoor detection analysis."""
        suspicious_count = len([p for p in ports if self._is_suspicious_port(p.get("local_port", 0))])
        
        return {
            "suspicious_ports_found": suspicious_count,
            "backdoor_risk_level": "High" if suspicious_count > 0 else "Low",
            "indicators": [
                f"Found {suspicious_count} suspicious ports" if suspicious_count > 0 else "No obvious backdoor indicators"
            ],
            "recommendations": [
                "Investigate processes using suspicious ports",
                "Check for unauthorized software",
                "Monitor network connections"
            ] if suspicious_count > 0 else []
        }
    
    async def _perform_baseline_comparison(self, ports: List[Dict[str, Any]], agent_id: str) -> Dict[str, Any]:
        """Compare current ports against baseline."""
        return {
            "baseline_available": False,
            "new_ports": 0,
            "removed_ports": 0,
            "changed_ports": 0,
            "status": "No baseline available for comparison"
        }
    
    async def _perform_firewall_analysis(self, ports: List[Dict[str, Any]], agent_id: str) -> Dict[str, Any]:
        """Analyze firewall rules and port protection."""
        return {
            "firewall_enabled": True,
            "protected_ports": 0,
            "unprotected_ports": len(ports),
            "recommendations": [
                "Enable firewall protection",
                "Configure port-specific rules",
                "Regular firewall rule review"
            ]
        }
    
    def _generate_security_insights(self, ports: List[Dict[str, Any]], report: Dict[str, Any]) -> Dict[str, Any]:
        """Generate security insights from port analysis."""
        high_risk_ports = [p for p in ports if self._calculate_port_risk_score(p) > 50]
        
        return {
            "overall_security_posture": "Fair" if len(high_risk_ports) == 0 else "Poor",
            "key_findings": [
                f"Found {len(ports)} active network ports",
                f"Identified {len(high_risk_ports)} high-risk ports" if high_risk_ports else "No high-risk ports detected"
            ],
            "attack_surface": {
                "size": "Medium" if len(ports) > 10 else "Small",
                "critical_services": len([p for p in ports if self._is_critical_service(p.get("local_port", 0))])
            }
        }
    
    def _generate_port_recommendations(self, ports: List[Dict[str, Any]], report: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate security recommendations based on port analysis."""
        recommendations = []
        
        # Check for high-risk ports
        high_risk_count = len([p for p in ports if self._calculate_port_risk_score(p) > 50])
        if high_risk_count > 0:
            recommendations.append({
                "priority": "HIGH",
                "category": "security",
                "title": "High-Risk Ports Detected",
                "description": f"Found {high_risk_count} ports with elevated security risk",
                "action": "Review and secure high-risk network ports",
                "impact": "Reduces attack surface and potential entry points"
            })
        
        # Check for suspicious ports
        suspicious_count = len([p for p in ports if self._is_suspicious_port(p.get("local_port", 0))])
        if suspicious_count > 0:
            recommendations.append({
                "priority": "HIGH",
                "category": "investigation",
                "title": "Suspicious Ports Found",
                "description": f"Detected {suspicious_count} ports commonly used by malware",
                "action": "Investigate processes using suspicious port numbers",
                "impact": "Identifies potential security compromises"
            })
        
        # General recommendations
        if len(ports) > 20:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "hardening",
                "title": "Large Attack Surface",
                "description": f"Agent has {len(ports)} open network ports",
                "action": "Review and close unnecessary network services",
                "impact": "Reduces overall security exposure"
            })
        
        return recommendations


async def main():
    """Main entry point with comprehensive error handling."""
    logger = get_logger(__name__)
    try:
        server = WazuhMCPServer()
        await server.run()
    except KeyboardInterrupt:
        logger.info("Server shutdown requested")
    except ConfigurationError as e:
        logger.error(f"Configuration error: {str(e)}")
        logger.error("Please check your .env file and environment variables")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())